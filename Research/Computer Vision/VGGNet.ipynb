{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M2bOS5W62SA"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "zO9VlVuiGiYz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFH8bsYnCixL"
      },
      "source": [
        "GLOBAL_SEED = 42\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random as np_rnd\n",
        "import random as rnd\n",
        "import gc\n",
        "import datetime\n",
        "import copy\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_polynomial_decay_schedule_with_warmup\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # python random\n",
        "    rnd.seed(seed)\n",
        "    # numpy random\n",
        "    np_rnd.seed(seed)\n",
        "    # tf random\n",
        "    try:\n",
        "        tf_rnd.set_seed(seed)\n",
        "    except:\n",
        "        pass\n",
        "    # RAPIDS random\n",
        "    try:\n",
        "        cupy.random.seed(seed)\n",
        "    except:\n",
        "        pass\n",
        "    # pytorch random\n",
        "    try:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def pickleIO(obj, src, op=\"w\"):\n",
        "    if op==\"w\":\n",
        "        with open(src, op + \"b\") as f:\n",
        "            pickle.dump(obj, f)\n",
        "    elif op==\"r\":\n",
        "        with open(src, op + \"b\") as f:\n",
        "            tmp = pickle.load(f)\n",
        "        return tmp\n",
        "    else:\n",
        "        print(\"unknown operation\")\n",
        "        return obj\n",
        "    \n",
        "def findIdx(data_x, col_names):\n",
        "    return [int(i) for i, j in enumerate(data_x) if j in col_names]\n",
        "\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print('Error: Creating directory. ' + directory)"
      ],
      "metadata": {
        "id": "7KDYdmNdH1RK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    local = False\n",
        "    debug = False\n",
        "    \n",
        "    epochs = 20\n",
        "    early_stopping_rounds = epochs // 5\n",
        "    batch_size = 128\n",
        "    number_of_labels = 10\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    eta = 5e-4\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "if CFG.debug:\n",
        "    CFG.epochs = 5"
      ],
      "metadata": {
        "id": "gpuSZEW8IEeh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading CIFAR10 Dataset"
      ],
      "metadata": {
        "id": "h-m3LndoE3BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance for training. \n",
        "# When we run this code for the first time, the CIFAR10 train dataset will be downloaded locally. \n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True)\n",
        "train_mean = [train_ds.data[:,:,:,i].mean() for i in range(train_ds.data.shape[-1])]\n",
        "del train_ds; gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th6rmDDqIswu",
        "outputId": "9c9796a8-6407-432f-c205-371a36fd909d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Train dataset loader\n",
        "train_ft = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "    transforms.Normalize(train_mean, 1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "])\n",
        "train_dl = DataLoader(\n",
        "    datasets.CIFAR10(root=\"./data\", train=True, transform=train_ft ,download=True),\n",
        "    batch_size=CFG.batch_size, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "# Create Test dataset loader\n",
        "test_ft = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "    transforms.Normalize(train_mean, 1),\n",
        "])\n",
        "test_dl = DataLoader(\n",
        "    datasets.CIFAR10(root=\"./data\", train=False, transform=test_ft, download=True),\n",
        "    batch_size=CFG.batch_size, shuffle=False, drop_last=False\n",
        ")\n",
        "\n",
        "print(\"The number of images in a test set is: \", len(test_dl) * CFG.batch_size)\n",
        "print(\"The number of iteration is: \", len(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaJWD7KxLc6z",
        "outputId": "8fad6d49-d03f-4b4f-e419-ee1f4de5c635"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "The number of images in a test set is:  10112\n",
            "The number of iteration is:  390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKO1CzhyFK5D"
      },
      "source": [
        "## Create Model & Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer_params(model, eta, weight_decay):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        # apply weight decay\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'lr': eta, 'weight_decay': weight_decay},\n",
        "        # don't apply weight decay for LayerNormalization layer\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'lr': eta, 'weight_decay': 0.0},\n",
        "    ]\n",
        "    return optimizer_parameters\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    scheduler = get_polynomial_decay_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps, power=0.5, lr_end=1e-7\n",
        "    )\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "R__Wsv7pQP3G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFO: The model is simplified architecture from original paper**\n"
      ],
      "metadata": {
        "id": "SR-UTxhsQR5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGNet(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(VGGNet,self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2,2)),\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2,2)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 8 * 8, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 1024 // 4),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(1024 // 4, CFG.number_of_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "ar8v-fLrNRhg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFj6geDUTPkl"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_training():\n",
        "    return_score_dic = []\n",
        "    best_score = np.inf\n",
        "    \n",
        "    seed_everything()\n",
        "    for epoch in range(CFG.epochs):\n",
        "        train_loss = AverageMeter('Loss', ':.4e')\n",
        "        valid_loss = AverageMeter('Loss', ':.4e')\n",
        "        train_accuracy = AverageMeter('Accuracy', ':.4e')\n",
        "        valid_accuracy = AverageMeter('Accuracy', ':.4e')\n",
        "        train_f1 = AverageMeter('F1', ':.4e')\n",
        "        valid_f1 = AverageMeter('F1', ':.4e')\n",
        "\n",
        "        model.train()\n",
        "        for feature, label in train_dl:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                feature = feature.to(device)\n",
        "                label = label.to(device)\n",
        "                output = model(feature)\n",
        "                loss = criterion(output, label)\n",
        "\n",
        "            # initialization gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            # get scaled gradients by float16 (default)\n",
        "            grad_scaler.scale(loss).backward()\n",
        "            # apply original gradients (unscaling) to parameters\n",
        "            # if these gradients do not contain infs or NaNs, optimizer.step() is then called.\n",
        "            # otherwise, optimizer.step() is skipped.\n",
        "            grad_scaler.step(optimizer)\n",
        "            grad_scaler.update()\n",
        "            scheduler.step()\n",
        "            \n",
        "            train_loss.update(loss.item())\n",
        "\n",
        "            y_pred = output.argmax(axis=-1).detach().cpu().numpy()\n",
        "            y_true = label.detach().cpu().numpy()\n",
        "\n",
        "            train_accuracy.update(accuracy_score(y_true, y_pred))        \n",
        "            train_f1.update(f1_score(y_true, y_pred, average=\"macro\"))\n",
        "\n",
        "        model.eval()\n",
        "        for feature, label in test_dl:\n",
        "            with torch.no_grad():\n",
        "                feature = feature.to(device)\n",
        "                label = label.to(device)\n",
        "                output = model(feature)\n",
        "                loss = criterion(output, label)\n",
        "\n",
        "            valid_loss.update(loss.item())  \n",
        "\n",
        "            y_pred = output.argmax(axis=-1).detach().cpu().numpy()\n",
        "            y_true = label.detach().cpu().numpy()\n",
        "\n",
        "            valid_accuracy.update(accuracy_score(y_true, y_pred))        \n",
        "            valid_f1.update(f1_score(y_true, y_pred, average=\"macro\"))\n",
        "\n",
        "        score = valid_loss.avg\n",
        "        return_score_dic.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss.avg,\n",
        "            \"valid_loss\": valid_loss.avg,\n",
        "            \"train_accuracy\": train_accuracy.avg,\n",
        "            \"valid_accuracy\": valid_accuracy.avg,\n",
        "            \"train_f1\": train_f1.avg,\n",
        "            \"valid_f1\": valid_f1.avg,  \n",
        "        })\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = copy.deepcopy(score)\n",
        "            early_stopping_cnt = 0 \n",
        "        else:\n",
        "            early_stopping_cnt += 1\n",
        "\n",
        "        if early_stopping_cnt == CFG.early_stopping_rounds:\n",
        "            print(\"INFO : Early Stopped ! (Epoch[{0}/{1}])\".format(epoch+1, CFG.epochs))  \n",
        "            break\n",
        "\n",
        "        print(f'[{epoch+1:02d}/{CFG.epochs}]:  * Train Loss {train_loss.avg:.3f} * Train Accuracy {train_accuracy.avg:.3f} * Train F1 {train_f1.avg:.3f} * Valid Loss {valid_loss.avg:.3f} * Valid Accuracy {valid_accuracy.avg:.3f} * Valid F1 {valid_f1.avg:.3f}')\n",
        "\n",
        "    return return_score_dic"
      ],
      "metadata": {
        "id": "kV41_PFCRbgE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ_CXj23St6c"
      },
      "source": [
        "# model\n",
        "model = VGGNet()\n",
        "model.to(device)\n",
        "\n",
        "# optimizer & scheduler\n",
        "optimizer_parameters = get_optimizer_params(\n",
        "    model,\n",
        "    eta=CFG.eta,\n",
        "    weight_decay=CFG.weight_decay\n",
        ")\n",
        "optimizer = AdamW(optimizer_parameters, lr=CFG.eta, weight_decay=CFG.weight_decay)\n",
        "scheduler = get_scheduler(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dl) * CFG.epochs)\n",
        ")\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# gradient scaler for fast operation with float16\n",
        "grad_scaler = torch.cuda.amp.GradScaler()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "score_dic = fn_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8dvyK8KSPeA",
        "outputId": "788a8d5b-3415-441f-edc5-7d765a038c41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/20]:  * Train Loss 2.304 * Train Accuracy 0.098 * Train F1 0.033 * Valid Loss 2.303 * Valid Accuracy 0.101 * Valid F1 0.018\n",
            "[02/20]:  * Train Loss 2.303 * Train Accuracy 0.099 * Train F1 0.033 * Valid Loss 2.303 * Valid Accuracy 0.101 * Valid F1 0.018\n",
            "[03/20]:  * Train Loss 2.303 * Train Accuracy 0.098 * Train F1 0.036 * Valid Loss 2.303 * Valid Accuracy 0.099 * Valid F1 0.018\n",
            "[04/20]:  * Train Loss 2.303 * Train Accuracy 0.101 * Train F1 0.035 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "[05/20]:  * Train Loss 2.303 * Train Accuracy 0.099 * Train F1 0.032 * Valid Loss 2.303 * Valid Accuracy 0.101 * Valid F1 0.018\n",
            "[06/20]:  * Train Loss 2.303 * Train Accuracy 0.100 * Train F1 0.031 * Valid Loss 2.303 * Valid Accuracy 0.101 * Valid F1 0.018\n",
            "[07/20]:  * Train Loss 2.303 * Train Accuracy 0.098 * Train F1 0.032 * Valid Loss 2.303 * Valid Accuracy 0.101 * Valid F1 0.018\n",
            "[08/20]:  * Train Loss 2.303 * Train Accuracy 0.097 * Train F1 0.036 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "[09/20]:  * Train Loss 2.303 * Train Accuracy 0.101 * Train F1 0.034 * Valid Loss 2.303 * Valid Accuracy 0.099 * Valid F1 0.018\n",
            "[10/20]:  * Train Loss 2.303 * Train Accuracy 0.099 * Train F1 0.032 * Valid Loss 2.303 * Valid Accuracy 0.101 * Valid F1 0.018\n",
            "[11/20]:  * Train Loss 2.303 * Train Accuracy 0.100 * Train F1 0.027 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "[12/20]:  * Train Loss 2.303 * Train Accuracy 0.097 * Train F1 0.045 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "[13/20]:  * Train Loss 2.303 * Train Accuracy 0.098 * Train F1 0.032 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "[14/20]:  * Train Loss 2.303 * Train Accuracy 0.097 * Train F1 0.034 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "[15/20]:  * Train Loss 2.303 * Train Accuracy 0.097 * Train F1 0.035 * Valid Loss 2.303 * Valid Accuracy 0.101 * Valid F1 0.018\n",
            "[16/20]:  * Train Loss 2.303 * Train Accuracy 0.099 * Train F1 0.040 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "[17/20]:  * Train Loss 2.303 * Train Accuracy 0.097 * Train F1 0.044 * Valid Loss 2.303 * Valid Accuracy 0.099 * Valid F1 0.018\n",
            "[18/20]:  * Train Loss 2.303 * Train Accuracy 0.096 * Train F1 0.045 * Valid Loss 2.303 * Valid Accuracy 0.100 * Valid F1 0.018\n",
            "INFO : Early Stopped ! (Epoch[19/20])\n",
            "CPU times: user 6min 8s, sys: 1.98 s, total: 6min 10s\n",
            "Wall time: 6min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(score_dic).mean().iloc[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq1n5mwzbA-A",
        "outputId": "b372b1d6-5c2f-47f4-b6e8-6523d3c36f18"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train_loss        2.302850\n",
              "valid_loss        2.302661\n",
              "train_accuracy    0.098431\n",
              "valid_accuracy    0.100168\n",
              "train_f1          0.036390\n",
              "valid_f1          0.018206\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QiKLnpMQmuU"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}