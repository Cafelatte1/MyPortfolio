{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleIO(obj, src, op=\"r\"):\n",
    "    if op==\"w\":\n",
    "        with open(src, op + \"b\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif op==\"r\":\n",
    "        with open(src, op + \"b\") as f:\n",
    "            tmp = pickle.load(f)\n",
    "        return tmp\n",
    "    else:\n",
    "        print(\"unknown operation\")\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_summary(key_word):\n",
    "    url = f'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={key_word}'\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "    res = requests.get(url, headers= headers)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    news_list = soup.find_all('div', attrs={'class':'info_group'})\n",
    "\n",
    "    news_url_list = [] # 네이버 뉴스 url 리스트\n",
    "    new_article_list= [] # 네이버 뉴스 기사 통합 리스트\n",
    "    new_article_head_list = [] \n",
    "    sec_article_list = []\n",
    "    for i in range(9): # 최대 5개 정도만 추출\n",
    "        news_article = news_list[i].find_all(\"a\", attrs={\"class\":\"info\"})\n",
    "        if len(news_article) > 1:\n",
    "            news_nm_url = news_article[1].get('href')\n",
    "            # print(news_nm_url)\n",
    "            news_url_list.append(news_nm_url)\n",
    "            res = requests.get(news_nm_url, headers=headers)\n",
    "            soup = BeautifulSoup(res.text, 'lxml')\n",
    "            try:\n",
    "                news_article_nm = soup.find_all('div', attrs = {'class': 'newsct_article _article_body'})\n",
    "                aritle = soup.find(attrs={'class' : 'go_trans _article_content'}).text\n",
    "                # print(i, '번쨰껄 가지고 옵니다. ')\n",
    "                sec_article_list.append(aritle)\n",
    "                news_article_head = soup.find( attrs = {'id': 'title_area'})\n",
    "                news_article_text = news_article_nm[0].get_text()\n",
    "                news_article_head_text = news_article_head.get_text()\n",
    "                new_article_list.append(news_article_text)\n",
    "                new_article_head_list.append(news_article_head_text)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return pd.DataFrame({\"head\": new_article_head_list, \"article\": new_article_list, \"sector\": sec_article_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = get_news_summary(\"삼성전자\")\n",
    "pickleIO(df_news, \"./news_삼성전자.pkl\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_from_11st(query):\n",
    "    # Browser configuration\n",
    "    # WARNiNG: chrome browser must be installed in local machine\n",
    "    chromedriver_autoinstaller.install()\n",
    "    service = Service(executable_path=\"chromedriver\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    # Link to root URL\n",
    "    url = f'https://search.11st.co.kr/Search.tmall?kwd={query}'\n",
    "    driver.get(url); driver.implicitly_wait(5)\n",
    "    # Extract product meta information\n",
    "    prod_list = driver.find_elements(By.CSS_SELECTOR, \"#layBodyWrap > div > div > div.l_search_content > div > section:nth-child(4) > ul > li:nth-child(n) > div > div:nth-child(2) > div.c_card_info_top > div.c_prd_name.c_prd_name_row_1\")\n",
    "    prod_title_list = [i.find_element(By.TAG_NAME, \"strong\").text for i in prod_list]\n",
    "    link_list = [i.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") for i in prod_list]\n",
    "    # create dataframe for review data\n",
    "    df_merge = []\n",
    "    for link, title in zip(link_list, prod_title_list):\n",
    "        # link to product page\n",
    "        driver.get(link); driver.implicitly_wait(5)\n",
    "        # switch to review iframe\n",
    "        review_frame = driver.find_element(By.CSS_SELECTOR, \"#ifrmReview\")\n",
    "        driver.switch_to.frame(review_frame)\n",
    "        # extract review data\n",
    "        reviews = [i.text for i in driver.find_elements(By.CSS_SELECTOR, \"#review-list-page-area > ul > li:nth-child(n) > div.c_product_review_cont > div > div.cont_text_wrap > p.text-expanded\")]\n",
    "        grades = [i.text for i in driver.find_elements(By.CSS_SELECTOR, \"#review-list-page-area > ul > li:nth-child(n) > div.c_product_review_cont > p > span > em\")]\n",
    "        review_output = {\n",
    "            \"prod_name\": [],\n",
    "            \"review\": [],\n",
    "            \"grade\": [],\n",
    "        }\n",
    "        for review, grade in zip(reviews, grades):\n",
    "            if len(review) > 0:\n",
    "                review_output[\"prod_name\"].append(title)\n",
    "                review_output[\"review\"].append(review)\n",
    "                review_output[\"grade\"].append(grade)\n",
    "        df_merge.append(pd.DataFrame(review_output))\n",
    "        time.sleep(1)\n",
    "    # concatenate all reviews on products\n",
    "    df_merge = pd.concat(df_merge).reset_index(drop=True)\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_shop = get_review_from_11st(\"LG그램\")\n",
    "pickleIO(df_review_shop, \"./review_lg그램.pkl\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_review():\n",
    "    # Browser configuration\n",
    "    # WARNiNG: chrome browser must be installed in local machine\n",
    "    chromedriver_autoinstaller.install()\n",
    "    service = Service(executable_path=\"chromedriver\")\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    # Link to root URL (네이버 영화 고요의 바다 리뷰 페이지)\n",
    "    url = f'https://tv.naver.com/v/23742179#comment_focus'\n",
    "    driver.get(url); driver.implicitly_wait(5)\n",
    "    time.sleep(3)\n",
    "    # stop video\n",
    "    driver.find_element(By.CSS_SELECTOR, \"video.webplayer-internal-video\").click()\n",
    "    # get reviews\n",
    "    review_list = driver.find_elements(By.CSS_SELECTOR, \"ul.u_cbox_list > li > div > div > div.u_cbox_text_wrap > span\")\n",
    "    review_list = [i.text for i in review_list]\n",
    "    driver.close()\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_netflix = get_content_review()\n",
    "df_review_netflix = pd.DataFrame(df_review_netflix)\n",
    "df_review_netflix.columns = [\"review\"]\n",
    "pickleIO(df_review_netflix, \"./review_netflix.pkl\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
