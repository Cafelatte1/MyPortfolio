{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M2bOS5W62SA"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "zO9VlVuiGiYz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFH8bsYnCixL"
      },
      "source": [
        "GLOBAL_SEED = 42\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random as np_rnd\n",
        "import random as rnd\n",
        "import gc\n",
        "import datetime\n",
        "import copy\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_polynomial_decay_schedule_with_warmup\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # python random\n",
        "    rnd.seed(seed)\n",
        "    # numpy random\n",
        "    np_rnd.seed(seed)\n",
        "    # tf random\n",
        "    try:\n",
        "        tf_rnd.set_seed(seed)\n",
        "    except:\n",
        "        pass\n",
        "    # RAPIDS random\n",
        "    try:\n",
        "        cupy.random.seed(seed)\n",
        "    except:\n",
        "        pass\n",
        "    # pytorch random\n",
        "    try:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def pickleIO(obj, src, op=\"w\"):\n",
        "    if op==\"w\":\n",
        "        with open(src, op + \"b\") as f:\n",
        "            pickle.dump(obj, f)\n",
        "    elif op==\"r\":\n",
        "        with open(src, op + \"b\") as f:\n",
        "            tmp = pickle.load(f)\n",
        "        return tmp\n",
        "    else:\n",
        "        print(\"unknown operation\")\n",
        "        return obj\n",
        "    \n",
        "def findIdx(data_x, col_names):\n",
        "    return [int(i) for i, j in enumerate(data_x) if j in col_names]\n",
        "\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print('Error: Creating directory. ' + directory)"
      ],
      "metadata": {
        "id": "7KDYdmNdH1RK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    local = False\n",
        "    debug = False\n",
        "    \n",
        "    epochs = 20\n",
        "    early_stopping_rounds = max(10, epochs // 5)\n",
        "    batch_size = 128\n",
        "    number_of_labels = 10\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    eta = 5e-4\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "if CFG.debug:\n",
        "    CFG.epochs = 5"
      ],
      "metadata": {
        "id": "gpuSZEW8IEeh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading CIFAR10 Dataset"
      ],
      "metadata": {
        "id": "h-m3LndoE3BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance for training. \n",
        "# When we run this code for the first time, the CIFAR10 train dataset will be downloaded locally. \n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True)\n",
        "train_mean = [train_ds.data[:,:,:,i].mean() for i in range(train_ds.data.shape[-1])]\n",
        "del train_ds; gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th6rmDDqIswu",
        "outputId": "6c0f0169-8149-43e5-ee53-806ba84813bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFO: Use MinMax Normalization 0~1 (different from the original paper)**"
      ],
      "metadata": {
        "id": "x7OfU4l5o3X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Train dataset loader\n",
        "train_ft = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "    transforms.Normalize(0, 255),\n",
        "    # transforms.Normalize(train_mean, 1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "])\n",
        "train_dl = DataLoader(\n",
        "    datasets.CIFAR10(root=\"./data\", train=True, transform=train_ft ,download=True),\n",
        "    batch_size=CFG.batch_size, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "# Create Test dataset loader\n",
        "test_ft = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "    transforms.Normalize(0, 255),\n",
        "    # transforms.Normalize(train_mean, 1),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "])\n",
        "test_dl = DataLoader(\n",
        "    datasets.CIFAR10(root=\"./data\", train=False, transform=test_ft, download=True),\n",
        "    batch_size=CFG.batch_size, shuffle=False, drop_last=False\n",
        ")\n",
        "\n",
        "print(\"The number of images in a test set is: \", len(test_dl) * CFG.batch_size)\n",
        "print(\"The number of iteration is: \", len(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaJWD7KxLc6z",
        "outputId": "3404311f-cca7-48b9-be91-2544526fc6c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "The number of images in a test set is:  10112\n",
            "The number of iteration is:  390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKO1CzhyFK5D"
      },
      "source": [
        "## Create Model & Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer_params(model, eta, weight_decay):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        # apply weight decay\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'lr': eta, 'weight_decay': weight_decay},\n",
        "        # don't apply weight decay for LayerNormalization layer\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'lr': eta, 'weight_decay': 0.0},\n",
        "    ]\n",
        "    return optimizer_parameters\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    scheduler = get_polynomial_decay_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps, power=0.5, lr_end=1e-7\n",
        "    )\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "R__Wsv7pQP3G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFO: The model is simplified architecture from original paper**\n"
      ],
      "metadata": {
        "id": "SR-UTxhsQR5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_hidden_layers = 16\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(ResNet,self).__init__()\n",
        "\n",
        "        self.conv1_pool = nn.Sequential(\n",
        "            nn.Conv2d(3, base_hidden_layers, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Conv2d(base_hidden_layers, base_hidden_layers, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv1_resd = nn.Sequential(\n",
        "            nn.Conv2d(base_hidden_layers, base_hidden_layers, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(base_hidden_layers, base_hidden_layers, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers),\n",
        "        )\n",
        "\n",
        "        self.conv2_pool = nn.Sequential(\n",
        "            nn.Conv2d(base_hidden_layers, base_hidden_layers * 2, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Conv2d(base_hidden_layers * 2, base_hidden_layers * 2, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers * 2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv2_resd = nn.Sequential(\n",
        "            nn.Conv2d(base_hidden_layers * 2, base_hidden_layers * 2, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(base_hidden_layers * 2, base_hidden_layers * 2, kernel_size=(3, 3), padding=\"same\"),\n",
        "            nn.BatchNorm2d(base_hidden_layers * 2),\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(base_hidden_layers * 2 * 8 * 8, base_hidden_layers * 2 * 8 * 8 // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(base_hidden_layers * 2 * 8 * 8 // 4, base_hidden_layers * 2 * 8 * 8 // 4 // 4),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(base_hidden_layers * 2 * 8 * 8 // 4 // 4, CFG.number_of_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1_pool(x)\n",
        "        x = F.relu(self.conv1_resd(x) + x)\n",
        "        x = self.conv2_pool(x)\n",
        "        x = F.relu(self.conv2_resd(x) + x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "ar8v-fLrNRhg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFj6geDUTPkl"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fn_training():\n",
        "    return_score_dic = []\n",
        "    best_score = np.inf\n",
        "    \n",
        "    seed_everything()\n",
        "    for epoch in range(CFG.epochs):\n",
        "        train_loss = AverageMeter('Loss', ':.4e')\n",
        "        valid_loss = AverageMeter('Loss', ':.4e')\n",
        "        train_accuracy = AverageMeter('Accuracy', ':.4e')\n",
        "        valid_accuracy = AverageMeter('Accuracy', ':.4e')\n",
        "        train_f1 = AverageMeter('F1', ':.4e')\n",
        "        valid_f1 = AverageMeter('F1', ':.4e')\n",
        "\n",
        "        model.train()\n",
        "        for feature, label in train_dl:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                feature = feature.to(device)\n",
        "                label = label.to(device)\n",
        "                output = model(feature)\n",
        "                loss = criterion(output, label)\n",
        "\n",
        "            # initialization gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            # get scaled gradients by float16 (default)\n",
        "            grad_scaler.scale(loss).backward()\n",
        "            # apply original gradients (unscaling) to parameters\n",
        "            # if these gradients do not contain infs or NaNs, optimizer.step() is then called.\n",
        "            # otherwise, optimizer.step() is skipped.\n",
        "            grad_scaler.step(optimizer)\n",
        "            grad_scaler.update()\n",
        "            scheduler.step()\n",
        "            \n",
        "            train_loss.update(loss.item())\n",
        "\n",
        "            y_pred = output.argmax(axis=-1).detach().cpu().numpy()\n",
        "            y_true = label.detach().cpu().numpy()\n",
        "\n",
        "            train_accuracy.update(accuracy_score(y_true, y_pred))        \n",
        "            train_f1.update(f1_score(y_true, y_pred, average=\"macro\"))\n",
        "\n",
        "        model.eval()\n",
        "        for feature, label in test_dl:\n",
        "            with torch.no_grad():\n",
        "                feature = feature.to(device)\n",
        "                label = label.to(device)\n",
        "                output = model(feature)\n",
        "                loss = criterion(output, label)\n",
        "\n",
        "            valid_loss.update(loss.item())  \n",
        "\n",
        "            y_pred = output.argmax(axis=-1).detach().cpu().numpy()\n",
        "            y_true = label.detach().cpu().numpy()\n",
        "\n",
        "            valid_accuracy.update(accuracy_score(y_true, y_pred))        \n",
        "            valid_f1.update(f1_score(y_true, y_pred, average=\"macro\"))\n",
        "\n",
        "        score = valid_loss.avg\n",
        "        return_score_dic.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss.avg,\n",
        "            \"valid_loss\": valid_loss.avg,\n",
        "            \"train_accuracy\": train_accuracy.avg,\n",
        "            \"valid_accuracy\": valid_accuracy.avg,\n",
        "            \"train_f1\": train_f1.avg,\n",
        "            \"valid_f1\": valid_f1.avg,  \n",
        "        })\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = copy.deepcopy(score)\n",
        "            early_stopping_cnt = 0 \n",
        "        else:\n",
        "            early_stopping_cnt += 1\n",
        "\n",
        "        if early_stopping_cnt == CFG.early_stopping_rounds:\n",
        "            print(\"INFO : Early Stopped ! (Epoch[{0}/{1}])\".format(epoch+1, CFG.epochs))  \n",
        "            break\n",
        "\n",
        "        print(f'[{epoch+1:02d}/{CFG.epochs}]:  * Train Loss {train_loss.avg:.3f} * Train Accuracy {train_accuracy.avg:.3f} * Train F1 {train_f1.avg:.3f} * Valid Loss {valid_loss.avg:.3f} * Valid Accuracy {valid_accuracy.avg:.3f} * Valid F1 {valid_f1.avg:.3f}')\n",
        "\n",
        "    return return_score_dic"
      ],
      "metadata": {
        "id": "kV41_PFCRbgE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ_CXj23St6c"
      },
      "source": [
        "# model\n",
        "model = ResNet()\n",
        "model.to(device)\n",
        "\n",
        "# optimizer & scheduler\n",
        "optimizer_parameters = get_optimizer_params(\n",
        "    model,\n",
        "    eta=CFG.eta,\n",
        "    weight_decay=CFG.weight_decay\n",
        ")\n",
        "optimizer = AdamW(optimizer_parameters, lr=CFG.eta, weight_decay=CFG.weight_decay)\n",
        "scheduler = get_scheduler(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dl) * CFG.epochs)\n",
        ")\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# gradient scaler for fast operation with float16\n",
        "grad_scaler = torch.cuda.amp.GradScaler()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "score_dic = fn_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8dvyK8KSPeA",
        "outputId": "84fc65eb-bc82-4cf7-89ae-03f2fc8f711b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/20]:  * Train Loss 1.488 * Train Accuracy 0.454 * Train F1 0.433 * Valid Loss 2.365 * Valid Accuracy 0.261 * Valid F1 0.201\n",
            "[02/20]:  * Train Loss 1.150 * Train Accuracy 0.589 * Train F1 0.575 * Valid Loss 3.137 * Valid Accuracy 0.178 * Valid F1 0.138\n",
            "[03/20]:  * Train Loss 1.019 * Train Accuracy 0.637 * Train F1 0.625 * Valid Loss 2.960 * Valid Accuracy 0.105 * Valid F1 0.033\n",
            "[04/20]:  * Train Loss 0.932 * Train Accuracy 0.671 * Train F1 0.660 * Valid Loss 4.543 * Valid Accuracy 0.144 * Valid F1 0.062\n",
            "[05/20]:  * Train Loss 0.862 * Train Accuracy 0.696 * Train F1 0.686 * Valid Loss 2.410 * Valid Accuracy 0.236 * Valid F1 0.152\n",
            "[06/20]:  * Train Loss 0.806 * Train Accuracy 0.716 * Train F1 0.707 * Valid Loss 4.232 * Valid Accuracy 0.102 * Valid F1 0.021\n",
            "[07/20]:  * Train Loss 0.759 * Train Accuracy 0.733 * Train F1 0.725 * Valid Loss 8.478 * Valid Accuracy 0.102 * Valid F1 0.023\n",
            "[08/20]:  * Train Loss 0.730 * Train Accuracy 0.743 * Train F1 0.735 * Valid Loss 5.305 * Valid Accuracy 0.158 * Valid F1 0.078\n",
            "[09/20]:  * Train Loss 0.689 * Train Accuracy 0.757 * Train F1 0.749 * Valid Loss 3.851 * Valid Accuracy 0.156 * Valid F1 0.085\n",
            "[10/20]:  * Train Loss 0.662 * Train Accuracy 0.767 * Train F1 0.759 * Valid Loss 3.937 * Valid Accuracy 0.152 * Valid F1 0.088\n",
            "INFO : Early Stopped ! (Epoch[11/20])\n",
            "CPU times: user 3min 45s, sys: 1.71 s, total: 3min 46s\n",
            "Wall time: 3min 52s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(score_dic).mean().iloc[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq1n5mwzbA-A",
        "outputId": "d14e6992-0be3-4310-9000-52b7b89831b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train_loss        0.884807\n",
              "valid_loss        4.388568\n",
              "train_accuracy    0.685511\n",
              "valid_accuracy    0.155054\n",
              "train_f1          0.674987\n",
              "valid_f1          0.083619\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QiKLnpMQmuU"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}