{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 김영준 - RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 김영준 RandomForest =====\n",
    "rnd.seed(334)\n",
    "ntrees = 500\n",
    "patientRate = 0.2\n",
    "eta = 0.01\n",
    "seed = 9191\n",
    "\n",
    "tuner_params = {\"num_leaves\": [pow(2, i) - 1 for i in [2, 4, 6, 8]],\n",
    "                \"subsample\": [0.4, 0.6, 0.8],\n",
    "                \"colsample_bytree\": [0.6, 0.8, 1],\n",
    "                \"reg_lambda\": list(np.linspace(0.1, 10, 10).round(3))}\n",
    "lgb_model = lgb.LGBMRegressor(boosting_type=\"rf\", objective=\"regression\",\n",
    "                              n_estimators=int(np.floor(ntrees * patientRate)),\n",
    "                              learning_rate=eta, silent=True, n_jobs=None,\n",
    "                              subsample_freq=1, random_state=seed)\n",
    "model_tuner = GridTuner(lgb_model, param_grid=tuner_params, cv=10, refit=False,\n",
    "                        n_jobs=multiprocessing.cpu_count(),\n",
    "                        pre_dispatch=multiprocessing.cpu_count(),\n",
    "                        scoring=\"neg_root_mean_squared_error\")\n",
    "model_tuner.fit(train_x, train_y, categorical_feature=findIdx(train_x, cat_vars), verbose=False)\n",
    "\n",
    "model_rf = {}\n",
    "print(\"Tuning Result --->\", model_tuner.best_params_)\n",
    "model_rf[\"best_params\"] = model_tuner.best_params_\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(boosting_type=\"rf\", objective=\"regression\",\n",
    "                              num_leaves=model_tuner.best_params_[\"num_leaves\"],\n",
    "                              n_estimators=ntrees, learning_rate=eta,\n",
    "                              n_jobs=multiprocessing.cpu_count(), random_state=seed+9,\n",
    "                              reg_lambda=model_tuner.best_params_[\"reg_lambda\"],\n",
    "                              subsample=model_tuner.best_params_[\"subsample\"],\n",
    "                              colsample_bytree=model_tuner.best_params_[\"colsample_bytree\"],\n",
    "                              subsample_freq=1, silent=True)\n",
    "model_rf[\"model\"] = lgb_model.fit(train_x, train_y, categorical_feature=findIdx(train_x, cat_vars),\n",
    "                                  eval_set=[(val_x, val_y)], eval_metric=\"rmse\", verbose=False,\n",
    "                                  early_stopping_rounds=int(np.floor(ntrees * patientRate)))\n",
    "\n",
    "model_rf[\"pred\"] = model_rf[\"model\"].predict(val_x)\n",
    "model_rf[\"performance\"] = {\"RMSE\": np.sqrt(metrics.mean_squared_error(val_y, model_rf[\"pred\"])),\n",
    "                           \"R2\": metrics.r2_score(val_y, model_rf[\"pred\"])}\n",
    "\n",
    "print(model_rf[\"model\"].best_iteration_)\n",
    "print(model_rf[\"best_params\"])\n",
    "print(model_rf[\"performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 김남이 - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================[ 김남이 XGBoost ]=======================#\n",
    "oh_encoder = MyOneHotEncoder()\n",
    "train_x_oh = oh_encoder.fit_transform(train_x, cat_vars)\t\n",
    "\n",
    "ntrees = 5000 \n",
    "model_xgb3 = XGBRegressor(booster=\"gbtree\", n_estimators=int(ntrees*0.3), objective=\"reg:squarederror\", seed=343)\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01,0.05],\n",
    "    'max_depth': [2,4,6],\n",
    "    'reg_lambda' : [0.5, 1, 5, 10],\n",
    "    'subsample' : [0.5, 0.6, 0.8]\n",
    "}\n",
    "\n",
    "xgb_grid3 = GridTuner(model_xgb3, param_grid=xgb_param_grid, scoring='neg_root_mean_squared_error',\n",
    "                     cv=10, n_jobs=-1, refit=False, verbose=1)\n",
    "xgb_grid3.fit(train_x_oh, train_y)\n",
    "\n",
    "cv_result = pd.DataFrame(xgb_grid3.cv_results_)\n",
    "cv_result.sort_values(by=['rank_test_score'], inplace=True)\n",
    "from IPython.core.display import display, HTML\n",
    "pd.set_option('display.max.colwidth', 500)\n",
    "\n",
    "cv2 = cv_result.loc[:,['params', 'mean_test_score','rank_test_score']]\n",
    "cv2.head()\n",
    "\n",
    "\n",
    "train_x, val_x, train_x_oh, val_x_oh, train_y, val_y = tts(train_x, train_x_oh, train_y, test_size=0.2, random_state=777)\n",
    "\n",
    "xgb4 = XGBRegressor(n_estimators=5000, learning_rate=0.01, max_depth=4, reg_lambda= 5, subsample=0.5, objective=\"reg:squarederror\",colsample_bytree=0.8,seed=343)\n",
    "evals=[(val_x_oh, val_y)]\n",
    "xgb4.fit(train_x_oh, train_y, early_stopping_rounds=500,eval_metric='rmse', eval_set=evals, verbose=1)\n",
    "\n",
    "xgb4_pred = xgb4.predict(val_x_oh)\n",
    "xgb4_rmse = mean_squared_error(val_y, xgb4_pred)\n",
    "xgb4_r2 = r2_score(val_y, xgb4_pred)\n",
    "print('Mean squared error: ', np.sqrt(xgb4_rmse))\n",
    "print('R2 score: ', xgb4_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이지예 - CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####------------------------[ 이지예의 Catboost ] -------------------------####\n",
    "start = time.time()\n",
    "\n",
    "### 1. CatBoost 최적 하이퍼 파라미터 찾기\n",
    "ntrees = 3000\n",
    "cb = cb.CatBoostRegressor(random_state=11, n_estimators=int(ntrees*0.2), loss_function = 'RMSE' )\n",
    "param = {\n",
    "    'learning_rate' : [0.05, 0.06, 0.1],\n",
    "    'max_depth' : [2,5,8],\n",
    "    'l2_leaf_reg' : [0,3,5,10]\n",
    "}\n",
    "grid_cv = GridSearchCV(cb, param_grid=param,  scoring='neg_root_mean_squared_error', cv=10, verbose=1, n_jobs=-1)\n",
    "grid_cv.fit(train_x_oh, train_y)\n",
    "print('최적 하이퍼 파라미터: \\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도(RMSE의 -값): {0:.4f}'.format(grid_cv.best_score_))\n",
    "\n",
    "### 2. 최적 하이퍼파라미터에 적용시키기\n",
    "ntrees = 3000\n",
    "cb1 = cb.CatBoostRegressor(l2_leaf_reg = 3,learning_rate = 0.06, n_estimators = ntrees, max_depth=5, boosting_type='Plain', early_stopping_rounds=500, use_best_model=True, loss_function = 'RMSE') # 최적 하이퍼파라미터에 적용한 후 다시 학습시키기\n",
    "cb1_model = cb1.fit(train_x_oh, train_y, eval_set=[(val_x_oh, val_y)])\n",
    "# GridSearchCV를 이용해 최적으로 학습된 estimators로 예측 수행\n",
    "cb1_model_predict = cb1_model.predict(val_x_oh)\n",
    "\n",
    "print(\"Time:%.1f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산\n",
    "print(\"RMSE: {:.3f}\".format(sqrt(mean_squared_error(val_y,cb1_model_predict))))\n",
    "print(\"R2: {:.3f}\".format(r2_score(val_y,cb1_model_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이예주 - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================이예주 LightGBM==============================#\n",
    "start = time.time()\n",
    "\n",
    "#최적 파라미터 찾기\n",
    "ntrees = 5000\n",
    "model_lgb = LGBMRegressor(boosting=\"goss\", n_estimators=int(ntrees*0.2), objective=\"regression\", seed=525)\n",
    "lgb_param_grid = {\n",
    "    'learning_rate': [0.01,0.05,0.1],\n",
    "    'num_leaves': [3,7,15,31],\n",
    "    'reg_lambda' : [0.1, 1, 10],\n",
    "    'subsample' : [0.5,0.6,0.7]\n",
    "}\n",
    "lgb_grid = GridSearchCV(model_lgb, param_grid=lgb_param_grid, \n",
    "                        scoring='neg_root_mean_squared_error',\n",
    "                     cv=10, n_jobs=-1, refit=False, verbose=1)\n",
    "lgb_grid.fit(train_x_oh, train_y)\n",
    "print(\"최적 하이퍼 파라미터:\" , lgb_grid.best_params_)\n",
    "\n",
    "#학습\n",
    "lgb = LGBMRegressor(\n",
    "    boosting=\"goss\", n_estimators=ntrees,\n",
    "    objective=\"regression\", seed=525,\n",
    "    learning_rate = 0.01, num_leaves = 7,\n",
    "    reg_lambda= 1,subsample= 0.5\n",
    ")\n",
    "evals = [(X_test, y_test)]\n",
    "lgb.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric='rmse', eval_set=evals, verbose=True)\n",
    "lgb_pred = lgb.predict(X_test)\n",
    "lgb_rmse = sqrt(mean_squared_error(y_test, lgb_pred))\n",
    "lgb_r2 = r2_score(y_test, lgb_pred)\n",
    "print('Mean squared error: ', lgb_rmse)\n",
    "print('R2 score: ', lgb_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- Stacked Ensemble----------------\n",
    "# RandomForest 제외 (성능 문제)\n",
    "rnd.seed(1234)\n",
    "stacking_base_models = [\n",
    "    (\"XGBoost\", xgb.XGBRegressor(booster=\"gbtree\", objective=\"reg:squarederror\",\n",
    "                                 n_estimators=2478, max_depth=4,\n",
    "                                 subsample=0.6, colsample_bytree=0.8,\n",
    "                                 reg_lambda=5, learning_rate=0.01,\n",
    "                                 verbosity=0, random_state=777)),\n",
    "    (\"LightGBM\", lgb.LGBMRegressor(boosting_type=\"goss\", objective=\"regression\",\n",
    "                                   n_estimators=912, num_leaves=2**3-1,\n",
    "                                   subsample=0.5, colsample_bytree=0.8,\n",
    "                                   reg_lambda=11, learning_rate=0.01,\n",
    "                                   silent=True, random_state=777)),\n",
    "    (\"CatBoost\", cat.CatBoostRegressor(boosting_type='Plain', loss_function='RMSE',\n",
    "                                       n_estimators=535, max_depth=5,\n",
    "                                       rsm=0.8, # rsm = colsample_bytree\n",
    "                                       l2_leaf_reg=3, learning_rate=0.06,\n",
    "                                       silent=True, random_seed=777))\n",
    "]\n",
    "\n",
    "meta_learner_model = lm.ElasticNetCV(l1_ratio=[.1, .3, .5, .6, .7, .75, .8, .85, .9, .95, .99, 1], n_alphas=1000, random_state=777)\n",
    "{'RMSE': 77.21209404644407, 'R2': 0.8712949169257178}\n",
    "meta_learner_model = lm.LinearRegression()\n",
    "{'RMSE': 76.8885220689633, 'R2': 0.8723713829212315}\n",
    "\n",
    "result_se = {}\n",
    "result_se[\"model\"] = ensemble.StackingRegressor(estimators=stacking_base_models,\n",
    "                                                final_estimator=meta_learner_model,\n",
    "                                                cv=10,\n",
    "                                                n_jobs=multiprocessing.cpu_count())\n",
    "\n",
    "result_se[\"model\"].fit(train_x_oh, train_y)\n",
    "result_se[\"pred\"] = result_se[\"model\"].predict(val_x_oh)\n",
    "result_se[\"performance\"] = {\"RMSE\": np.sqrt(metrics.mean_squared_error(val_y, result_se[\"pred\"])),\n",
    "                             \"R2\": metrics.r2_score(val_y, result_se[\"pred\"])}\n",
    "print(result_se[\"performance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
