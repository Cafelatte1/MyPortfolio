{"cells":[{"cell_type":"markdown","source":["# LightGBM(GPU) Install"],"metadata":{"id":"AHNQiz11XWr6"},"id":"AHNQiz11XWr6"},{"cell_type":"code","source":["!git clone --recursive https://github.com/Microsoft/LightGBM\n","%cd /content/LightGBM\n","!mkdir build\n","!cmake -DUSE_GPU=1 #avoid ..\n","!make -j$(nproc)\n","!sudo apt-get -y install python-pip\n","!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n","%cd /content/LightGBM/python-package\n","!sudo python setup.py install --precompile"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QSf89gFCjMF","executionInfo":{"status":"ok","timestamp":1655353078569,"user_tz":-540,"elapsed":147714,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"d350e01e-03c3-4524-cddf-9c5ee845bd97"},"id":"0QSf89gFCjMF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'LightGBM'...\n","remote: Enumerating objects: 26285, done.\u001b[K\n","remote: Counting objects: 100% (86/86), done.\u001b[K\n","remote: Compressing objects: 100% (64/64), done.\u001b[K\n","remote: Total 26285 (delta 36), reused 56 (delta 21), pack-reused 26199\u001b[K\n","Receiving objects: 100% (26285/26285), 19.04 MiB | 16.72 MiB/s, done.\n","Resolving deltas: 100% (19418/19418), done.\n","Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'external_libs/compute'\n","Submodule 'eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'external_libs/eigen'\n","Submodule 'external_libs/fast_double_parser' (https://github.com/lemire/fast_double_parser.git) registered for path 'external_libs/fast_double_parser'\n","Submodule 'external_libs/fmt' (https://github.com/fmtlib/fmt.git) registered for path 'external_libs/fmt'\n","Cloning into '/content/LightGBM/external_libs/compute'...\n","remote: Enumerating objects: 21733, done.        \n","remote: Counting objects: 100% (5/5), done.        \n","remote: Compressing objects: 100% (5/5), done.        \n","remote: Total 21733 (delta 1), reused 2 (delta 0), pack-reused 21728        \n","Receiving objects: 100% (21733/21733), 8.51 MiB | 25.26 MiB/s, done.\n","Resolving deltas: 100% (17567/17567), done.\n","Cloning into '/content/LightGBM/external_libs/eigen'...\n","remote: Enumerating objects: 116187, done.        \n","remote: Counting objects: 100% (1257/1257), done.        \n","remote: Compressing objects: 100% (486/486), done.        \n","remote: Total 116187 (delta 798), reused 1167 (delta 771), pack-reused 114930        \n","Receiving objects: 100% (116187/116187), 103.28 MiB | 23.36 MiB/s, done.\n","Resolving deltas: 100% (95604/95604), done.\n","Cloning into '/content/LightGBM/external_libs/fast_double_parser'...\n","remote: Enumerating objects: 718, done.        \n","remote: Counting objects: 100% (117/117), done.        \n","remote: Compressing objects: 100% (26/26), done.        \n","remote: Total 718 (delta 95), reused 95 (delta 87), pack-reused 601        \n","Receiving objects: 100% (718/718), 814.83 KiB | 10.18 MiB/s, done.\n","Resolving deltas: 100% (366/366), done.\n","Cloning into '/content/LightGBM/external_libs/fmt'...\n","remote: Enumerating objects: 29793, done.        \n","remote: Counting objects: 100% (231/231), done.        \n","remote: Compressing objects: 100% (94/94), done.        \n","remote: Total 29793 (delta 129), reused 196 (delta 107), pack-reused 29562        \n","Receiving objects: 100% (29793/29793), 14.23 MiB | 20.52 MiB/s, done.\n","Resolving deltas: 100% (20083/20083), done.\n","Submodule path 'external_libs/compute': checked out '36350b7de849300bd3d72a05d8bf890ca405a014'\n","Submodule path 'external_libs/eigen': checked out '3147391d946bb4b6c68edd901f2add6ac1f31f8c'\n","Submodule path 'external_libs/fast_double_parser': checked out 'ace60646c02dc54c57f19d644e49a61e7e7758ec'\n","Submodule 'benchmark/dependencies/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'\n","Submodule 'benchmark/dependencies/double-conversion' (https://github.com/google/double-conversion.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'\n","Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'...\n","remote: Enumerating objects: 17244, done.        \n","remote: Counting objects: 100% (149/149), done.        \n","remote: Compressing objects: 100% (106/106), done.        \n","remote: Total 17244 (delta 59), reused 104 (delta 43), pack-reused 17095        \n","Receiving objects: 100% (17244/17244), 10.91 MiB | 14.82 MiB/s, done.\n","Resolving deltas: 100% (13369/13369), done.\n","Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'...\n","remote: Enumerating objects: 1338, done.        \n","remote: Counting objects: 100% (182/182), done.        \n","remote: Compressing objects: 100% (94/94), done.        \n","remote: Total 1338 (delta 97), reused 150 (delta 81), pack-reused 1156        \n","Receiving objects: 100% (1338/1338), 7.14 MiB | 2.88 MiB/s, done.\n","Resolving deltas: 100% (869/869), done.\n","Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp': checked out 'd936052d32a5b7ca08b0199a6724724aea432309'\n","Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion': checked out 'f4cb2384efa55dee0e6652f8674b05763441ab09'\n","Submodule path 'external_libs/fmt': checked out 'b6f4ceaed0a0a24ccf575fab6c56dd50ccf6f1a9'\n","/content/LightGBM\n","\u001b[33mCMake Warning:\n","  No source or binary directory provided.  Both will be assumed to be the\n","  same as the current working directory, but note that this warning will\n","  become a fatal error in future CMake releases.\n","\n","\u001b[0m\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n","-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n","-- Found OpenMP: TRUE (found version \"4.5\")  \n","-- Looking for CL_VERSION_2_2\n","-- Looking for CL_VERSION_2_2 - found\n","-- Found OpenCL: /usr/lib/x86_64-linux-gnu/libOpenCL.so (found version \"2.2\") \n","-- OpenCL include directory: /usr/include\n","-- Found Boost: /usr/include (found suitable version \"1.65.1\", minimum required is \"1.56.0\") found components: filesystem system \n","-- Performing Test MM_PREFETCH\n","-- Performing Test MM_PREFETCH - Success\n","-- Using _mm_prefetch\n","-- Performing Test MM_MALLOC\n","-- Performing Test MM_MALLOC - Success\n","-- Using _mm_malloc\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/LightGBM\n","[  2%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_capi_objs.dir/src/c_api.cpp.o\u001b[0m\n","[  4%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/boosting.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/cuda/cuda_utils.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/bin.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config_auto.cpp.o\u001b[0m\n","[ 20%] Built target lightgbm_capi_objs\n","[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_column_data.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_metadata.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_row_data.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_tree.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset_loader.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dense_bin.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/file_io.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/json11.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/metadata.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/multi_val_dense_bin.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/multi_val_sparse_bin.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/parser.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/sparse_bin.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/train_share_states.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/tree.cpp.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/metric.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linker_topo.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_socket.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/network.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/objective_function.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_best_split_finder.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_data_partition.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_histogram_constructor.cpp.o\u001b[0m\n","[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_leaf_splits.cpp.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_single_gpu_tree_learner.cpp.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda_tree_learner.cpp.o\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n","[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/linear_tree_learner.cpp.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n","[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n","[ 92%] Built target lightgbm_objs\n","[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n","[ 96%] \u001b[32m\u001b[1mLinking CXX shared library lib_lightgbm.so\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n","[ 98%] Built target _lightgbm\n","[100%] \u001b[32m\u001b[1mLinking CXX executable lightgbm\u001b[0m\n","[100%] Built target lightgbm\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'sudo apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  libpython-all-dev python-all python-all-dev python-asn1crypto\n","  python-cffi-backend python-crypto python-cryptography python-dbus\n","  python-enum34 python-gi python-idna python-ipaddress python-keyring\n","  python-keyrings.alt python-pip-whl python-pkg-resources python-secretstorage\n","  python-setuptools python-six python-wheel python-xdg\n","Suggested packages:\n","  python-crypto-doc python-cryptography-doc python-cryptography-vectors\n","  python-dbus-dbg python-dbus-doc python-enum34-doc python-gi-cairo\n","  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0 python-fs\n","  python-gdata python-keyczar python-secretstorage-doc python-setuptools-doc\n","The following NEW packages will be installed:\n","  libpython-all-dev python-all python-all-dev python-asn1crypto\n","  python-cffi-backend python-crypto python-cryptography python-dbus\n","  python-enum34 python-gi python-idna python-ipaddress python-keyring\n","  python-keyrings.alt python-pip python-pip-whl python-pkg-resources\n","  python-secretstorage python-setuptools python-six python-wheel python-xdg\n","0 upgraded, 22 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 3,430 kB of archives.\n","After this operation, 10.8 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpython-all-dev amd64 2.7.15~rc1-1 [1,092 B]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-all amd64 2.7.15~rc1-1 [1,076 B]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-all-dev amd64 2.7.15~rc1-1 [1,100 B]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-asn1crypto all 0.24.0-1 [72.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-cffi-backend amd64 1.11.5-1 [63.4 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-enum34 all 1.1.6-2 [34.8 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-idna all 2.6-1 [32.4 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-ipaddress all 1.0.17-1 [18.2 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-cryptography amd64 2.1.4-1ubuntu1.4 [276 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-dbus amd64 1.2.6-1 [90.2 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-gi amd64 3.26.1-2ubuntu1 [197 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-secretstorage all 2.3.1-2 [11.8 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-keyring all 10.6.0-1 [30.6 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-keyrings.alt all 3.0-1 [16.7 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.5 [1,653 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip all 9.0.1-2.3~ubuntu1.18.04.5 [151 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-setuptools all 39.0.1-2 [329 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-wheel all 0.30.0-0.2 [36.4 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-xdg all 0.25-4ubuntu1.1 [31.2 kB]\n","Fetched 3,430 kB in 1s (3,355 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 22.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libpython-all-dev:amd64.\n","(Reading database ... 155632 files and directories currently installed.)\n","Preparing to unpack .../00-libpython-all-dev_2.7.15~rc1-1_amd64.deb ...\n","Unpacking libpython-all-dev:amd64 (2.7.15~rc1-1) ...\n","Selecting previously unselected package python-all.\n","Preparing to unpack .../01-python-all_2.7.15~rc1-1_amd64.deb ...\n","Unpacking python-all (2.7.15~rc1-1) ...\n","Selecting previously unselected package python-all-dev.\n","Preparing to unpack .../02-python-all-dev_2.7.15~rc1-1_amd64.deb ...\n","Unpacking python-all-dev (2.7.15~rc1-1) ...\n","Selecting previously unselected package python-asn1crypto.\n","Preparing to unpack .../03-python-asn1crypto_0.24.0-1_all.deb ...\n","Unpacking python-asn1crypto (0.24.0-1) ...\n","Selecting previously unselected package python-cffi-backend.\n","Preparing to unpack .../04-python-cffi-backend_1.11.5-1_amd64.deb ...\n","Unpacking python-cffi-backend (1.11.5-1) ...\n","Selecting previously unselected package python-crypto.\n","Preparing to unpack .../05-python-crypto_2.6.1-8ubuntu2_amd64.deb ...\n","Unpacking python-crypto (2.6.1-8ubuntu2) ...\n","Selecting previously unselected package python-enum34.\n","Preparing to unpack .../06-python-enum34_1.1.6-2_all.deb ...\n","Unpacking python-enum34 (1.1.6-2) ...\n","Selecting previously unselected package python-idna.\n","Preparing to unpack .../07-python-idna_2.6-1_all.deb ...\n","Unpacking python-idna (2.6-1) ...\n","Selecting previously unselected package python-ipaddress.\n","Preparing to unpack .../08-python-ipaddress_1.0.17-1_all.deb ...\n","Unpacking python-ipaddress (1.0.17-1) ...\n","Selecting previously unselected package python-six.\n","Preparing to unpack .../09-python-six_1.11.0-2_all.deb ...\n","Unpacking python-six (1.11.0-2) ...\n","Selecting previously unselected package python-cryptography.\n","Preparing to unpack .../10-python-cryptography_2.1.4-1ubuntu1.4_amd64.deb ...\n","Unpacking python-cryptography (2.1.4-1ubuntu1.4) ...\n","Selecting previously unselected package python-dbus.\n","Preparing to unpack .../11-python-dbus_1.2.6-1_amd64.deb ...\n","Unpacking python-dbus (1.2.6-1) ...\n","Selecting previously unselected package python-gi.\n","Preparing to unpack .../12-python-gi_3.26.1-2ubuntu1_amd64.deb ...\n","Unpacking python-gi (3.26.1-2ubuntu1) ...\n","Selecting previously unselected package python-secretstorage.\n","Preparing to unpack .../13-python-secretstorage_2.3.1-2_all.deb ...\n","Unpacking python-secretstorage (2.3.1-2) ...\n","Selecting previously unselected package python-keyring.\n","Preparing to unpack .../14-python-keyring_10.6.0-1_all.deb ...\n","Unpacking python-keyring (10.6.0-1) ...\n","Selecting previously unselected package python-keyrings.alt.\n","Preparing to unpack .../15-python-keyrings.alt_3.0-1_all.deb ...\n","Unpacking python-keyrings.alt (3.0-1) ...\n","Selecting previously unselected package python-pip-whl.\n","Preparing to unpack .../16-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n","Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n","Selecting previously unselected package python-pip.\n","Preparing to unpack .../17-python-pip_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n","Unpacking python-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n","Selecting previously unselected package python-pkg-resources.\n","Preparing to unpack .../18-python-pkg-resources_39.0.1-2_all.deb ...\n","Unpacking python-pkg-resources (39.0.1-2) ...\n","Selecting previously unselected package python-setuptools.\n","Preparing to unpack .../19-python-setuptools_39.0.1-2_all.deb ...\n","Unpacking python-setuptools (39.0.1-2) ...\n","Selecting previously unselected package python-wheel.\n","Preparing to unpack .../20-python-wheel_0.30.0-0.2_all.deb ...\n","Unpacking python-wheel (0.30.0-0.2) ...\n","Selecting previously unselected package python-xdg.\n","Preparing to unpack .../21-python-xdg_0.25-4ubuntu1.1_all.deb ...\n","Unpacking python-xdg (0.25-4ubuntu1.1) ...\n","Setting up python-idna (2.6-1) ...\n","Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n","Setting up python-asn1crypto (0.24.0-1) ...\n","Setting up python-crypto (2.6.1-8ubuntu2) ...\n","Setting up python-wheel (0.30.0-0.2) ...\n","Setting up libpython-all-dev:amd64 (2.7.15~rc1-1) ...\n","Setting up python-pkg-resources (39.0.1-2) ...\n","Setting up python-cffi-backend (1.11.5-1) ...\n","Setting up python-gi (3.26.1-2ubuntu1) ...\n","Setting up python-six (1.11.0-2) ...\n","Setting up python-enum34 (1.1.6-2) ...\n","Setting up python-dbus (1.2.6-1) ...\n","Setting up python-ipaddress (1.0.17-1) ...\n","Setting up python-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n","Setting up python-all (2.7.15~rc1-1) ...\n","Setting up python-xdg (0.25-4ubuntu1.1) ...\n","Setting up python-setuptools (39.0.1-2) ...\n","Setting up python-keyrings.alt (3.0-1) ...\n","Setting up python-all-dev (2.7.15~rc1-1) ...\n","Setting up python-cryptography (2.1.4-1ubuntu1.4) ...\n","Setting up python-secretstorage (2.3.1-2) ...\n","Setting up python-keyring (10.6.0-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-62.4.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 13.2 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n","Collecting scipy\n","  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n","\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n","Installing collected packages: scipy, setuptools\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.7.3 setuptools-62.4.0\n","/content/LightGBM/python-package\n","running install\n","/usr/local/lib/python3.7/dist-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n","  setuptools.SetuptoolsDeprecationWarning,\n","running build\n","running build_py\n","creating build\n","creating build/lib\n","creating build/lib/lightgbm\n","copying lightgbm/dask.py -> build/lib/lightgbm\n","copying lightgbm/sklearn.py -> build/lib/lightgbm\n","copying lightgbm/libpath.py -> build/lib/lightgbm\n","copying lightgbm/__init__.py -> build/lib/lightgbm\n","copying lightgbm/callback.py -> build/lib/lightgbm\n","copying lightgbm/compat.py -> build/lib/lightgbm\n","copying lightgbm/plotting.py -> build/lib/lightgbm\n","copying lightgbm/basic.py -> build/lib/lightgbm\n","copying lightgbm/engine.py -> build/lib/lightgbm\n","running egg_info\n","creating lightgbm.egg-info\n","writing lightgbm.egg-info/PKG-INFO\n","writing dependency_links to lightgbm.egg-info/dependency_links.txt\n","writing requirements to lightgbm.egg-info/requires.txt\n","writing top-level names to lightgbm.egg-info/top_level.txt\n","writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n","reading manifest template 'MANIFEST.in'\n","no previously-included directories found matching 'build'\n","warning: no files found matching 'LICENSE'\n","warning: no files found matching '*.txt'\n","warning: no files found matching '*.so' under directory 'lightgbm'\n","warning: no files found matching 'compile/CMakeLists.txt'\n","warning: no files found matching 'compile/cmake/IntegratedOpenCL.cmake'\n","warning: no files found matching '*.so' under directory 'compile'\n","warning: no files found matching '*.dll' under directory 'compile/Release'\n","warning: no files found matching 'compile/external_libs/compute/CMakeLists.txt'\n","warning: no files found matching '*' under directory 'compile/external_libs/compute/cmake'\n","warning: no files found matching '*' under directory 'compile/external_libs/compute/include'\n","warning: no files found matching '*' under directory 'compile/external_libs/compute/meta'\n","warning: no files found matching 'compile/external_libs/eigen/CMakeLists.txt'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Cholesky'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Core'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Dense'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Eigenvalues'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Geometry'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Householder'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/Jacobi'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/LU'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/QR'\n","warning: no files found matching 'compile/external_libs/eigen/Eigen/SVD'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Cholesky'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Core'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Eigenvalues'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Geometry'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Householder'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Jacobi'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/LU'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/misc'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/plugins'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/QR'\n","warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/SVD'\n","warning: no files found matching 'compile/external_libs/fast_double_parser/CMakeLists.txt'\n","warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE'\n","warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE.BSL'\n","warning: no files found matching '*' under directory 'compile/external_libs/fast_double_parser/include'\n","warning: no files found matching 'compile/external_libs/fmt/CMakeLists.txt'\n","warning: no files found matching 'compile/external_libs/fmt/LICENSE.rst'\n","warning: no files found matching '*' under directory 'compile/external_libs/fmt/include'\n","warning: no files found matching '*' under directory 'compile/include'\n","warning: no files found matching '*' under directory 'compile/src'\n","warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n","warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n","warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n","warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n","warning: no previously-included files found matching 'compile/external_libs/compute/.git'\n","writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n","copying lightgbm/VERSION.txt -> build/lib/lightgbm\n","running install_lib\n","creating /usr/lib/python3.7/site-packages\n","creating /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/dask.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/sklearn.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/libpath.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/__init__.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/callback.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/compat.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/plotting.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/VERSION.txt -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/basic.py -> /usr/lib/python3.7/site-packages/lightgbm\n","copying build/lib/lightgbm/engine.py -> /usr/lib/python3.7/site-packages/lightgbm\n","INFO:LightGBM:Installing lib_lightgbm from: ['/content/LightGBM/lib_lightgbm.so']\n","copying /content/LightGBM/lib_lightgbm.so -> /usr/lib/python3.7/site-packages/lightgbm\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/dask.py to dask.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/sklearn.py to sklearn.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/libpath.py to libpath.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/__init__.py to __init__.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/callback.py to callback.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/compat.py to compat.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/plotting.py to plotting.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/basic.py to basic.cpython-37.pyc\n","byte-compiling /usr/lib/python3.7/site-packages/lightgbm/engine.py to engine.cpython-37.pyc\n","running install_egg_info\n","Copying lightgbm.egg-info to /usr/lib/python3.7/site-packages/lightgbm-3.3.2.99-py3.7.egg-info\n","running install_scripts\n"]}]},{"cell_type":"markdown","id":"4ffc0fb5-5918-4ed2-8374-0c6513a267a2","metadata":{"id":"4ffc0fb5-5918-4ed2-8374-0c6513a267a2"},"source":["# Setup"]},{"cell_type":"code","source":["import os\n","# python global seed\n","os.environ['PYTHONHASHSEED'] = str(42)\n","# tensorflow seed (not working for GPU)\n","# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n","# weight and bias\n","os.environ[\"WANDB_API_KEY\"] = \"b838b62906ab267778c6e05b913ba6c4a27699b2\""],"metadata":{"id":"tyVrEsPQ3XxM","executionInfo":{"status":"ok","timestamp":1655355398875,"user_tz":-540,"elapsed":423,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"id":"tyVrEsPQ3XxM","execution_count":14,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# # TextVectorization layer in tf 2.6 don't support \"sparse=True\" option which is used on TF-IDF\n","# # !pip install -q tensorflow==2.6.0\n","!pip install -q tensorflow==2.8\n","!pip install -q tensorflow-recommenders==0.6\n","# !pip install -q scann==1.2.3\n","# !pip install -q scann==1.2.6\n","# # !pip install -q tensorflow-datasets\n","!pip install -q tensorflow-addons\n","# !pip install -q tensorflow-hub\n","# # !pip install -q keras-tuner\n","\n","# !pip install -q transformers\n","\n","!pip install -q statsmodels\n","# !pip install -q xgboost\n","# !pip install -q lightgbm\n","# !pip install -q catboost\n","!pip install -q missingpy\n","\n","!pip install -q optuna\n","!pip install -q wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qkK1FnIA3c1R","executionInfo":{"status":"ok","timestamp":1655355426045,"user_tz":-540,"elapsed":26708,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"86bb2437-0768-421a-8c75-c5d8a1f354a8"},"id":"qkK1FnIA3c1R","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def config_missingpy():\n","    try:\n","        with open(\"/usr/local/lib/python3.7/site-packages/missingpy/knnimpute.py\", 'r') as file:\n","        # read a list of lines into data\n","            data = file.readlines()\n","            # now change the 2nd line, note that you have to add a newline\n","            data[12] = 'from sklearn.neighbors._base import _check_weights\\n'\n","            data[13] = 'from sklearn.neighbors._base import _get_weights\\n'\n","        # and write everything back\n","        with open(\"/usr/local/lib/python3.7/site-packages/missingpy/knnimpute.py\", 'w') as file:\n","            file.writelines(data)\n","    except:\n","        print(\"path error : /usr/local/lib/python3.7/site-packages/missingpy/knnimpute.py\")\n","    try:\n","        with open(\"/usr/local/lib/python3.7/dist-packages/missingpy/knnimpute.py\", 'r') as file:\n","        # read a list of lines into data\n","            data = file.readlines()\n","            # now change the 2nd line, note that you have to add a newline\n","            data[12] = 'from sklearn.neighbors._base import _check_weights\\n'\n","            data[13] = 'from sklearn.neighbors._base import _get_weights\\n'\n","        # and write everything back\n","        with open(\"/usr/local/lib/python3.7/dist-packages/missingpy/knnimpute.py\", 'w') as file:\n","            file.writelines(data)\n","    except:\n","        print(\"path error : /usr/local/lib/python3.7/site-packages/missingpy/knnimpute.py\")"],"metadata":{"id":"5BmL3Ic2aKS8","executionInfo":{"status":"ok","timestamp":1655355426046,"user_tz":-540,"elapsed":10,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"id":"5BmL3Ic2aKS8","execution_count":16,"outputs":[]},{"cell_type":"code","source":["import sys\n","import shutil\n","from glob import glob\n","import multiprocessing as mp\n","import gc\n","from pathlib import Path\n","from scipy import stats\n","from scipy.special import boxcox, softmax\n","from scipy import sparse\n","import itertools\n","\n","from multiprocessing import cpu_count\n","import copy\n","import pickle\n","import warnings\n","from datetime import datetime, timedelta\n","from time import time, sleep, mktime\n","from matplotlib import font_manager as fm, rc, rcParams\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","import re\n","import random as rnd\n","import psutil\n","from optuna import Trial, create_study\n","from optuna.samplers import TPESampler\n","import wandb\n","\n","import numpy as np\n","from numpy import array, nan, random as np_rnd, where\n","from numpy import dot\n","from numpy.linalg import norm\n","import pandas as pd\n","from pandas import DataFrame as dataframe, Series as series, isna, read_csv\n","from pandas.tseries.offsets import DateOffset\n","\n","from sklearn.model_selection import train_test_split as tts, StratifiedKFold, GroupKFold, GroupShuffleSplit, StratifiedGroupKFold\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, KBinsDiscretizer\n","from sklearn import metrics\n","# from sklearn.compose import ColumnTransformer\n","config_missingpy(); from missingpy import MissForest\n","# from sklearn.impute import KNNImputer\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# try:\n","#     # RAPIDS config\n","#     os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","#     os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","#     os.environ['CONDA_PREFIX'] = '/usr/local'\n","#     import cudf as cd\n","#     import cupy as cp\n","#     from cuml.cluster import KMeans\n","#     from cuml.neighbors import NearestNeighbors\n","#     from cuml.metrics.cluster import silhouette_score\n","# except:\n","#     print(\"RAPIDS Import ERROR\")\n","\n","# import xgboost as xgb\n","import lightgbm as lgb\n","# import catboost as cat\n","\n","# ===== tensorflow =====\n","import tensorflow as tf\n","from tensorflow import random as tf_rnd\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras import metrics as tf_metrics\n","from tensorflow.keras import callbacks as tf_callbacks\n","from tqdm.keras import TqdmCallback\n","import tensorflow_addons as tfa\n","from tensorflow.keras.utils import plot_model\n","from keras.utils.layer_utils import count_params\n","\n","# import keras_tuner as kt\n","# from keras_tuner import HyperModel\n","import tensorflow_hub as tf_hub\n","import tensorflow_recommenders as tfrs\n","\n","# # # ===== pytorch =====\n","# import torch\n","# from torch.utils.data import DataLoader\n","# from transformers import AutoTokenizer\n","# from transformers import AutoModel\n","\n","import librosa\n","\n","# GPU memory setting\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    tf.config.experimental.set_memory_growth(gpus[0], True)\n","  except RuntimeError as e:\n","    print(e)\n","\n","warnings.filterwarnings(action='ignore')\n","rcParams['axes.unicode_minus'] = False\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.width', 1000)\n","pd.set_option('max_colwidth', 200)\n","# plt.rc('font', family='NanumSquareB')"],"metadata":{"id":"k_JAH_Vj3jQ0","executionInfo":{"status":"ok","timestamp":1655355426590,"user_tz":-540,"elapsed":553,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02944b5f-3952-47a8-e025-f6afa3508a16"},"id":"k_JAH_Vj3jQ0","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["path error : /usr/local/lib/python3.7/site-packages/missingpy/knnimpute.py\n"]}]},{"cell_type":"code","source":["# ===== utility functions =====\n","# label encoding for categorical column with excepting na value\n","def seed_everything(seed=42):\n","    # python random module\n","    rnd.seed(seed)\n","    # numpy random\n","    np_rnd.seed(seed)\n","    # tf random\n","    try:\n","        tf_rnd.set_seed(seed)\n","    except:\n","        pass\n","    # RAPIDS random\n","    try:\n","        cp.random.seed(seed)\n","    except:\n","        pass\n","    # pytorch random\n","    try:\n","        torch.manual_seed(seed)\n","    except:\n","        pass\n","def which(bool_list):\n","    return where(bool_list)[0]\n","def easyIO(x=None, path=None, op=\"r\"):\n","    tmp = None\n","    if op == \"r\":\n","        with open(path, \"rb\") as f:\n","            tmp = pickle.load(f)\n","        return tmp\n","    elif op == \"w\":\n","        with open(path, \"wb\") as f:\n","            pickle.dump(x, f)\n","    else:\n","        print(\"Unknown operation type\")\n","def diff(first, second):\n","    second = set(second)\n","    return [item for item in first if item not in second]\n","def findIdx(data_x, col_names):\n","    return [int(i) for i, j in enumerate(data_x) if j in col_names]\n","def orderElems(for_order, using_ref):\n","    return [i for i in using_ref if i in for_order]\n","# concatenate by row\n","def cbr(df1, df2):\n","    if type(df1) == series:\n","        tmp_concat = series(pd.concat([dataframe(df1), dataframe(df2)], axis=0, ignore_index=True).iloc[:,0])\n","        tmp_concat.reset_index(drop=True, inplace=True)\n","    elif type(df1) == dataframe:\n","        tmp_concat = pd.concat([df1, df2], axis=0, ignore_index=True)\n","        tmp_concat.reset_index(drop=True, inplace=True)\n","    elif type(df1) == np.ndarray:\n","        tmp_concat = np.concatenate([df1, df2], axis=0)\n","    else:\n","        print(\"Unknown Type: return 1st argument\")\n","        tmp_concat = df1\n","    return tmp_concat\n","def change_width(ax, new_value):\n","    for patch in ax.patches :\n","        current_width = patch.get_width()\n","        adj_value = current_width - new_value\n","        # we change the bar width\n","        patch.set_width(new_value)\n","        # we recenter the bar\n","        patch.set_x(patch.get_x() + adj_value * .5)\n","def week_of_month(date):\n","    month = date.month\n","    week = 0\n","    while date.month == month:\n","        week += 1\n","        date -= timedelta(days=7)\n","    return week\n","def getSeason(date):\n","    month = date.month\n","    if month in [3, 4, 5]:\n","        return \"Spring\"\n","    elif month in [6, 7, 8]:\n","        return \"Summer\"\n","    elif month in [9, 10, 11]:\n","        return \"Fall\"\n","    else:\n","        return \"Winter\"\n","def createFolder(directory):\n","    try:\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","    except OSError:\n","        print('Error: Creating directory. ' + directory)\n","def sigmoid(x):\n","    return 1/(1 + np.exp(-x))\n","def dispPerformance(result_dic):\n","    perf_table = dataframe()\n","    index_names = []\n","    for k, v in result_dic.items():\n","        index_names.append(k)\n","        perf_table = pd.concat([perf_table, series(v[\"performance\"]).to_frame().T], ignore_index=True, axis=0)\n","    perf_table.index = index_names\n","    perf_table.sort_values(perf_table.columns[0], inplace=True)\n","    print(perf_table)\n","    return perf_table\n","def powspace(start, stop, power, num):\n","    start = np.power(start, 1/float(power))\n","    stop = np.power(stop, 1/float(power))\n","    return np.power(np.linspace(start, stop, num=num), power)\n","def xgb_custom_lossfunction(alpha = 1):\n","    def support_under_mse(label, pred):\n","        # grad : 1차 미분\n","        # hess : 2차 미분\n","        residual = (label - pred).astype(\"float\")\n","        grad = np.where(residual > 0, -2 * alpha * residual, -2 * residual)\n","        hess = np.where(residual > 0, 2 * alpha, 2.0)\n","        return grad, hess\n","    return support_under_mse\n","def pd_flatten(df):\n","    df = df.unstack()\n","    df.index = [str(i) + \"_\" + str(j) for i, j in df.index]\n","    return df\n","def tf_losses_rmse(y_true, y_pred, sample_weight=None):\n","    return tf.sqrt(tf.reduce_mean((y_true - y_pred) ** 2)) if sample_weight is None else tf.sqrt(tf.reduce_mean(((y_true - y_pred) ** 2) * sample_weight))\n","def tf_loss_nmae(y_true, y_pred, sample_weight=False):\n","    mae = tf.reduce_mean(tf.math.abs(y_true - y_pred))\n","    score = tf.math.divide(mae, tf.reduce_mean(tf.math.abs(y_true)))\n","    return score\n","def text_extractor(string, lang=\"eng\", spacing=True):\n","    # # 괄호를 포함한 괄호 안 문자 제거 정규식\n","    # re.sub(r'\\([^)]*\\)', '', remove_text)\n","    # # <>를 포함한 <> 안 문자 제거 정규식\n","    # re.sub(r'\\<[^)]*\\>', '', remove_text)\n","    if lang == \"eng\":\n","        text_finder = re.compile('[^ A-Za-z]') if spacing else re.compile('[^A-Za-z]')\n","    elif lang == \"kor\":\n","        text_finder = re.compile('[^ ㄱ-ㅣ가-힣+]') if spacing else re.compile('[^ㄱ-ㅣ가-힣+]')\n","    # default : kor + eng\n","    else:\n","        text_finder = re.compile('[^ A-Za-zㄱ-ㅣ가-힣+]') if spacing else re.compile('[^A-Za-zㄱ-ㅣ가-힣+]')\n","    return text_finder.sub('', string)\n","def memory_usage(message='debug'):\n","    # current process RAM usage\n","    p = psutil.Process()\n","    rss = p.memory_info().rss / 2 ** 20 # Bytes to MB\n","    print(f\"[{message}] memory usage: {rss: 10.3f} MB\")\n","    return rss\n","def cos_sim(a, b):\n","  return dot(a, b)/(norm(a) * norm(b))\n","class MyLabelEncoder:\n","    def __init__(self, preset={}):\n","        # dic_cat format -> {\"col_name\": {\"value\": replace}}\n","        self.dic_cat = preset\n","    def fit_transform(self, data_x, col_names):\n","        tmp_x = copy.deepcopy(data_x)\n","        for i in col_names:\n","            # if key is not in dic, update dic\n","            if i not in self.dic_cat.keys():\n","                tmp_dic = dict.fromkeys(sorted(set(tmp_x[i]).difference([nan])))\n","                label_cnt = 0\n","                for j in tmp_dic.keys():\n","                    tmp_dic[j] = label_cnt\n","                    label_cnt += 1\n","                self.dic_cat[i] = tmp_dic\n","            # transform value which is not in dic to nan\n","            tmp_x[i] = tmp_x[i].astype(\"object\")\n","            conv = tmp_x[i].replace(self.dic_cat[i])\n","            for conv_idx, j in enumerate(conv):\n","                if j not in self.dic_cat[i].values():\n","                    conv[conv_idx] = nan\n","            # final return\n","            tmp_x[i] = conv.astype(\"float\")\n","        return tmp_x\n","    def transform(self, data_x):\n","        tmp_x = copy.deepcopy(data_x)\n","        for i in self.dic_cat.keys():\n","            # transform value which is not in dic to nan\n","            tmp_x[i] = tmp_x[i].astype(\"object\")\n","            conv = tmp_x[i].replace(self.dic_cat[i])\n","            for conv_idx, j in enumerate(conv):\n","                if j not in self.dic_cat[i].values():\n","                    conv[conv_idx] = nan\n","            # final return\n","            tmp_x[i] = conv.astype(\"float\")\n","        return tmp_x\n","    def clear(self):\n","        self.dic_cat = {}\n","class MyOneHotEncoder:\n","    def __init__(self, label_preset={}):\n","        self.dic_cat = {}\n","        self.label_preset = label_preset\n","    def fit_transform(self, data_x, col_names):\n","        tmp_x = dataframe()\n","        for i in data_x:\n","            if i not in col_names:\n","                tmp_x = pd.concat([tmp_x, dataframe(data_x[i])], axis=1)\n","            else:\n","                if not ((data_x[i].dtype.name == \"object\") or (data_x[i].dtype.name == \"category\")):\n","                    print(F\"WARNING : {i} is not object or category\")\n","                self.dic_cat[i] = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n","                conv = self.dic_cat[i].fit_transform(dataframe(data_x[i])).astype(\"int\")\n","                col_list = []\n","                for j in self.dic_cat[i].categories_[0]:\n","                    if i in self.label_preset.keys():\n","                        for k, v in self.label_preset[i].items():\n","                            if v == j:\n","                                col_list.append(str(i) + \"_\" + str(k))\n","                    else:\n","                        col_list.append(str(i) + \"_\" + str(j))\n","                conv = dataframe(conv, columns=col_list)\n","                tmp_x = pd.concat([tmp_x, conv], axis=1)\n","        return tmp_x\n","    def transform(self, data_x):\n","        tmp_x = dataframe()\n","        for i in data_x:\n","            if not i in list(self.dic_cat.keys()):\n","                tmp_x = pd.concat([tmp_x, dataframe(data_x[i])], axis=1)\n","            else:\n","                if not ((data_x[i].dtype.name == \"object\") or (data_x[i].dtype.name == \"category\")):\n","                    print(F\"WARNING : {i} is not object or category\")\n","                conv = self.dic_cat[i].transform(dataframe(data_x[i])).astype(\"int\")\n","                col_list = []\n","                for j in self.dic_cat[i].categories_[0]:\n","                    if i in self.label_preset.keys():\n","                        for k, v in self.label_preset[i].items():\n","                            if v == j: col_list.append(str(i) + \"_\" + str(k))\n","                    else:\n","                        col_list.append(str(i) + \"_\" + str(j))\n","                conv = dataframe(conv, columns=col_list)\n","                tmp_x = pd.concat([tmp_x, conv], axis=1)\n","        return tmp_x\n","    def clear(self):\n","        self.dic_cat = {}\n","        self.label_preset = {}\n","class MyKNNImputer:\n","    def __init__(self, k=5):\n","        self.imputer = KNNImputer(n_neighbors=k)\n","        self.dic_cat = {}\n","    def fit_transform(self, x, cat_vars=None):\n","        if cat_vars is None:\n","            x_imp = dataframe(self.imputer.fit_transform(x), columns=x.columns)\n","        else:\n","            naIdx = dict.fromkeys(cat_vars)\n","            for i in cat_vars:\n","                self.dic_cat[i] = diff(list(sorted(set(x[i]))), [nan])\n","                naIdx[i] = list(which(array(x[i].isna())))\n","            x_imp = dataframe(self.imputer.fit_transform(x), columns=x.columns)\n","\n","            # if imputed categorical value are not in the range, adjust the value\n","            for i in cat_vars:\n","                x_imp[i] = x_imp[i].apply(lambda x: int(round(x, 0)))\n","                for j in naIdx[i]:\n","                    if x_imp[i][j] not in self.dic_cat[i]:\n","                        if x_imp[i][j] < self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][0]\n","                        elif x_imp[i][j] > self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][len(self.dic_cat[i]) - 1]\n","        return x_imp\n","    def transform(self, x):\n","        if len(self.dic_cat.keys()) == 0:\n","            x_imp = dataframe(self.imputer.transform(x), columns=x.columns)\n","        else:\n","            naIdx = dict.fromkeys(self.dic_cat.keys())\n","            for i in self.dic_cat.keys():\n","                naIdx[i] = list(which(array(x[i].isna())))\n","            x_imp = dataframe(self.imputer.transform(x), columns=x.columns)\n","\n","            # if imputed categorical value are not in the range, adjust the value\n","            for i in self.dic_cat.keys():\n","                x_imp[i] = x_imp[i].apply(lambda x: int(round(x, 0)))\n","                for j in naIdx[i]:\n","                    if x_imp[i][j] not in self.dic_cat[i]:\n","                        if x_imp[i][j] < self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][0]\n","                        elif x_imp[i][j] > self.dic_cat[i][0]:\n","                            x_imp[i][naIdx[i]] = self.dic_cat[i][len(self.dic_cat[i]) - 1]\n","        return x_imp\n","    def clear(self):\n","        self.imputer = None\n","        self.dic_cat = {}\n","def remove_outlier(df, std=3, mode=\"remove\"):\n","    tmp_df = df.copy()\n","    if mode == \"remove\":\n","        outlier_mask = (np.abs(stats.zscore(tmp_df)) > std).all(axis=1)\n","        print(\"found outlier :\", outlier_mask.sum())\n","        tmp_df = tmp_df[~outlier_mask]\n","    elif mode == \"interpolate\":\n","        tmp_outlier = []\n","        for i in tmp_df:\n","            outlier_mask = (np.abs(stats.zscore(tmp_df[i])) > std)\n","            tmp_outlier.append(outlier_mask.sum())\n","            if tmp_outlier[-1] == 0:\n","                continue\n","            tmp_df[i][outlier_mask] = np.nan\n","            tmp_df[i] = tmp_df[i].interpolate(method='linear').bfill()\n","        print(\"found outlier :\", np.sum(outlier_mask))\n","    return tmp_df\n","def convert_sparse_matrix_to_sparse_tensor(X, sorted=True):\n","    coo = X.tocoo()\n","    indices = np.mat([coo.row, coo.col]).transpose()\n","    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape)) if sorted else tf.SparseTensor(indices, coo.data, coo.shape)\n","seed_everything()"],"metadata":{"id":"U6nKyyOj33UL","executionInfo":{"status":"ok","timestamp":1655355427429,"user_tz":-540,"elapsed":841,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"id":"U6nKyyOj33UL","execution_count":18,"outputs":[]},{"cell_type":"code","source":["folder_path = \"/content/drive/MyDrive/Colab Notebooks/projects/Dacon/covid19_diagnostics/\""],"metadata":{"id":"WRUzOxHA_y-2","executionInfo":{"status":"ok","timestamp":1655355427430,"user_tz":-540,"elapsed":11,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"id":"WRUzOxHA_y-2","execution_count":19,"outputs":[]},{"cell_type":"markdown","id":"72d57a51-dc27-4af5-9261-91f61194d975","metadata":{"id":"72d57a51-dc27-4af5-9261-91f61194d975"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":20,"id":"0da03dd7-888b-44f2-80ae-cb28fd091de3","metadata":{"id":"0da03dd7-888b-44f2-80ae-cb28fd091de3","executionInfo":{"status":"ok","timestamp":1655355427431,"user_tz":-540,"elapsed":11,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"outputs":[],"source":["df_full = pd.read_csv(folder_path + 'open.zip (Unzipped Files)/train_data.csv')\n","df_test = pd.read_csv(folder_path + 'open.zip (Unzipped Files)/test_data.csv')"]},{"cell_type":"code","source":["df_full.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"mNLadPhF8FRK","executionInfo":{"status":"ok","timestamp":1655355427431,"user_tz":-540,"elapsed":11,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"be4a0bc5-e4a3-4c60-ea20-d882456e9209"},"id":"mNLadPhF8FRK","execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id  age  gender  respiratory_condition  fever_or_muscle_pain  covid19\n","0   1   24  female                      0                     1        0\n","1   2   51    male                      0                     0        0\n","2   3   22    male                      0                     0        0\n","3   4   29  female                      1                     0        0\n","4   5   23    male                      0                     0        0"],"text/html":["\n","  <div id=\"df-332b3659-fc65-40aa-9f3d-d1186eb599ce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>respiratory_condition</th>\n","      <th>fever_or_muscle_pain</th>\n","      <th>covid19</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>51</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>22</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>29</td>\n","      <td>female</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>23</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-332b3659-fc65-40aa-9f3d-d1186eb599ce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-332b3659-fc65-40aa-9f3d-d1186eb599ce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-332b3659-fc65-40aa-9f3d-d1186eb599ce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["df_test.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Y2rZb93e8GJO","executionInfo":{"status":"ok","timestamp":1655355427433,"user_tz":-540,"elapsed":11,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"574b2074-fb7f-4255-e2ea-363571239664"},"id":"Y2rZb93e8GJO","execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     id  age  gender  respiratory_condition  fever_or_muscle_pain\n","0  3806   48  female                      1                     0\n","1  3807   24  female                      0                     0\n","2  3808   29    male                      0                     0\n","3  3809   39  female                      0                     0\n","4  3810   34    male                      0                     0"],"text/html":["\n","  <div id=\"df-329acad4-a09d-4d36-8f5e-fe43b418303c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>respiratory_condition</th>\n","      <th>fever_or_muscle_pain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3806</td>\n","      <td>48</td>\n","      <td>female</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3807</td>\n","      <td>24</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3808</td>\n","      <td>29</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3809</td>\n","      <td>39</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3810</td>\n","      <td>34</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-329acad4-a09d-4d36-8f5e-fe43b418303c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-329acad4-a09d-4d36-8f5e-fe43b418303c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-329acad4-a09d-4d36-8f5e-fe43b418303c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":23,"id":"9601f0b0-6d28-48de-88d3-3c68d43ad2c0","metadata":{"id":"9601f0b0-6d28-48de-88d3-3c68d43ad2c0","executionInfo":{"status":"ok","timestamp":1655355427433,"user_tz":-540,"elapsed":11,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"outputs":[],"source":["def get_mfcc_feature(df, data_type):\n","    # Data Folder path\n","    root_folder = folder_path + 'open.zip (Unzipped Files)/'\n","    features = []\n","    for uid in tqdm(df['id']):\n","        root_path = os.path.join(root_folder, data_type)\n","        path = os.path.join(root_path, str(uid).zfill(5)+'.wav')\n","\n","        # librosa패키지를 사용하여 wav 파일 load\n","        y, sr = librosa.load(path, sr=CFG['SR'])\n","        \n","        # librosa패키지를 사용하여 mfcc 추출\n","        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CFG['N_MFCC'])\n","\n","        y_feature = []\n","        # 추출된 MFCC들의 평균을 Feature로 사용\n","        for e in mfcc:\n","            y_feature.append(np.mean(e))\n","        features.append(y_feature)\n","    \n","    # 기존의 자가진단 정보를 담은 데이터프레임에 추출된 오디오 Feature를 추가\n","    mfcc_df = pd.DataFrame(features, columns=['mfcc_'+str(x) for x in range(1,CFG['N_MFCC']+1)])\n","    df = pd.concat([df, mfcc_df], axis=1)\n","    return df\n","    print('Done.')"]},{"cell_type":"code","source":["def get_vggish_feature(df, data_type):\n","    pretrained_vggish = tf_hub.load('https://tfhub.dev/google/vggish/1')\n","    avg_pooling = layers.GlobalAveragePooling1D()\n","\n","    # Data Folder path\n","    root_folder = folder_path + 'open.zip (Unzipped Files)/'\n","    features = []\n","    for uid in tqdm(df['id']):\n","        root_path = os.path.join(root_folder, data_type)\n","        path = os.path.join(root_path, str(uid).zfill(5)+'.wav')\n","\n","        # librosa패키지를 사용하여 wav 파일 load\n","        # sampling rate : 16000 (16kHz)\n","        y, sr = librosa.load(path, sr=16000)\n","\n","        # getting averaged vggish feature\n","        y = pretrained_vggish(y)\n","        features.append(avg_pooling(tf.expand_dims(y, 0)).numpy()[0])\n","    \n","    # 기존의 자가진단 정보를 담은 데이터프레임에 추출된 오디오 Feature를 추가\n","    feature_df = pd.DataFrame(features, columns=['vggish_' + str(x) for x in range(128)])\n","    if feature_df.isna().sum().sum() > 0:\n","        print(\"INFO : df includes na values\")\n","    df = pd.concat([df, feature_df], axis=1)\n","    return df\n","    print('Done.')"],"metadata":{"id":"eMJ4olxTRU-T","executionInfo":{"status":"ok","timestamp":1655355427434,"user_tz":-540,"elapsed":11,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"id":"eMJ4olxTRU-T","execution_count":24,"outputs":[]},{"cell_type":"code","source":["CFG = {\n","    'SR':16000,\n","    'N_MFCC':64, # MFCC 벡터를 추출할 개수\n","    'SEED':41\n","}\n","\n","df_full = get_mfcc_feature(df_full, 'train')\n","df_test = get_mfcc_feature(df_test, 'test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsH_37d92jVC","executionInfo":{"status":"ok","timestamp":1655364876623,"user_tz":-540,"elapsed":9449200,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"534e1e3b-16c3-4019-aab2-97115c78e052"},"id":"YsH_37d92jVC","execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3805/3805 [1:04:44<00:00,  1.02s/it]\n","100%|██████████| 5732/5732 [1:32:44<00:00,  1.03it/s]\n"]}]},{"cell_type":"code","source":["easyIO(df_full, folder_path + \"dataset/train_mfcc_64.csv\", \"w\")\n","easyIO(df_test, folder_path + \"dataset/test_mfcc_64.csv\", \"w\")"],"metadata":{"id":"5PIfeD-H2j6i","executionInfo":{"status":"ok","timestamp":1655364877154,"user_tz":-540,"elapsed":534,"user":{"displayName":"김영준","userId":"06606532799291918175"}}},"id":"5PIfeD-H2j6i","execution_count":27,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0fe5713d-4bff-4598-bbfd-5cfb1b3109ac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fe5713d-4bff-4598-bbfd-5cfb1b3109ac","executionInfo":{"status":"ok","timestamp":1655217460602,"user_tz":-540,"elapsed":11720454,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"41ee4d6a-c42c-4635-90ed-2637c43fa11e"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3805/3805 [1:15:31<00:00,  1.19s/it]\n","100%|██████████| 5732/5732 [1:59:49<00:00,  1.25s/it]\n"]}],"source":["# df_full = get_mfcc_feature(df_full, 'train')\n","# df_test = get_mfcc_feature(df_test, 'test')"]},{"cell_type":"code","source":["CFG = {\n","    'SR':16000,\n","    'N_MFCC':32, # MFCC 벡터를 추출할 개수\n","    'SEED':41\n","}\n","\n","# easyIO(df_full, folder_path + \"dataset/train_mfcc.csv\", \"w\")\n","# easyIO(df_test, folder_path + \"dataset/test_mfcc.csv\", \"w\")"],"metadata":{"id":"FI_6x4v0IhaQ"},"id":"FI_6x4v0IhaQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_full = get_vggish_feature(df_full, 'train')\n","# df_test = get_vggish_feature(df_test, 'test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7psvO9tUPyU","executionInfo":{"status":"ok","timestamp":1655347373185,"user_tz":-540,"elapsed":6073131,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"fe7373fe-6135-49f1-f1c9-d5b1f1a63406"},"id":"-7psvO9tUPyU","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5732/5732 [1:41:12<00:00,  1.06s/it]\n"]},{"output_type":"stream","name":"stdout","text":["INFO : df includes na values\n"]}]},{"cell_type":"code","source":["# easyIO(df_full, folder_path + \"dataset/train_vggish.csv\", \"w\")\n","# easyIO(df_test, folder_path + \"dataset/test_vggish.csv\", \"w\")"],"metadata":{"id":"rCNMTLnhUQOZ"},"id":"rCNMTLnhUQOZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c998e196-6a3b-4214-8495-f026c015f8c5","metadata":{"id":"c998e196-6a3b-4214-8495-f026c015f8c5"},"source":["# Quick Start Session"]},{"cell_type":"code","source":["# audio_feature_type = \"mfcc\"\n","audio_feature_type = \"vggish\""],"metadata":{"id":"oc_yLvvmlROe"},"id":"oc_yLvvmlROe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_var = \"covid19\"\n","num_vars = [\"age\"]\n","bin_vars = [\"gender\", \"respiratory_condition\", \"fever_or_muscle_pain\"]\n","cat_vars = []\n","\n","if audio_feature_type == \"vggish\":\n","    audio_vars = [audio_feature_type + \"_\" + str(i) for i in range(128)]\n","elif audio_feature_type == \"mfcc\":\n","    audio_vars = [audio_feature_type + \"_\" + str(i) for i in range(1,CFG['N_MFCC']+1)]"],"metadata":{"id":"4mYZL_9BEVSx"},"id":"4mYZL_9BEVSx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_full = easyIO(None, folder_path + \"dataset/train_\" + audio_feature_type + \".csv\", \"r\")\n","df_test = easyIO(None, folder_path + \"dataset/test_\" + audio_feature_type + \".csv\", \"r\")"],"metadata":{"id":"iXlH3W9Q-9jq"},"id":"iXlH3W9Q-9jq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_full.drop(\"id\", axis=1, inplace=True)\n","df_test.drop(\"id\", axis=1, inplace=True)"],"metadata":{"id":"Ba0jbJxQQ4V9"},"id":"Ba0jbJxQQ4V9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_full.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYMrAoW5Gh7A","executionInfo":{"status":"ok","timestamp":1655353216555,"user_tz":-540,"elapsed":4,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"afa695db-0571-49d2-d3f9-684d758c1dd0"},"id":"wYMrAoW5Gh7A","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3805 entries, 0 to 3804\n","Columns: 133 entries, age to vggish_127\n","dtypes: float32(128), int64(4), object(1)\n","memory usage: 2.0+ MB\n"]}]},{"cell_type":"code","source":["df_test.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wo0owegZGj17","executionInfo":{"status":"ok","timestamp":1655353216555,"user_tz":-540,"elapsed":3,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"ef71610d-6e62-4db5-bdfb-012966cc5bf5"},"id":"wo0owegZGj17","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5732 entries, 0 to 5731\n","Columns: 132 entries, age to vggish_127\n","dtypes: float32(128), int64(3), object(1)\n","memory usage: 3.0+ MB\n"]}]},{"cell_type":"markdown","source":["# Feature Enginnering"],"metadata":{"id":"qfIQDoMKKgfj"},"id":"qfIQDoMKKgfj"},{"cell_type":"code","source":["# for i in df_full:\n","#     print(df_full[i].value_counts(), \"\\n\")"],"metadata":{"id":"lWuGdYxBEmYz"},"id":"lWuGdYxBEmYz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp = []\n","for i in df_full[\"gender\"]:\n","    if i == \"male\":\n","        tmp.append(1.0)\n","    elif i == \"female\":\n","        tmp.append(0.0)\n","    else:\n","        tmp.append(nan)\n","df_full[\"gender\"] = tmp\n","df_full = df_full.dropna().reset_index(drop=True)\n","\n","tmp = []\n","for i in df_test[\"gender\"]:\n","    if i == \"male\":\n","        tmp.append(1.0)\n","    elif i == \"female\":\n","        tmp.append(0.0)\n","    else:\n","        tmp.append(0.5)\n","df_test[\"gender\"] = tmp"],"metadata":{"id":"F6C4ot43Kj75"},"id":"F6C4ot43Kj75","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# age discretization for 8, 4 groups\n","\n","df_full[\"age_disc_8\"] = pd.cut(df_full[\"age\"], bins=[-np.inf, 10, 20, 30, 40, 50, 60, 70, np.inf], right=False, labels=list(range(8))).astype(\"float32\")\n","df_full[\"age_disc_4\"] = pd.cut(df_full[\"age\"], bins=[-np.inf, 20, 40, 60, np.inf], right=False, labels=list(range(4))).astype(\"float32\")\n","\n","df_test[\"age_disc_8\"] = pd.cut(df_test[\"age\"], bins=[-np.inf, 10, 20, 30, 40, 50, 60, 70, np.inf], right=False, labels=list(range(8))).astype(\"float32\")\n","df_test[\"age_disc_4\"] = pd.cut(df_test[\"age\"], bins=[-np.inf, 20, 40, 60, np.inf], right=False, labels=list(range(4))).astype(\"float32\")\n","\n","df_full = pd.concat([df_full.drop(\"age_disc_8\", axis=1), dataframe(tf.keras.utils.to_categorical(df_full[\"age_disc_8\"], num_classes=8, dtype='float32'), columns=[\"age_disc8_\" + str(i) for i in range(8)])], axis=1)\n","df_full = pd.concat([df_full.drop(\"age_disc_4\", axis=1), dataframe(tf.keras.utils.to_categorical(df_full[\"age_disc_4\"], num_classes=4, dtype='float32'), columns=[\"age_disc4_\" + str(i) for i in range(4)])], axis=1)\n","\n","df_test = pd.concat([df_test.drop(\"age_disc_8\", axis=1), dataframe(tf.keras.utils.to_categorical(df_test[\"age_disc_8\"], num_classes=8, dtype='float32'), columns=[\"age_disc8_\" + str(i) for i in range(8)])], axis=1)\n","df_test = pd.concat([df_test.drop(\"age_disc_4\", axis=1), dataframe(tf.keras.utils.to_categorical(df_test[\"age_disc_4\"], num_classes=4, dtype='float32'), columns=[\"age_disc4_\" + str(i) for i in range(4)])], axis=1)"],"metadata":{"id":"Un9YpSsKGydP"},"id":"Un9YpSsKGydP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# covid19 symptom interaction (2:both, 1:either, 0:neither)\n","\n","df_full[\"symptom_class\"] = (df_full[\"respiratory_condition\"] + df_full[\"fever_or_muscle_pain\"]).astype(\"float32\")\n","df_test[\"symptom_class\"] = (df_test[\"respiratory_condition\"] + df_test[\"fever_or_muscle_pain\"]).astype(\"float32\")\n","\n","df_full = pd.concat([df_full.drop(\"symptom_class\", axis=1), dataframe(tf.keras.utils.to_categorical(df_full[\"symptom_class\"], num_classes=3, dtype='float32'), columns=[\"symptom_class_\" + str(i) for i in range(3)])], axis=1)\n","df_test = pd.concat([df_test.drop(\"symptom_class\", axis=1), dataframe(tf.keras.utils.to_categorical(df_test[\"symptom_class\"], num_classes=3, dtype='float32'), columns=[\"symptom_class_\" + str(i) for i in range(3)])], axis=1)"],"metadata":{"id":"NssUtrMDhGkK"},"id":"NssUtrMDhGkK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vulnerable class (age>=60 and symptom_class==2)\n","\n","df_full[\"vulnerable\"] = (df_full[\"symptom_class_2\"].astype(\"bool\") & df_full[\"age_disc4_3\"].astype(\"bool\")).astype(\"float32\")\n","df_test[\"vulnerable\"] = (df_test[\"symptom_class_2\"].astype(\"bool\") & df_test[\"age_disc4_3\"].astype(\"bool\")).astype(\"float32\")\n","\n","bin_vars += [\"vulnerable\"]"],"metadata":{"id":"ymOq8KCoADWC"},"id":"ymOq8KCoADWC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_vars = diff(df_full.columns, bin_vars + cat_vars + audio_vars + [target_var])"],"metadata":{"id":"EDUHu-2KOhuB"},"id":"EDUHu-2KOhuB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_full[num_vars + bin_vars + audio_vars] = df_full[num_vars + bin_vars + audio_vars].astype(\"float32\")\n","df_full[target_var] = df_full[target_var].astype(\"int32\")\n","\n","df_test[num_vars + bin_vars + audio_vars] = df_test[num_vars + bin_vars + audio_vars].astype(\"float32\")"],"metadata":{"id":"Jbqap4i3AbQP"},"id":"Jbqap4i3AbQP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_vars"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fvnscfbKbA2","executionInfo":{"status":"ok","timestamp":1655353217086,"user_tz":-540,"elapsed":7,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"ccff5604-bd1d-4e6d-cefd-631cb2552ae2"},"id":"9fvnscfbKbA2","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['age',\n"," 'age_disc8_0',\n"," 'age_disc8_1',\n"," 'age_disc8_2',\n"," 'age_disc8_3',\n"," 'age_disc8_4',\n"," 'age_disc8_5',\n"," 'age_disc8_6',\n"," 'age_disc8_7',\n"," 'age_disc4_0',\n"," 'age_disc4_1',\n"," 'age_disc4_2',\n"," 'age_disc4_3',\n"," 'symptom_class_0',\n"," 'symptom_class_1',\n"," 'symptom_class_2']"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["bin_vars"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_XX5lBHKa-m","executionInfo":{"status":"ok","timestamp":1655353217086,"user_tz":-540,"elapsed":6,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"37f5a2dd-10f9-47a9-fb1c-4b9304c9c599"},"id":"5_XX5lBHKa-m","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['gender', 'respiratory_condition', 'fever_or_muscle_pain', 'vulnerable']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["bin_vars += num_vars[1:]\n","num_vars = [\"age\"]"],"metadata":{"id":"3ZdPvopCokhp"},"id":"3ZdPvopCokhp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["bin_vars"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mwGzA5VOovPp","executionInfo":{"status":"ok","timestamp":1655353217087,"user_tz":-540,"elapsed":7,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"876fcc40-1e0b-415f-e14a-b6eb7cc70f42"},"id":"mwGzA5VOovPp","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['gender',\n"," 'respiratory_condition',\n"," 'fever_or_muscle_pain',\n"," 'vulnerable',\n"," 'age_disc8_0',\n"," 'age_disc8_1',\n"," 'age_disc8_2',\n"," 'age_disc8_3',\n"," 'age_disc8_4',\n"," 'age_disc8_5',\n"," 'age_disc8_6',\n"," 'age_disc8_7',\n"," 'age_disc4_0',\n"," 'age_disc4_1',\n"," 'age_disc4_2',\n"," 'age_disc4_3',\n"," 'symptom_class_0',\n"," 'symptom_class_1',\n"," 'symptom_class_2']"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["cat_vars"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCUHq45SPAO_","executionInfo":{"status":"ok","timestamp":1655353217087,"user_tz":-540,"elapsed":6,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"f8c0fe4c-880e-4462-ac8a-a81fb95a7e03"},"id":"SCUHq45SPAO_","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["audio_vars"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8LJUK8PPCBF","executionInfo":{"status":"ok","timestamp":1655353217087,"user_tz":-540,"elapsed":6,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"a03cedd3-c72d-4ab7-db16-5396e3db3a12"},"id":"E8LJUK8PPCBF","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['vggish_0',\n"," 'vggish_1',\n"," 'vggish_2',\n"," 'vggish_3',\n"," 'vggish_4',\n"," 'vggish_5',\n"," 'vggish_6',\n"," 'vggish_7',\n"," 'vggish_8',\n"," 'vggish_9',\n"," 'vggish_10',\n"," 'vggish_11',\n"," 'vggish_12',\n"," 'vggish_13',\n"," 'vggish_14',\n"," 'vggish_15',\n"," 'vggish_16',\n"," 'vggish_17',\n"," 'vggish_18',\n"," 'vggish_19',\n"," 'vggish_20',\n"," 'vggish_21',\n"," 'vggish_22',\n"," 'vggish_23',\n"," 'vggish_24',\n"," 'vggish_25',\n"," 'vggish_26',\n"," 'vggish_27',\n"," 'vggish_28',\n"," 'vggish_29',\n"," 'vggish_30',\n"," 'vggish_31',\n"," 'vggish_32',\n"," 'vggish_33',\n"," 'vggish_34',\n"," 'vggish_35',\n"," 'vggish_36',\n"," 'vggish_37',\n"," 'vggish_38',\n"," 'vggish_39',\n"," 'vggish_40',\n"," 'vggish_41',\n"," 'vggish_42',\n"," 'vggish_43',\n"," 'vggish_44',\n"," 'vggish_45',\n"," 'vggish_46',\n"," 'vggish_47',\n"," 'vggish_48',\n"," 'vggish_49',\n"," 'vggish_50',\n"," 'vggish_51',\n"," 'vggish_52',\n"," 'vggish_53',\n"," 'vggish_54',\n"," 'vggish_55',\n"," 'vggish_56',\n"," 'vggish_57',\n"," 'vggish_58',\n"," 'vggish_59',\n"," 'vggish_60',\n"," 'vggish_61',\n"," 'vggish_62',\n"," 'vggish_63',\n"," 'vggish_64',\n"," 'vggish_65',\n"," 'vggish_66',\n"," 'vggish_67',\n"," 'vggish_68',\n"," 'vggish_69',\n"," 'vggish_70',\n"," 'vggish_71',\n"," 'vggish_72',\n"," 'vggish_73',\n"," 'vggish_74',\n"," 'vggish_75',\n"," 'vggish_76',\n"," 'vggish_77',\n"," 'vggish_78',\n"," 'vggish_79',\n"," 'vggish_80',\n"," 'vggish_81',\n"," 'vggish_82',\n"," 'vggish_83',\n"," 'vggish_84',\n"," 'vggish_85',\n"," 'vggish_86',\n"," 'vggish_87',\n"," 'vggish_88',\n"," 'vggish_89',\n"," 'vggish_90',\n"," 'vggish_91',\n"," 'vggish_92',\n"," 'vggish_93',\n"," 'vggish_94',\n"," 'vggish_95',\n"," 'vggish_96',\n"," 'vggish_97',\n"," 'vggish_98',\n"," 'vggish_99',\n"," 'vggish_100',\n"," 'vggish_101',\n"," 'vggish_102',\n"," 'vggish_103',\n"," 'vggish_104',\n"," 'vggish_105',\n"," 'vggish_106',\n"," 'vggish_107',\n"," 'vggish_108',\n"," 'vggish_109',\n"," 'vggish_110',\n"," 'vggish_111',\n"," 'vggish_112',\n"," 'vggish_113',\n"," 'vggish_114',\n"," 'vggish_115',\n"," 'vggish_116',\n"," 'vggish_117',\n"," 'vggish_118',\n"," 'vggish_119',\n"," 'vggish_120',\n"," 'vggish_121',\n"," 'vggish_122',\n"," 'vggish_123',\n"," 'vggish_124',\n"," 'vggish_125',\n"," 'vggish_126',\n"," 'vggish_127']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["df_full.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"Crkj2Zg_NsRf","executionInfo":{"status":"ok","timestamp":1655353217087,"user_tz":-540,"elapsed":6,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"413a83ee-def9-4e00-ebdd-061daee1deb7"},"id":"Crkj2Zg_NsRf","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    age  gender  respiratory_condition  fever_or_muscle_pain  covid19  vggish_0  vggish_1  vggish_2  vggish_3  vggish_4  vggish_5  vggish_6  vggish_7  vggish_8  vggish_9  vggish_10  vggish_11  vggish_12  vggish_13  vggish_14  vggish_15  vggish_16  vggish_17  vggish_18  vggish_19  vggish_20  vggish_21  vggish_22  vggish_23  vggish_24  vggish_25  vggish_26  vggish_27  vggish_28  vggish_29  vggish_30  vggish_31  vggish_32  vggish_33  vggish_34  vggish_35  vggish_36  vggish_37  vggish_38  vggish_39  vggish_40  vggish_41  vggish_42  vggish_43  vggish_44  ...  vggish_94  vggish_95  vggish_96  vggish_97  vggish_98  vggish_99  vggish_100  vggish_101  vggish_102  vggish_103  vggish_104  vggish_105  vggish_106  vggish_107  vggish_108  vggish_109  vggish_110  vggish_111  vggish_112  vggish_113  vggish_114  vggish_115  vggish_116  vggish_117  vggish_118  vggish_119  vggish_120  vggish_121  vggish_122  vggish_123  vggish_124  vggish_125  vggish_126  vggish_127  age_disc8_0  age_disc8_1  \\\n","0  24.0     0.0                    0.0                   1.0        0  0.074367 -0.304656  0.444292 -0.578650 -0.150864 -0.474929 -0.407196 -0.495602 -0.878000  0.271428  -0.539890  -0.369853  -0.417934  -0.303931  -0.110700  -0.402886  -0.192091   0.479554  -0.333110  -0.190150   0.298838  -0.561066  -0.413499  -0.085026   0.340623   0.327000   0.929875  -0.025833  -0.176998  -0.227616  -0.287722  -0.334935   0.053041   0.741705   0.431359  -0.110280  -0.567165   0.028924  -0.363699  -0.180546   0.569059  -0.235530   0.069347  -0.232013   0.163835  ...   0.030110   0.276897  -0.025295  -0.758403  -0.413647   0.010866   -0.096699   -0.015649    0.273723    0.238724   -0.394732   -0.303550    0.195753   -0.705073    0.280866   -0.297937    0.119591   -0.420765   -0.227821   -0.245124   -0.604481   -0.158429   -0.603614   -0.202446   -0.877170    0.174617    0.055384   -0.305561   -0.340612    0.682212    0.114898   -0.233344   -0.286952   -0.310891          0.0          0.0   \n","1  51.0     1.0                    0.0                   0.0        0 -0.093080 -0.323049  0.228054 -0.253214 -0.201783 -0.593411 -0.281786 -0.253731 -0.278699 -0.072951  -0.490047  -0.503245  -0.350297  -0.339051  -0.002062  -0.231233  -0.181500   0.311492  -0.262562  -0.192817   0.545472  -0.518024  -0.137717   0.070471   0.228461   0.560039   0.633050   0.086444  -0.147594  -0.133464  -0.160392  -0.177724   0.185062   0.741338   0.196328  -0.076370  -0.549249  -0.055279  -0.254215  -0.142908   0.512294  -0.259097   0.063067  -0.161039   0.093234  ...   0.027641   0.237238  -0.089823  -0.546613  -0.461019  -0.062111    0.377900    0.031675   -0.014577    0.253654   -0.366609   -0.412507    0.349866   -0.466986    0.318550   -0.399750    0.162004   -0.444358   -0.055705   -0.014060   -0.342550   -0.254980   -0.435774    0.006863   -0.590584   -0.026447    0.032024   -0.168996   -0.115205    0.099536   -0.089518   -0.368148   -0.261253   -0.418885          0.0          0.0   \n","2  22.0     1.0                    0.0                   0.0        0 -0.164700 -0.172472  0.426343 -0.175452  0.010170 -0.466768 -0.258091 -0.289365 -0.301333 -0.068874  -0.358127  -0.365384  -0.336614  -0.281883  -0.151231  -0.172183   0.013820   0.290865  -0.238683  -0.257594   0.262282  -0.429282  -0.177642   0.039664   0.273470   0.383673   0.420238   0.081119   0.034558  -0.084735  -0.173926  -0.308173   0.111967   0.845682   0.424526  -0.100941  -0.512767  -0.113160  -0.115093  -0.165239   0.395894  -0.116302   0.221822  -0.130291   0.172963  ...  -0.022540   0.287411  -0.111606  -0.499588  -0.290404  -0.099194    0.083804   -0.012489    0.108274    0.191909   -0.301285   -0.486360    0.326836   -0.340729    0.354032   -0.345735    0.107432   -0.253056   -0.178793    0.008702   -0.317845    0.021927   -0.378676   -0.128969   -0.657660   -0.003223    0.178623   -0.175769   -0.100840    0.230077   -0.014840   -0.122037   -0.041008   -0.299935          0.0          0.0   \n","3  29.0     0.0                    1.0                   0.0        0  0.088070 -0.352619  0.365449 -0.385953 -0.079941 -0.321737 -0.384355 -0.383507 -0.441021  0.317373  -0.634525  -0.614604  -0.300173  -0.422857  -0.180514  -0.445056  -0.255017   0.234226  -0.319845  -0.330160   0.205797  -0.450726  -0.244148   0.105189   0.334995   0.458771   0.730664  -0.042996  -0.139413  -0.277734  -0.203148  -0.286275   0.150699   0.536051   0.327327  -0.108489  -0.634628   0.270626  -0.255406  -0.302750   0.563379  -0.224966  -0.091500  -0.209208   0.210078  ...  -0.112439   0.254594   0.073100  -0.697025  -0.288771  -0.160099    0.091154    0.062764    0.260474    0.147259   -0.253866   -0.402676    0.284678   -0.506566    0.442841   -0.416390    0.043350   -0.337494   -0.098187   -0.183057   -0.531770   -0.114612   -0.496977   -0.225663   -0.748375    0.061227   -0.070348   -0.398370   -0.204521    0.566465    0.116680   -0.291255   -0.378922   -0.411382          0.0          0.0   \n","4  23.0     1.0                    0.0                   0.0        0 -0.213872 -0.087589  0.194127  0.047982 -0.028293 -0.442092 -0.229419 -0.223341 -0.548548  0.003911  -0.370923  -0.303497  -0.118739  -0.098019  -0.071205  -0.242102   0.114097   0.210606  -0.001671  -0.155339   0.113149  -0.267410  -0.238237   0.022462   0.379822   0.415068   0.302623   0.063197   0.330626  -0.109190  -0.114288  -0.164801  -0.037144   0.464741   0.239383  -0.099023  -0.302174  -0.041982  -0.129017  -0.044680   0.290609  -0.001871   0.060366  -0.173853   0.300038  ...   0.330181   0.518418  -0.106621  -0.396754  -0.224137   0.075243    0.167808    0.103960    0.089840    0.297089   -0.397926   -0.261434    0.334759   -0.288069    0.315628   -0.143351   -0.089417    0.233368    0.028403   -0.030995   -0.320734    0.033128   -0.283002    0.118382   -0.593944    0.073050   -0.171575   -0.130059   -0.189703    0.150264    0.068701   -0.183185    0.023927   -0.136294          0.0          0.0   \n","\n","   age_disc8_2  age_disc8_3  age_disc8_4  age_disc8_5  age_disc8_6  age_disc8_7  age_disc4_0  age_disc4_1  age_disc4_2  age_disc4_3  symptom_class_0  symptom_class_1  symptom_class_2  vulnerable  \n","0          1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              0.0              1.0              0.0         0.0  \n","1          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          1.0          0.0              1.0              0.0              0.0         0.0  \n","2          1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","3          1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              0.0              1.0              0.0         0.0  \n","4          1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","\n","[5 rows x 149 columns]"],"text/html":["\n","  <div id=\"df-48c4608d-7de4-4caa-ad82-14c86d92c1ae\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>respiratory_condition</th>\n","      <th>fever_or_muscle_pain</th>\n","      <th>covid19</th>\n","      <th>vggish_0</th>\n","      <th>vggish_1</th>\n","      <th>vggish_2</th>\n","      <th>vggish_3</th>\n","      <th>vggish_4</th>\n","      <th>vggish_5</th>\n","      <th>vggish_6</th>\n","      <th>vggish_7</th>\n","      <th>vggish_8</th>\n","      <th>vggish_9</th>\n","      <th>vggish_10</th>\n","      <th>vggish_11</th>\n","      <th>vggish_12</th>\n","      <th>vggish_13</th>\n","      <th>vggish_14</th>\n","      <th>vggish_15</th>\n","      <th>vggish_16</th>\n","      <th>vggish_17</th>\n","      <th>vggish_18</th>\n","      <th>vggish_19</th>\n","      <th>vggish_20</th>\n","      <th>vggish_21</th>\n","      <th>vggish_22</th>\n","      <th>vggish_23</th>\n","      <th>vggish_24</th>\n","      <th>vggish_25</th>\n","      <th>vggish_26</th>\n","      <th>vggish_27</th>\n","      <th>vggish_28</th>\n","      <th>vggish_29</th>\n","      <th>vggish_30</th>\n","      <th>vggish_31</th>\n","      <th>vggish_32</th>\n","      <th>vggish_33</th>\n","      <th>vggish_34</th>\n","      <th>vggish_35</th>\n","      <th>vggish_36</th>\n","      <th>vggish_37</th>\n","      <th>vggish_38</th>\n","      <th>vggish_39</th>\n","      <th>vggish_40</th>\n","      <th>vggish_41</th>\n","      <th>vggish_42</th>\n","      <th>vggish_43</th>\n","      <th>vggish_44</th>\n","      <th>...</th>\n","      <th>vggish_94</th>\n","      <th>vggish_95</th>\n","      <th>vggish_96</th>\n","      <th>vggish_97</th>\n","      <th>vggish_98</th>\n","      <th>vggish_99</th>\n","      <th>vggish_100</th>\n","      <th>vggish_101</th>\n","      <th>vggish_102</th>\n","      <th>vggish_103</th>\n","      <th>vggish_104</th>\n","      <th>vggish_105</th>\n","      <th>vggish_106</th>\n","      <th>vggish_107</th>\n","      <th>vggish_108</th>\n","      <th>vggish_109</th>\n","      <th>vggish_110</th>\n","      <th>vggish_111</th>\n","      <th>vggish_112</th>\n","      <th>vggish_113</th>\n","      <th>vggish_114</th>\n","      <th>vggish_115</th>\n","      <th>vggish_116</th>\n","      <th>vggish_117</th>\n","      <th>vggish_118</th>\n","      <th>vggish_119</th>\n","      <th>vggish_120</th>\n","      <th>vggish_121</th>\n","      <th>vggish_122</th>\n","      <th>vggish_123</th>\n","      <th>vggish_124</th>\n","      <th>vggish_125</th>\n","      <th>vggish_126</th>\n","      <th>vggish_127</th>\n","      <th>age_disc8_0</th>\n","      <th>age_disc8_1</th>\n","      <th>age_disc8_2</th>\n","      <th>age_disc8_3</th>\n","      <th>age_disc8_4</th>\n","      <th>age_disc8_5</th>\n","      <th>age_disc8_6</th>\n","      <th>age_disc8_7</th>\n","      <th>age_disc4_0</th>\n","      <th>age_disc4_1</th>\n","      <th>age_disc4_2</th>\n","      <th>age_disc4_3</th>\n","      <th>symptom_class_0</th>\n","      <th>symptom_class_1</th>\n","      <th>symptom_class_2</th>\n","      <th>vulnerable</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>24.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0.074367</td>\n","      <td>-0.304656</td>\n","      <td>0.444292</td>\n","      <td>-0.578650</td>\n","      <td>-0.150864</td>\n","      <td>-0.474929</td>\n","      <td>-0.407196</td>\n","      <td>-0.495602</td>\n","      <td>-0.878000</td>\n","      <td>0.271428</td>\n","      <td>-0.539890</td>\n","      <td>-0.369853</td>\n","      <td>-0.417934</td>\n","      <td>-0.303931</td>\n","      <td>-0.110700</td>\n","      <td>-0.402886</td>\n","      <td>-0.192091</td>\n","      <td>0.479554</td>\n","      <td>-0.333110</td>\n","      <td>-0.190150</td>\n","      <td>0.298838</td>\n","      <td>-0.561066</td>\n","      <td>-0.413499</td>\n","      <td>-0.085026</td>\n","      <td>0.340623</td>\n","      <td>0.327000</td>\n","      <td>0.929875</td>\n","      <td>-0.025833</td>\n","      <td>-0.176998</td>\n","      <td>-0.227616</td>\n","      <td>-0.287722</td>\n","      <td>-0.334935</td>\n","      <td>0.053041</td>\n","      <td>0.741705</td>\n","      <td>0.431359</td>\n","      <td>-0.110280</td>\n","      <td>-0.567165</td>\n","      <td>0.028924</td>\n","      <td>-0.363699</td>\n","      <td>-0.180546</td>\n","      <td>0.569059</td>\n","      <td>-0.235530</td>\n","      <td>0.069347</td>\n","      <td>-0.232013</td>\n","      <td>0.163835</td>\n","      <td>...</td>\n","      <td>0.030110</td>\n","      <td>0.276897</td>\n","      <td>-0.025295</td>\n","      <td>-0.758403</td>\n","      <td>-0.413647</td>\n","      <td>0.010866</td>\n","      <td>-0.096699</td>\n","      <td>-0.015649</td>\n","      <td>0.273723</td>\n","      <td>0.238724</td>\n","      <td>-0.394732</td>\n","      <td>-0.303550</td>\n","      <td>0.195753</td>\n","      <td>-0.705073</td>\n","      <td>0.280866</td>\n","      <td>-0.297937</td>\n","      <td>0.119591</td>\n","      <td>-0.420765</td>\n","      <td>-0.227821</td>\n","      <td>-0.245124</td>\n","      <td>-0.604481</td>\n","      <td>-0.158429</td>\n","      <td>-0.603614</td>\n","      <td>-0.202446</td>\n","      <td>-0.877170</td>\n","      <td>0.174617</td>\n","      <td>0.055384</td>\n","      <td>-0.305561</td>\n","      <td>-0.340612</td>\n","      <td>0.682212</td>\n","      <td>0.114898</td>\n","      <td>-0.233344</td>\n","      <td>-0.286952</td>\n","      <td>-0.310891</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>51.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.093080</td>\n","      <td>-0.323049</td>\n","      <td>0.228054</td>\n","      <td>-0.253214</td>\n","      <td>-0.201783</td>\n","      <td>-0.593411</td>\n","      <td>-0.281786</td>\n","      <td>-0.253731</td>\n","      <td>-0.278699</td>\n","      <td>-0.072951</td>\n","      <td>-0.490047</td>\n","      <td>-0.503245</td>\n","      <td>-0.350297</td>\n","      <td>-0.339051</td>\n","      <td>-0.002062</td>\n","      <td>-0.231233</td>\n","      <td>-0.181500</td>\n","      <td>0.311492</td>\n","      <td>-0.262562</td>\n","      <td>-0.192817</td>\n","      <td>0.545472</td>\n","      <td>-0.518024</td>\n","      <td>-0.137717</td>\n","      <td>0.070471</td>\n","      <td>0.228461</td>\n","      <td>0.560039</td>\n","      <td>0.633050</td>\n","      <td>0.086444</td>\n","      <td>-0.147594</td>\n","      <td>-0.133464</td>\n","      <td>-0.160392</td>\n","      <td>-0.177724</td>\n","      <td>0.185062</td>\n","      <td>0.741338</td>\n","      <td>0.196328</td>\n","      <td>-0.076370</td>\n","      <td>-0.549249</td>\n","      <td>-0.055279</td>\n","      <td>-0.254215</td>\n","      <td>-0.142908</td>\n","      <td>0.512294</td>\n","      <td>-0.259097</td>\n","      <td>0.063067</td>\n","      <td>-0.161039</td>\n","      <td>0.093234</td>\n","      <td>...</td>\n","      <td>0.027641</td>\n","      <td>0.237238</td>\n","      <td>-0.089823</td>\n","      <td>-0.546613</td>\n","      <td>-0.461019</td>\n","      <td>-0.062111</td>\n","      <td>0.377900</td>\n","      <td>0.031675</td>\n","      <td>-0.014577</td>\n","      <td>0.253654</td>\n","      <td>-0.366609</td>\n","      <td>-0.412507</td>\n","      <td>0.349866</td>\n","      <td>-0.466986</td>\n","      <td>0.318550</td>\n","      <td>-0.399750</td>\n","      <td>0.162004</td>\n","      <td>-0.444358</td>\n","      <td>-0.055705</td>\n","      <td>-0.014060</td>\n","      <td>-0.342550</td>\n","      <td>-0.254980</td>\n","      <td>-0.435774</td>\n","      <td>0.006863</td>\n","      <td>-0.590584</td>\n","      <td>-0.026447</td>\n","      <td>0.032024</td>\n","      <td>-0.168996</td>\n","      <td>-0.115205</td>\n","      <td>0.099536</td>\n","      <td>-0.089518</td>\n","      <td>-0.368148</td>\n","      <td>-0.261253</td>\n","      <td>-0.418885</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.164700</td>\n","      <td>-0.172472</td>\n","      <td>0.426343</td>\n","      <td>-0.175452</td>\n","      <td>0.010170</td>\n","      <td>-0.466768</td>\n","      <td>-0.258091</td>\n","      <td>-0.289365</td>\n","      <td>-0.301333</td>\n","      <td>-0.068874</td>\n","      <td>-0.358127</td>\n","      <td>-0.365384</td>\n","      <td>-0.336614</td>\n","      <td>-0.281883</td>\n","      <td>-0.151231</td>\n","      <td>-0.172183</td>\n","      <td>0.013820</td>\n","      <td>0.290865</td>\n","      <td>-0.238683</td>\n","      <td>-0.257594</td>\n","      <td>0.262282</td>\n","      <td>-0.429282</td>\n","      <td>-0.177642</td>\n","      <td>0.039664</td>\n","      <td>0.273470</td>\n","      <td>0.383673</td>\n","      <td>0.420238</td>\n","      <td>0.081119</td>\n","      <td>0.034558</td>\n","      <td>-0.084735</td>\n","      <td>-0.173926</td>\n","      <td>-0.308173</td>\n","      <td>0.111967</td>\n","      <td>0.845682</td>\n","      <td>0.424526</td>\n","      <td>-0.100941</td>\n","      <td>-0.512767</td>\n","      <td>-0.113160</td>\n","      <td>-0.115093</td>\n","      <td>-0.165239</td>\n","      <td>0.395894</td>\n","      <td>-0.116302</td>\n","      <td>0.221822</td>\n","      <td>-0.130291</td>\n","      <td>0.172963</td>\n","      <td>...</td>\n","      <td>-0.022540</td>\n","      <td>0.287411</td>\n","      <td>-0.111606</td>\n","      <td>-0.499588</td>\n","      <td>-0.290404</td>\n","      <td>-0.099194</td>\n","      <td>0.083804</td>\n","      <td>-0.012489</td>\n","      <td>0.108274</td>\n","      <td>0.191909</td>\n","      <td>-0.301285</td>\n","      <td>-0.486360</td>\n","      <td>0.326836</td>\n","      <td>-0.340729</td>\n","      <td>0.354032</td>\n","      <td>-0.345735</td>\n","      <td>0.107432</td>\n","      <td>-0.253056</td>\n","      <td>-0.178793</td>\n","      <td>0.008702</td>\n","      <td>-0.317845</td>\n","      <td>0.021927</td>\n","      <td>-0.378676</td>\n","      <td>-0.128969</td>\n","      <td>-0.657660</td>\n","      <td>-0.003223</td>\n","      <td>0.178623</td>\n","      <td>-0.175769</td>\n","      <td>-0.100840</td>\n","      <td>0.230077</td>\n","      <td>-0.014840</td>\n","      <td>-0.122037</td>\n","      <td>-0.041008</td>\n","      <td>-0.299935</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.088070</td>\n","      <td>-0.352619</td>\n","      <td>0.365449</td>\n","      <td>-0.385953</td>\n","      <td>-0.079941</td>\n","      <td>-0.321737</td>\n","      <td>-0.384355</td>\n","      <td>-0.383507</td>\n","      <td>-0.441021</td>\n","      <td>0.317373</td>\n","      <td>-0.634525</td>\n","      <td>-0.614604</td>\n","      <td>-0.300173</td>\n","      <td>-0.422857</td>\n","      <td>-0.180514</td>\n","      <td>-0.445056</td>\n","      <td>-0.255017</td>\n","      <td>0.234226</td>\n","      <td>-0.319845</td>\n","      <td>-0.330160</td>\n","      <td>0.205797</td>\n","      <td>-0.450726</td>\n","      <td>-0.244148</td>\n","      <td>0.105189</td>\n","      <td>0.334995</td>\n","      <td>0.458771</td>\n","      <td>0.730664</td>\n","      <td>-0.042996</td>\n","      <td>-0.139413</td>\n","      <td>-0.277734</td>\n","      <td>-0.203148</td>\n","      <td>-0.286275</td>\n","      <td>0.150699</td>\n","      <td>0.536051</td>\n","      <td>0.327327</td>\n","      <td>-0.108489</td>\n","      <td>-0.634628</td>\n","      <td>0.270626</td>\n","      <td>-0.255406</td>\n","      <td>-0.302750</td>\n","      <td>0.563379</td>\n","      <td>-0.224966</td>\n","      <td>-0.091500</td>\n","      <td>-0.209208</td>\n","      <td>0.210078</td>\n","      <td>...</td>\n","      <td>-0.112439</td>\n","      <td>0.254594</td>\n","      <td>0.073100</td>\n","      <td>-0.697025</td>\n","      <td>-0.288771</td>\n","      <td>-0.160099</td>\n","      <td>0.091154</td>\n","      <td>0.062764</td>\n","      <td>0.260474</td>\n","      <td>0.147259</td>\n","      <td>-0.253866</td>\n","      <td>-0.402676</td>\n","      <td>0.284678</td>\n","      <td>-0.506566</td>\n","      <td>0.442841</td>\n","      <td>-0.416390</td>\n","      <td>0.043350</td>\n","      <td>-0.337494</td>\n","      <td>-0.098187</td>\n","      <td>-0.183057</td>\n","      <td>-0.531770</td>\n","      <td>-0.114612</td>\n","      <td>-0.496977</td>\n","      <td>-0.225663</td>\n","      <td>-0.748375</td>\n","      <td>0.061227</td>\n","      <td>-0.070348</td>\n","      <td>-0.398370</td>\n","      <td>-0.204521</td>\n","      <td>0.566465</td>\n","      <td>0.116680</td>\n","      <td>-0.291255</td>\n","      <td>-0.378922</td>\n","      <td>-0.411382</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>23.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.213872</td>\n","      <td>-0.087589</td>\n","      <td>0.194127</td>\n","      <td>0.047982</td>\n","      <td>-0.028293</td>\n","      <td>-0.442092</td>\n","      <td>-0.229419</td>\n","      <td>-0.223341</td>\n","      <td>-0.548548</td>\n","      <td>0.003911</td>\n","      <td>-0.370923</td>\n","      <td>-0.303497</td>\n","      <td>-0.118739</td>\n","      <td>-0.098019</td>\n","      <td>-0.071205</td>\n","      <td>-0.242102</td>\n","      <td>0.114097</td>\n","      <td>0.210606</td>\n","      <td>-0.001671</td>\n","      <td>-0.155339</td>\n","      <td>0.113149</td>\n","      <td>-0.267410</td>\n","      <td>-0.238237</td>\n","      <td>0.022462</td>\n","      <td>0.379822</td>\n","      <td>0.415068</td>\n","      <td>0.302623</td>\n","      <td>0.063197</td>\n","      <td>0.330626</td>\n","      <td>-0.109190</td>\n","      <td>-0.114288</td>\n","      <td>-0.164801</td>\n","      <td>-0.037144</td>\n","      <td>0.464741</td>\n","      <td>0.239383</td>\n","      <td>-0.099023</td>\n","      <td>-0.302174</td>\n","      <td>-0.041982</td>\n","      <td>-0.129017</td>\n","      <td>-0.044680</td>\n","      <td>0.290609</td>\n","      <td>-0.001871</td>\n","      <td>0.060366</td>\n","      <td>-0.173853</td>\n","      <td>0.300038</td>\n","      <td>...</td>\n","      <td>0.330181</td>\n","      <td>0.518418</td>\n","      <td>-0.106621</td>\n","      <td>-0.396754</td>\n","      <td>-0.224137</td>\n","      <td>0.075243</td>\n","      <td>0.167808</td>\n","      <td>0.103960</td>\n","      <td>0.089840</td>\n","      <td>0.297089</td>\n","      <td>-0.397926</td>\n","      <td>-0.261434</td>\n","      <td>0.334759</td>\n","      <td>-0.288069</td>\n","      <td>0.315628</td>\n","      <td>-0.143351</td>\n","      <td>-0.089417</td>\n","      <td>0.233368</td>\n","      <td>0.028403</td>\n","      <td>-0.030995</td>\n","      <td>-0.320734</td>\n","      <td>0.033128</td>\n","      <td>-0.283002</td>\n","      <td>0.118382</td>\n","      <td>-0.593944</td>\n","      <td>0.073050</td>\n","      <td>-0.171575</td>\n","      <td>-0.130059</td>\n","      <td>-0.189703</td>\n","      <td>0.150264</td>\n","      <td>0.068701</td>\n","      <td>-0.183185</td>\n","      <td>0.023927</td>\n","      <td>-0.136294</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 149 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48c4608d-7de4-4caa-ad82-14c86d92c1ae')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-48c4608d-7de4-4caa-ad82-14c86d92c1ae button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-48c4608d-7de4-4caa-ad82-14c86d92c1ae');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["df_test.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"DrIjaQMMNs30","executionInfo":{"status":"ok","timestamp":1655353217478,"user_tz":-540,"elapsed":396,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"1ca856b6-a865-42de-d72c-6e51289db069"},"id":"DrIjaQMMNs30","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    age  gender  respiratory_condition  fever_or_muscle_pain  vggish_0  vggish_1  vggish_2  vggish_3  vggish_4  vggish_5  vggish_6  vggish_7  vggish_8  vggish_9  vggish_10  vggish_11  vggish_12  vggish_13  vggish_14  vggish_15  vggish_16  vggish_17  vggish_18  vggish_19  vggish_20  vggish_21  vggish_22  vggish_23  vggish_24  vggish_25  vggish_26  vggish_27  vggish_28  vggish_29  vggish_30  vggish_31  vggish_32  vggish_33  vggish_34  vggish_35  vggish_36  vggish_37  vggish_38  vggish_39  vggish_40  vggish_41  vggish_42  vggish_43  vggish_44  vggish_45  ...  vggish_94  vggish_95  vggish_96  vggish_97  vggish_98  vggish_99  vggish_100  vggish_101  vggish_102  vggish_103  vggish_104  vggish_105  vggish_106  vggish_107  vggish_108  vggish_109  vggish_110  vggish_111  vggish_112  vggish_113  vggish_114  vggish_115  vggish_116  vggish_117  vggish_118  vggish_119  vggish_120  vggish_121  vggish_122  vggish_123  vggish_124  vggish_125  vggish_126  vggish_127  age_disc8_0  age_disc8_1  \\\n","0  48.0     0.0                    1.0                   0.0 -0.249435 -0.078036 -0.000848  0.146939 -0.045798 -0.159696 -0.107600 -0.118128 -0.229618 -0.085126  -0.235071  -0.259579   0.385549   0.086088  -0.055775  -0.087479   0.143160   0.361288   0.108084  -0.330335  -0.034617  -0.221370  -0.095338  -0.120964   0.247845   0.081271   0.015907   0.196405   0.307760  -0.038643  -0.072448  -0.123231   0.097034   0.271419   0.324465  -0.217953  -0.258507  -0.049462  -0.064540  -0.146689   0.439659  -0.045267   0.085254  -0.078951   0.541457  -0.115800  ...   0.206394   0.211534  -0.039425  -0.022362  -0.067527   0.073929    0.098903    0.455746    0.028536    0.340663   -0.370271   -0.114770    0.183220   -0.307789    0.077042   -0.072964   -0.037340    0.489214    0.029793   -0.089756   -0.226815    0.011774   -0.221705    0.017363   -0.237012   -0.021080   -0.035114   -0.075009   -0.027227   -0.024461    0.132206   -0.164106    0.161094    0.090653          0.0          0.0   \n","1  24.0     0.0                    0.0                   0.0 -0.365054 -0.158612  0.104900  0.102022 -0.092483 -0.316609 -0.133178 -0.212336 -0.312242 -0.101015  -0.244832  -0.253776   0.163712  -0.070663  -0.103565  -0.139592   0.103058   0.406807   0.004901  -0.351940   0.073389  -0.343818  -0.119598  -0.132139   0.204288   0.271953   0.231882   0.171854   0.296670  -0.113120  -0.056802  -0.177380   0.068660   0.481462   0.246493  -0.197000  -0.326874  -0.126799  -0.089023  -0.181524   0.418555  -0.128327   0.135198  -0.115831   0.394721  -0.059560  ...   0.181847   0.237135  -0.081053  -0.158384  -0.192924  -0.014655    0.253809    0.304555    0.006955    0.353097   -0.396016   -0.249069    0.200262   -0.315313    0.082205   -0.155889    0.002312    0.223830   -0.033760   -0.098017   -0.280393    0.009500   -0.320952    0.001696   -0.343810   -0.123523   -0.024368   -0.177301   -0.066536   -0.022950    0.021847   -0.326030    0.092071   -0.040008          0.0          0.0   \n","2  29.0     1.0                    0.0                   0.0 -0.266173 -0.051474 -0.135684  0.300016 -0.048960 -0.072094 -0.043378 -0.105313 -0.176645 -0.053787  -0.157787  -0.235288   0.587219   0.159131  -0.093491  -0.038367   0.234486   0.322382   0.232740  -0.354743  -0.063508  -0.165087  -0.044461  -0.133523   0.226273  -0.025890  -0.119254   0.260348   0.438883  -0.014112  -0.039560  -0.134782   0.144375   0.078759   0.245981  -0.261044  -0.198053  -0.057812  -0.076063  -0.125583   0.376237  -0.013202   0.051288  -0.074818   0.592029  -0.133659  ...   0.244246   0.253046  -0.031076   0.113678  -0.049720   0.099078    0.119885    0.541312   -0.055865    0.269268   -0.384827   -0.104771    0.138724   -0.228172    0.007556   -0.020042   -0.065018    0.687625    0.074215   -0.045640   -0.177862    0.054304   -0.169483    0.023931   -0.093769   -0.032683   -0.112096   -0.030724   -0.032954   -0.105866    0.207592   -0.156844    0.227877    0.175801          0.0          0.0   \n","3  39.0     0.0                    0.0                   0.0  0.081596 -0.050033  0.091697 -0.195561 -0.050593 -0.308308 -0.194966 -0.248588 -0.258360  0.204068  -0.289227  -0.391996  -0.264257  -0.129608  -0.087989  -0.159570  -0.054177   0.388775  -0.044596  -0.044411   0.229416  -0.203256  -0.181956  -0.072175   0.544561   0.309737   0.507254   0.135759   0.069490  -0.173852  -0.133196  -0.101946  -0.019489   0.627335   0.282040  -0.153673  -0.309699  -0.138420  -0.322991  -0.158139   0.379478   0.034558   0.046640  -0.202109   0.188757  -0.107157  ...   0.143310   0.389718  -0.006484  -0.479807  -0.220119   0.121994   -0.074072    0.040810    0.052372    0.176469   -0.292621   -0.362384    0.321456   -0.293191    0.136225   -0.115177    0.002313   -0.085969   -0.092449   -0.088201   -0.316369   -0.092986   -0.476835    0.087741   -0.567856    0.282586    0.031993   -0.020578   -0.174614    0.170744    0.138038   -0.023719    0.018652   -0.154509          0.0          0.0   \n","4  34.0     1.0                    0.0                   0.0 -0.201897 -0.207558  0.470994 -0.310761  0.003336 -0.356504 -0.182039 -0.243932 -0.548647 -0.098514  -0.256118  -0.366254  -0.196931  -0.145483  -0.109275  -0.194539   0.027524   0.261817  -0.250724  -0.224270   0.219435  -0.439139  -0.232983  -0.032888   0.125310   0.373186   0.390432   0.042330  -0.040499  -0.108811  -0.235327  -0.236051   0.065434   0.777719   0.330530  -0.066482  -0.326282  -0.138608  -0.063585  -0.179068   0.567870  -0.308946   0.269137  -0.144893   0.251727  -0.050229  ...   0.047370   0.288538  -0.127788  -0.439673  -0.255373  -0.063710    0.102492    0.096573    0.300080    0.282765   -0.288261   -0.193158    0.233706   -0.515813    0.223579   -0.186225    0.244542   -0.147010   -0.128894   -0.201853   -0.376645   -0.102247   -0.341130   -0.181266   -0.486127    0.070523    0.060550   -0.203526   -0.092903    0.160788    0.006748   -0.235371   -0.048243   -0.208136          0.0          0.0   \n","\n","   age_disc8_2  age_disc8_3  age_disc8_4  age_disc8_5  age_disc8_6  age_disc8_7  age_disc4_0  age_disc4_1  age_disc4_2  age_disc4_3  symptom_class_0  symptom_class_1  symptom_class_2  vulnerable  \n","0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0              0.0              1.0              0.0         0.0  \n","1          1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","2          1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","3          0.0          1.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","4          0.0          1.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","\n","[5 rows x 148 columns]"],"text/html":["\n","  <div id=\"df-0b12394a-964c-4713-8ce4-59a231db00c8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>respiratory_condition</th>\n","      <th>fever_or_muscle_pain</th>\n","      <th>vggish_0</th>\n","      <th>vggish_1</th>\n","      <th>vggish_2</th>\n","      <th>vggish_3</th>\n","      <th>vggish_4</th>\n","      <th>vggish_5</th>\n","      <th>vggish_6</th>\n","      <th>vggish_7</th>\n","      <th>vggish_8</th>\n","      <th>vggish_9</th>\n","      <th>vggish_10</th>\n","      <th>vggish_11</th>\n","      <th>vggish_12</th>\n","      <th>vggish_13</th>\n","      <th>vggish_14</th>\n","      <th>vggish_15</th>\n","      <th>vggish_16</th>\n","      <th>vggish_17</th>\n","      <th>vggish_18</th>\n","      <th>vggish_19</th>\n","      <th>vggish_20</th>\n","      <th>vggish_21</th>\n","      <th>vggish_22</th>\n","      <th>vggish_23</th>\n","      <th>vggish_24</th>\n","      <th>vggish_25</th>\n","      <th>vggish_26</th>\n","      <th>vggish_27</th>\n","      <th>vggish_28</th>\n","      <th>vggish_29</th>\n","      <th>vggish_30</th>\n","      <th>vggish_31</th>\n","      <th>vggish_32</th>\n","      <th>vggish_33</th>\n","      <th>vggish_34</th>\n","      <th>vggish_35</th>\n","      <th>vggish_36</th>\n","      <th>vggish_37</th>\n","      <th>vggish_38</th>\n","      <th>vggish_39</th>\n","      <th>vggish_40</th>\n","      <th>vggish_41</th>\n","      <th>vggish_42</th>\n","      <th>vggish_43</th>\n","      <th>vggish_44</th>\n","      <th>vggish_45</th>\n","      <th>...</th>\n","      <th>vggish_94</th>\n","      <th>vggish_95</th>\n","      <th>vggish_96</th>\n","      <th>vggish_97</th>\n","      <th>vggish_98</th>\n","      <th>vggish_99</th>\n","      <th>vggish_100</th>\n","      <th>vggish_101</th>\n","      <th>vggish_102</th>\n","      <th>vggish_103</th>\n","      <th>vggish_104</th>\n","      <th>vggish_105</th>\n","      <th>vggish_106</th>\n","      <th>vggish_107</th>\n","      <th>vggish_108</th>\n","      <th>vggish_109</th>\n","      <th>vggish_110</th>\n","      <th>vggish_111</th>\n","      <th>vggish_112</th>\n","      <th>vggish_113</th>\n","      <th>vggish_114</th>\n","      <th>vggish_115</th>\n","      <th>vggish_116</th>\n","      <th>vggish_117</th>\n","      <th>vggish_118</th>\n","      <th>vggish_119</th>\n","      <th>vggish_120</th>\n","      <th>vggish_121</th>\n","      <th>vggish_122</th>\n","      <th>vggish_123</th>\n","      <th>vggish_124</th>\n","      <th>vggish_125</th>\n","      <th>vggish_126</th>\n","      <th>vggish_127</th>\n","      <th>age_disc8_0</th>\n","      <th>age_disc8_1</th>\n","      <th>age_disc8_2</th>\n","      <th>age_disc8_3</th>\n","      <th>age_disc8_4</th>\n","      <th>age_disc8_5</th>\n","      <th>age_disc8_6</th>\n","      <th>age_disc8_7</th>\n","      <th>age_disc4_0</th>\n","      <th>age_disc4_1</th>\n","      <th>age_disc4_2</th>\n","      <th>age_disc4_3</th>\n","      <th>symptom_class_0</th>\n","      <th>symptom_class_1</th>\n","      <th>symptom_class_2</th>\n","      <th>vulnerable</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>48.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>-0.249435</td>\n","      <td>-0.078036</td>\n","      <td>-0.000848</td>\n","      <td>0.146939</td>\n","      <td>-0.045798</td>\n","      <td>-0.159696</td>\n","      <td>-0.107600</td>\n","      <td>-0.118128</td>\n","      <td>-0.229618</td>\n","      <td>-0.085126</td>\n","      <td>-0.235071</td>\n","      <td>-0.259579</td>\n","      <td>0.385549</td>\n","      <td>0.086088</td>\n","      <td>-0.055775</td>\n","      <td>-0.087479</td>\n","      <td>0.143160</td>\n","      <td>0.361288</td>\n","      <td>0.108084</td>\n","      <td>-0.330335</td>\n","      <td>-0.034617</td>\n","      <td>-0.221370</td>\n","      <td>-0.095338</td>\n","      <td>-0.120964</td>\n","      <td>0.247845</td>\n","      <td>0.081271</td>\n","      <td>0.015907</td>\n","      <td>0.196405</td>\n","      <td>0.307760</td>\n","      <td>-0.038643</td>\n","      <td>-0.072448</td>\n","      <td>-0.123231</td>\n","      <td>0.097034</td>\n","      <td>0.271419</td>\n","      <td>0.324465</td>\n","      <td>-0.217953</td>\n","      <td>-0.258507</td>\n","      <td>-0.049462</td>\n","      <td>-0.064540</td>\n","      <td>-0.146689</td>\n","      <td>0.439659</td>\n","      <td>-0.045267</td>\n","      <td>0.085254</td>\n","      <td>-0.078951</td>\n","      <td>0.541457</td>\n","      <td>-0.115800</td>\n","      <td>...</td>\n","      <td>0.206394</td>\n","      <td>0.211534</td>\n","      <td>-0.039425</td>\n","      <td>-0.022362</td>\n","      <td>-0.067527</td>\n","      <td>0.073929</td>\n","      <td>0.098903</td>\n","      <td>0.455746</td>\n","      <td>0.028536</td>\n","      <td>0.340663</td>\n","      <td>-0.370271</td>\n","      <td>-0.114770</td>\n","      <td>0.183220</td>\n","      <td>-0.307789</td>\n","      <td>0.077042</td>\n","      <td>-0.072964</td>\n","      <td>-0.037340</td>\n","      <td>0.489214</td>\n","      <td>0.029793</td>\n","      <td>-0.089756</td>\n","      <td>-0.226815</td>\n","      <td>0.011774</td>\n","      <td>-0.221705</td>\n","      <td>0.017363</td>\n","      <td>-0.237012</td>\n","      <td>-0.021080</td>\n","      <td>-0.035114</td>\n","      <td>-0.075009</td>\n","      <td>-0.027227</td>\n","      <td>-0.024461</td>\n","      <td>0.132206</td>\n","      <td>-0.164106</td>\n","      <td>0.161094</td>\n","      <td>0.090653</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>24.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.365054</td>\n","      <td>-0.158612</td>\n","      <td>0.104900</td>\n","      <td>0.102022</td>\n","      <td>-0.092483</td>\n","      <td>-0.316609</td>\n","      <td>-0.133178</td>\n","      <td>-0.212336</td>\n","      <td>-0.312242</td>\n","      <td>-0.101015</td>\n","      <td>-0.244832</td>\n","      <td>-0.253776</td>\n","      <td>0.163712</td>\n","      <td>-0.070663</td>\n","      <td>-0.103565</td>\n","      <td>-0.139592</td>\n","      <td>0.103058</td>\n","      <td>0.406807</td>\n","      <td>0.004901</td>\n","      <td>-0.351940</td>\n","      <td>0.073389</td>\n","      <td>-0.343818</td>\n","      <td>-0.119598</td>\n","      <td>-0.132139</td>\n","      <td>0.204288</td>\n","      <td>0.271953</td>\n","      <td>0.231882</td>\n","      <td>0.171854</td>\n","      <td>0.296670</td>\n","      <td>-0.113120</td>\n","      <td>-0.056802</td>\n","      <td>-0.177380</td>\n","      <td>0.068660</td>\n","      <td>0.481462</td>\n","      <td>0.246493</td>\n","      <td>-0.197000</td>\n","      <td>-0.326874</td>\n","      <td>-0.126799</td>\n","      <td>-0.089023</td>\n","      <td>-0.181524</td>\n","      <td>0.418555</td>\n","      <td>-0.128327</td>\n","      <td>0.135198</td>\n","      <td>-0.115831</td>\n","      <td>0.394721</td>\n","      <td>-0.059560</td>\n","      <td>...</td>\n","      <td>0.181847</td>\n","      <td>0.237135</td>\n","      <td>-0.081053</td>\n","      <td>-0.158384</td>\n","      <td>-0.192924</td>\n","      <td>-0.014655</td>\n","      <td>0.253809</td>\n","      <td>0.304555</td>\n","      <td>0.006955</td>\n","      <td>0.353097</td>\n","      <td>-0.396016</td>\n","      <td>-0.249069</td>\n","      <td>0.200262</td>\n","      <td>-0.315313</td>\n","      <td>0.082205</td>\n","      <td>-0.155889</td>\n","      <td>0.002312</td>\n","      <td>0.223830</td>\n","      <td>-0.033760</td>\n","      <td>-0.098017</td>\n","      <td>-0.280393</td>\n","      <td>0.009500</td>\n","      <td>-0.320952</td>\n","      <td>0.001696</td>\n","      <td>-0.343810</td>\n","      <td>-0.123523</td>\n","      <td>-0.024368</td>\n","      <td>-0.177301</td>\n","      <td>-0.066536</td>\n","      <td>-0.022950</td>\n","      <td>0.021847</td>\n","      <td>-0.326030</td>\n","      <td>0.092071</td>\n","      <td>-0.040008</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>29.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.266173</td>\n","      <td>-0.051474</td>\n","      <td>-0.135684</td>\n","      <td>0.300016</td>\n","      <td>-0.048960</td>\n","      <td>-0.072094</td>\n","      <td>-0.043378</td>\n","      <td>-0.105313</td>\n","      <td>-0.176645</td>\n","      <td>-0.053787</td>\n","      <td>-0.157787</td>\n","      <td>-0.235288</td>\n","      <td>0.587219</td>\n","      <td>0.159131</td>\n","      <td>-0.093491</td>\n","      <td>-0.038367</td>\n","      <td>0.234486</td>\n","      <td>0.322382</td>\n","      <td>0.232740</td>\n","      <td>-0.354743</td>\n","      <td>-0.063508</td>\n","      <td>-0.165087</td>\n","      <td>-0.044461</td>\n","      <td>-0.133523</td>\n","      <td>0.226273</td>\n","      <td>-0.025890</td>\n","      <td>-0.119254</td>\n","      <td>0.260348</td>\n","      <td>0.438883</td>\n","      <td>-0.014112</td>\n","      <td>-0.039560</td>\n","      <td>-0.134782</td>\n","      <td>0.144375</td>\n","      <td>0.078759</td>\n","      <td>0.245981</td>\n","      <td>-0.261044</td>\n","      <td>-0.198053</td>\n","      <td>-0.057812</td>\n","      <td>-0.076063</td>\n","      <td>-0.125583</td>\n","      <td>0.376237</td>\n","      <td>-0.013202</td>\n","      <td>0.051288</td>\n","      <td>-0.074818</td>\n","      <td>0.592029</td>\n","      <td>-0.133659</td>\n","      <td>...</td>\n","      <td>0.244246</td>\n","      <td>0.253046</td>\n","      <td>-0.031076</td>\n","      <td>0.113678</td>\n","      <td>-0.049720</td>\n","      <td>0.099078</td>\n","      <td>0.119885</td>\n","      <td>0.541312</td>\n","      <td>-0.055865</td>\n","      <td>0.269268</td>\n","      <td>-0.384827</td>\n","      <td>-0.104771</td>\n","      <td>0.138724</td>\n","      <td>-0.228172</td>\n","      <td>0.007556</td>\n","      <td>-0.020042</td>\n","      <td>-0.065018</td>\n","      <td>0.687625</td>\n","      <td>0.074215</td>\n","      <td>-0.045640</td>\n","      <td>-0.177862</td>\n","      <td>0.054304</td>\n","      <td>-0.169483</td>\n","      <td>0.023931</td>\n","      <td>-0.093769</td>\n","      <td>-0.032683</td>\n","      <td>-0.112096</td>\n","      <td>-0.030724</td>\n","      <td>-0.032954</td>\n","      <td>-0.105866</td>\n","      <td>0.207592</td>\n","      <td>-0.156844</td>\n","      <td>0.227877</td>\n","      <td>0.175801</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>39.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.081596</td>\n","      <td>-0.050033</td>\n","      <td>0.091697</td>\n","      <td>-0.195561</td>\n","      <td>-0.050593</td>\n","      <td>-0.308308</td>\n","      <td>-0.194966</td>\n","      <td>-0.248588</td>\n","      <td>-0.258360</td>\n","      <td>0.204068</td>\n","      <td>-0.289227</td>\n","      <td>-0.391996</td>\n","      <td>-0.264257</td>\n","      <td>-0.129608</td>\n","      <td>-0.087989</td>\n","      <td>-0.159570</td>\n","      <td>-0.054177</td>\n","      <td>0.388775</td>\n","      <td>-0.044596</td>\n","      <td>-0.044411</td>\n","      <td>0.229416</td>\n","      <td>-0.203256</td>\n","      <td>-0.181956</td>\n","      <td>-0.072175</td>\n","      <td>0.544561</td>\n","      <td>0.309737</td>\n","      <td>0.507254</td>\n","      <td>0.135759</td>\n","      <td>0.069490</td>\n","      <td>-0.173852</td>\n","      <td>-0.133196</td>\n","      <td>-0.101946</td>\n","      <td>-0.019489</td>\n","      <td>0.627335</td>\n","      <td>0.282040</td>\n","      <td>-0.153673</td>\n","      <td>-0.309699</td>\n","      <td>-0.138420</td>\n","      <td>-0.322991</td>\n","      <td>-0.158139</td>\n","      <td>0.379478</td>\n","      <td>0.034558</td>\n","      <td>0.046640</td>\n","      <td>-0.202109</td>\n","      <td>0.188757</td>\n","      <td>-0.107157</td>\n","      <td>...</td>\n","      <td>0.143310</td>\n","      <td>0.389718</td>\n","      <td>-0.006484</td>\n","      <td>-0.479807</td>\n","      <td>-0.220119</td>\n","      <td>0.121994</td>\n","      <td>-0.074072</td>\n","      <td>0.040810</td>\n","      <td>0.052372</td>\n","      <td>0.176469</td>\n","      <td>-0.292621</td>\n","      <td>-0.362384</td>\n","      <td>0.321456</td>\n","      <td>-0.293191</td>\n","      <td>0.136225</td>\n","      <td>-0.115177</td>\n","      <td>0.002313</td>\n","      <td>-0.085969</td>\n","      <td>-0.092449</td>\n","      <td>-0.088201</td>\n","      <td>-0.316369</td>\n","      <td>-0.092986</td>\n","      <td>-0.476835</td>\n","      <td>0.087741</td>\n","      <td>-0.567856</td>\n","      <td>0.282586</td>\n","      <td>0.031993</td>\n","      <td>-0.020578</td>\n","      <td>-0.174614</td>\n","      <td>0.170744</td>\n","      <td>0.138038</td>\n","      <td>-0.023719</td>\n","      <td>0.018652</td>\n","      <td>-0.154509</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>34.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-0.201897</td>\n","      <td>-0.207558</td>\n","      <td>0.470994</td>\n","      <td>-0.310761</td>\n","      <td>0.003336</td>\n","      <td>-0.356504</td>\n","      <td>-0.182039</td>\n","      <td>-0.243932</td>\n","      <td>-0.548647</td>\n","      <td>-0.098514</td>\n","      <td>-0.256118</td>\n","      <td>-0.366254</td>\n","      <td>-0.196931</td>\n","      <td>-0.145483</td>\n","      <td>-0.109275</td>\n","      <td>-0.194539</td>\n","      <td>0.027524</td>\n","      <td>0.261817</td>\n","      <td>-0.250724</td>\n","      <td>-0.224270</td>\n","      <td>0.219435</td>\n","      <td>-0.439139</td>\n","      <td>-0.232983</td>\n","      <td>-0.032888</td>\n","      <td>0.125310</td>\n","      <td>0.373186</td>\n","      <td>0.390432</td>\n","      <td>0.042330</td>\n","      <td>-0.040499</td>\n","      <td>-0.108811</td>\n","      <td>-0.235327</td>\n","      <td>-0.236051</td>\n","      <td>0.065434</td>\n","      <td>0.777719</td>\n","      <td>0.330530</td>\n","      <td>-0.066482</td>\n","      <td>-0.326282</td>\n","      <td>-0.138608</td>\n","      <td>-0.063585</td>\n","      <td>-0.179068</td>\n","      <td>0.567870</td>\n","      <td>-0.308946</td>\n","      <td>0.269137</td>\n","      <td>-0.144893</td>\n","      <td>0.251727</td>\n","      <td>-0.050229</td>\n","      <td>...</td>\n","      <td>0.047370</td>\n","      <td>0.288538</td>\n","      <td>-0.127788</td>\n","      <td>-0.439673</td>\n","      <td>-0.255373</td>\n","      <td>-0.063710</td>\n","      <td>0.102492</td>\n","      <td>0.096573</td>\n","      <td>0.300080</td>\n","      <td>0.282765</td>\n","      <td>-0.288261</td>\n","      <td>-0.193158</td>\n","      <td>0.233706</td>\n","      <td>-0.515813</td>\n","      <td>0.223579</td>\n","      <td>-0.186225</td>\n","      <td>0.244542</td>\n","      <td>-0.147010</td>\n","      <td>-0.128894</td>\n","      <td>-0.201853</td>\n","      <td>-0.376645</td>\n","      <td>-0.102247</td>\n","      <td>-0.341130</td>\n","      <td>-0.181266</td>\n","      <td>-0.486127</td>\n","      <td>0.070523</td>\n","      <td>0.060550</td>\n","      <td>-0.203526</td>\n","      <td>-0.092903</td>\n","      <td>0.160788</td>\n","      <td>0.006748</td>\n","      <td>-0.235371</td>\n","      <td>-0.048243</td>\n","      <td>-0.208136</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 148 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b12394a-964c-4713-8ce4-59a231db00c8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0b12394a-964c-4713-8ce4-59a231db00c8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0b12394a-964c-4713-8ce4-59a231db00c8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["print(df_full.isna().sum().sum())\n","print(df_test.isna().sum().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GKJfF0pNjMz","executionInfo":{"status":"ok","timestamp":1655353217479,"user_tz":-540,"elapsed":4,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"9ae7faa0-3b64-4c00-86b9-2b7106b9e2e3"},"id":"5GKJfF0pNjMz","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","896\n"]}]},{"cell_type":"code","source":["df_test.isna().sum().index[df_test.isna().sum() > 0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SGJ0GPmjtttL","executionInfo":{"status":"ok","timestamp":1655353217479,"user_tz":-540,"elapsed":3,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"16073dcb-4ee8-4054-c42e-9f0692adaeb6"},"id":"SGJ0GPmjtttL","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['vggish_0', 'vggish_1', 'vggish_2', 'vggish_3', 'vggish_4', 'vggish_5', 'vggish_6', 'vggish_7', 'vggish_8', 'vggish_9',\n","       ...\n","       'vggish_118', 'vggish_119', 'vggish_120', 'vggish_121', 'vggish_122', 'vggish_123', 'vggish_124', 'vggish_125', 'vggish_126', 'vggish_127'], dtype='object', length=128)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# linear_model,\n","# explicitly require this experimental feature\n","from sklearn.experimental import enable_iterative_imputer  # noqa\n","# now you can import normally from sklearn.impute\n","from sklearn.impute import IterativeImputer\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import make_pipeline\n","\n","# make_pipeline(StandardScaler(), LinearRegression())\n","\n","imputor = IterativeImputer(\n","    estimator=make_pipeline(StandardScaler(), LinearRegression()),\n","    max_iter=10, random_state=42\n",")\n","\n","# imputing na values on audio features in test dataset\n","imputor.fit(df_full.drop(target_var, axis=1).to_numpy())\n","\n","df_test.iloc[:,:] = imputor.transform(df_test.to_numpy())"],"metadata":{"id":"OhJ5Vf4LoJ2h"},"id":"OhJ5Vf4LoJ2h","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # imputing na values on audio features in test dataset\n","\n","# imputor = MissForest(max_iter=10, n_estimators=100, max_depth=4, min_samples_leaf=3, verbose=1, random_state=42)\n","# imputor.fit(df_full.drop(target_var, axis=1).to_numpy())\n","\n","# df_test[:,:] = imputor.transform(df_test.to_numpy())"],"metadata":{"id":"EXWr0xiIZufG"},"id":"EXWr0xiIZufG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_full.isna().sum().sum())\n","print(df_test.isna().sum().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66VvnJK-bGm3","executionInfo":{"status":"ok","timestamp":1655353230286,"user_tz":-540,"elapsed":6,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"0a294bb6-7f42-4e07-910b-a0b2358dc53f"},"id":"66VvnJK-bGm3","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n"]}]},{"cell_type":"code","source":["df_full = df_full.drop_duplicates().reset_index(drop=True)\n","df_full.sample(frac=1, random_state=42).reset_index(drop=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"qj6_74iiKa8a","executionInfo":{"status":"ok","timestamp":1655353230286,"user_tz":-540,"elapsed":5,"user":{"displayName":"김영준","userId":"06606532799291918175"}},"outputId":"aac07d0d-c22b-4aa7-ea58-29f43811caba"},"id":"qj6_74iiKa8a","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       age  gender  respiratory_condition  fever_or_muscle_pain  covid19  vggish_0  vggish_1  vggish_2  vggish_3  vggish_4  vggish_5  vggish_6  vggish_7  vggish_8  vggish_9  vggish_10  vggish_11  vggish_12  vggish_13  vggish_14  vggish_15  vggish_16  vggish_17  vggish_18  vggish_19  vggish_20  vggish_21  vggish_22  vggish_23  vggish_24  vggish_25  vggish_26  vggish_27  vggish_28  vggish_29  vggish_30  vggish_31  vggish_32  vggish_33  vggish_34  vggish_35  vggish_36  vggish_37  vggish_38  vggish_39  vggish_40  vggish_41  vggish_42  vggish_43  vggish_44  ...  vggish_94  vggish_95  vggish_96  vggish_97  vggish_98  vggish_99  vggish_100  vggish_101  vggish_102  vggish_103  vggish_104  vggish_105  vggish_106  vggish_107  vggish_108  vggish_109  vggish_110  vggish_111  vggish_112  vggish_113  vggish_114  vggish_115  vggish_116  vggish_117  vggish_118  vggish_119  vggish_120  vggish_121  vggish_122  vggish_123  vggish_124  vggish_125  vggish_126  vggish_127  age_disc8_0  age_disc8_1  \\\n","0     27.0     1.0                    0.0                   0.0        0  0.237233 -0.073406  0.142055 -0.539462 -0.054550 -0.457476 -0.277106 -0.374962 -0.193999  0.135108  -0.685005  -0.552347  -0.382332  -0.179146   0.085340  -0.287859  -0.125214   0.629780  -0.346372  -0.098754   0.320120  -0.379453  -0.325220  -0.036740   0.455812   0.213303   0.531806   0.213881  -0.179417  -0.270110  -0.239606  -0.155629  -0.018713   0.639012   0.427209  -0.128684  -0.552983  -0.000528  -0.309293  -0.021275   0.377659  -0.134463   0.198480  -0.261788   0.014866  ...   0.097233   0.221708   0.016513  -0.646305  -0.445976   0.163898   -0.216428   -0.002031    0.221547    0.330069   -0.362868   -0.170088    0.376592   -0.461542    0.362308   -0.227808   -0.194761   -0.259064   -0.121842    0.059296   -0.381911   -0.278796   -0.497189    0.052893   -0.858647    0.253602    0.176621   -0.052145   -0.088078    0.347476    0.031137   -0.037026   -0.059619   -0.262939          0.0          0.0   \n","1     26.0     0.0                    0.0                   0.0        0  0.246134 -0.148192  0.244273 -0.538205 -0.337523 -0.593265 -0.311994 -0.429375 -0.532566  0.282096  -0.808344  -0.636284  -0.525758  -0.101947   0.089822  -0.137083  -0.029002   0.338129  -0.202859  -0.015267   0.312917  -0.102637  -0.262040   0.089277   0.297634   0.217412   0.472763   0.407411  -0.235386  -0.319075   0.004311  -0.177443   0.011335   0.692153   0.180619   0.222378  -0.572982  -0.159462  -0.438346   0.069036   0.315911   0.065443   0.149918  -0.115082   0.204928  ...  -0.058818   0.199952  -0.109857  -0.661890  -0.535261  -0.008445   -0.197057    0.303818   -0.003170    0.225542   -0.452083   -0.424431    0.354409   -0.191299    0.330623   -0.153216   -0.113831   -0.202335   -0.015565   -0.017402   -0.653620   -0.330262   -0.408605    0.028116   -1.047375    0.246120    0.070484   -0.083004   -0.088040    0.252036   -0.007256    0.090935   -0.067936   -0.358746          0.0          0.0   \n","2     27.0     1.0                    0.0                   0.0        0 -0.061355 -0.104770  0.196464  0.256682  0.008873 -0.406786 -0.341376 -0.238717 -0.307947  0.039246  -0.475410  -0.464350  -0.124527  -0.061451  -0.096154  -0.199233   0.069042   0.077799   0.014512  -0.261682   0.219502  -0.286834  -0.178618   0.106887   0.232501   0.102398   0.084113   0.075658   0.324012  -0.062375  -0.094696  -0.273715   0.143767   0.459655   0.268433  -0.109267  -0.437196   0.252963  -0.247513  -0.107284   0.291054  -0.064484   0.082671  -0.155126   0.336953  ...   0.224578   0.291128   0.063938  -0.272569  -0.237854  -0.038205    0.077042    0.109512    0.081803    0.132673   -0.329243   -0.353951    0.336664   -0.255079    0.467882   -0.175494   -0.225324    0.365005    0.024953    0.074411   -0.251430    0.023905   -0.355071   -0.024995   -0.566465   -0.008698   -0.067261   -0.180592   -0.229741    0.200737    0.144086   -0.227922   -0.003502   -0.203552          0.0          0.0   \n","3     37.0     1.0                    0.0                   0.0        0 -0.134299 -0.150072  0.405094 -0.510863 -0.086288 -0.418875  0.001268 -0.251929 -0.498573  0.072656  -0.201595  -0.357154  -0.282462  -0.169092  -0.099354  -0.152573  -0.107116   0.530403  -0.247494  -0.064046   0.335060  -0.395883  -0.224059  -0.209563   0.361936   0.483415   0.785008   0.148584  -0.128865  -0.204075  -0.192620  -0.088760  -0.030923   0.896775   0.330546  -0.102979  -0.244311  -0.307154  -0.237432  -0.107218   0.666889  -0.215102   0.207663  -0.194329   0.128014  ...   0.059770   0.377729  -0.127123  -0.457038  -0.179448   0.072016    0.060581    0.187417    0.153641    0.205097   -0.260940   -0.244228    0.232359   -0.593012    0.237451   -0.104367    0.126822   -0.298137   -0.204943   -0.079213   -0.305575   -0.057126   -0.387948   -0.043517   -0.397912    0.062316    0.193401   -0.161362   -0.163191    0.223964   -0.052330   -0.260159    0.033872   -0.185345          0.0          0.0   \n","4     30.0     1.0                    0.0                   0.0        0 -0.003999 -0.111768  0.203734 -0.315624  0.017499 -0.426326 -0.214551 -0.391016 -0.603783  0.163266  -0.374928  -0.397450  -0.407722  -0.112975  -0.052706  -0.255393  -0.050976   0.384551  -0.172650  -0.037434   0.243605  -0.092323  -0.337894   0.015879   0.259033   0.281155   0.412646   0.127118   0.044594  -0.146128  -0.161955  -0.105097   0.054556   0.359450   0.364263  -0.158441  -0.350316  -0.143393  -0.397220   0.020893   0.451105  -0.084920   0.002359  -0.116057   0.249334  ...   0.063617   0.294652  -0.040682  -0.571301  -0.259738   0.110305   -0.209054    0.250293    0.106468    0.062130   -0.293578   -0.304958    0.187921   -0.595932    0.258083   -0.161055   -0.107944   -0.097554   -0.127796   -0.115905   -0.373250   -0.201690   -0.280232   -0.184360   -0.616527    0.065762    0.004534   -0.077136   -0.274606    0.431737    0.111949   -0.047648   -0.066311   -0.203175          0.0          0.0   \n","...    ...     ...                    ...                   ...      ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...        ...  ...        ...        ...        ...        ...        ...        ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...         ...          ...          ...   \n","3769  12.0     1.0                    0.0                   0.0        0 -0.246177 -0.034868 -0.129657  0.322590 -0.035816 -0.070364 -0.049894 -0.113831 -0.177191 -0.038279  -0.144693  -0.224197   0.585655   0.157981  -0.093875  -0.038411   0.241772   0.322176   0.244930  -0.338993  -0.056917  -0.147660  -0.060325  -0.120323   0.223656  -0.026892  -0.111760   0.266951   0.456231  -0.000417  -0.045390  -0.142784   0.138059   0.080557   0.251560  -0.261386  -0.196583  -0.055117  -0.084223  -0.119749   0.373634   0.003988   0.048978  -0.088418   0.591958  ...   0.249724   0.246064  -0.026176   0.104687  -0.049091   0.103767    0.124341    0.511819   -0.057119    0.265238   -0.373399   -0.099381    0.150342   -0.215783    0.000922   -0.016490   -0.073646    0.701981    0.070872   -0.037070   -0.181049    0.067465   -0.207613    0.023224   -0.092705   -0.034168   -0.087253   -0.025519   -0.038996   -0.108309    0.217582   -0.135895    0.222678    0.166042          0.0          1.0   \n","3770  40.0     1.0                    0.0                   0.0        0 -0.158730 -0.199019  0.178682 -0.004860 -0.096780 -0.397793 -0.399140 -0.317382 -0.261474 -0.093012  -0.539748  -0.491350  -0.229894  -0.181734  -0.172093  -0.309356   0.008652   0.062677  -0.019963  -0.379514   0.162992  -0.463177  -0.180228   0.160992   0.264425   0.313263   0.199507  -0.000034   0.079976  -0.126106  -0.204884  -0.226403  -0.036004   0.847588   0.416150  -0.128587  -0.468832   0.187249  -0.124072  -0.239662   0.444036  -0.207752   0.118965  -0.136653   0.361643  ...   0.237841   0.110573   0.063766  -0.413122  -0.240286  -0.102974    0.177032   -0.009918    0.065011    0.486265   -0.365485   -0.320821    0.291606   -0.350020    0.355880   -0.246199   -0.097304    0.109610   -0.077862   -0.122714   -0.315635   -0.025315   -0.448783    0.000860   -0.650030    0.067289   -0.048413   -0.276386   -0.096411    0.200029    0.124908   -0.157733   -0.173501   -0.230978          0.0          0.0   \n","3771  52.0     1.0                    0.0                   0.0        0 -0.218338 -0.279103  0.525833 -0.108297 -0.185997 -0.617595 -0.306278 -0.382984 -0.547010  0.008517  -0.510100  -0.514966  -0.360227   0.016236  -0.086525  -0.028767   0.089325   0.223187  -0.172069  -0.165562   0.263279  -0.417298  -0.165531   0.017997   0.256129   0.302277   0.371870   0.194196   0.004732  -0.241855  -0.086430  -0.312172   0.114562   1.078538   0.250224  -0.017264  -0.434224  -0.053257  -0.174554   0.029593   0.315223  -0.163284   0.309744  -0.101035   0.075791  ...   0.042481   0.285718  -0.229131  -0.468140  -0.270336  -0.151340   -0.048428    0.090265    0.073199    0.167030   -0.454323   -0.515871    0.255998   -0.334426    0.503163   -0.247414   -0.052052   -0.059043   -0.129739    0.022377   -0.360486   -0.064188   -0.370561   -0.008928   -0.680877   -0.029006    0.163282   -0.156853   -0.127626    0.225402   -0.064305   -0.306678    0.023402   -0.257915          0.0          0.0   \n","3772  62.0     0.0                    0.0                   1.0        0  0.051889 -0.291487  0.405999 -0.360844 -0.227700 -0.609881 -0.569284 -0.463257 -0.558362  0.112397  -0.745912  -0.751715  -0.424379  -0.127704  -0.187913  -0.384001  -0.098747   0.129411  -0.093938  -0.226741   0.305927  -0.403796  -0.308442   0.151738   0.250305   0.173764   0.363731  -0.176612  -0.156437  -0.152733  -0.283506  -0.171910   0.050520   0.606909   0.503767  -0.050151  -0.605851   0.566846  -0.262849  -0.235550   0.624544  -0.238147   0.128646  -0.127235   0.462242  ...   0.056546   0.076265   0.064280  -0.595994  -0.325737  -0.139608    0.055215    0.007831    0.270474    0.381247   -0.347967   -0.388788    0.231242   -0.356458    0.468366   -0.242079   -0.061456    0.057655   -0.316158   -0.149110   -0.493682   -0.235335   -0.522741   -0.275454   -0.837066   -0.012493   -0.046785   -0.308019   -0.093133    0.641744    0.198202   -0.256241   -0.471158   -0.419664          0.0          0.0   \n","3773  23.0     1.0                    0.0                   0.0        0  0.131263 -0.198462  0.255228 -0.247892 -0.060811 -0.493453 -0.410188 -0.272169 -0.467947  0.083193  -0.603117  -0.531949  -0.437531  -0.097524  -0.097889  -0.240539  -0.162941   0.224409  -0.130183  -0.135267   0.289788  -0.380005  -0.243555   0.149766   0.346917   0.239214   0.323762   0.068050  -0.076920  -0.181245  -0.245248  -0.228070  -0.021809   0.653546   0.313528  -0.077071  -0.490280   0.087046  -0.303495  -0.118340   0.340005  -0.140137   0.056045  -0.139294   0.222145  ...   0.193020   0.324166   0.076427  -0.538110  -0.295215  -0.000099   -0.014354    0.026480    0.159162    0.386102   -0.391337   -0.321090    0.372665   -0.277563    0.439164   -0.208235   -0.148346   -0.118792   -0.052307   -0.084601   -0.353431   -0.114149   -0.390444    0.056941   -0.703342    0.190980   -0.029953   -0.215388   -0.237869    0.424635    0.148985   -0.077656   -0.182943   -0.384087          0.0          0.0   \n","\n","      age_disc8_2  age_disc8_3  age_disc8_4  age_disc8_5  age_disc8_6  age_disc8_7  age_disc4_0  age_disc4_1  age_disc4_2  age_disc4_3  symptom_class_0  symptom_class_1  symptom_class_2  vulnerable  \n","0             1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","1             1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","2             1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","3             0.0          1.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","4             0.0          1.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","...           ...          ...          ...          ...          ...          ...          ...          ...          ...          ...              ...              ...              ...         ...  \n","3769          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","3770          0.0          0.0          1.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0              1.0              0.0              0.0         0.0  \n","3771          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          1.0          0.0              1.0              0.0              0.0         0.0  \n","3772          0.0          0.0          0.0          0.0          1.0          0.0          0.0          0.0          0.0          1.0              0.0              1.0              0.0         0.0  \n","3773          1.0          0.0          0.0          0.0          0.0          0.0          0.0          1.0          0.0          0.0              1.0              0.0              0.0         0.0  \n","\n","[3774 rows x 149 columns]"],"text/html":["\n","  <div id=\"df-ef409ea0-c2db-4158-8921-34efe8d7fb15\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>respiratory_condition</th>\n","      <th>fever_or_muscle_pain</th>\n","      <th>covid19</th>\n","      <th>vggish_0</th>\n","      <th>vggish_1</th>\n","      <th>vggish_2</th>\n","      <th>vggish_3</th>\n","      <th>vggish_4</th>\n","      <th>vggish_5</th>\n","      <th>vggish_6</th>\n","      <th>vggish_7</th>\n","      <th>vggish_8</th>\n","      <th>vggish_9</th>\n","      <th>vggish_10</th>\n","      <th>vggish_11</th>\n","      <th>vggish_12</th>\n","      <th>vggish_13</th>\n","      <th>vggish_14</th>\n","      <th>vggish_15</th>\n","      <th>vggish_16</th>\n","      <th>vggish_17</th>\n","      <th>vggish_18</th>\n","      <th>vggish_19</th>\n","      <th>vggish_20</th>\n","      <th>vggish_21</th>\n","      <th>vggish_22</th>\n","      <th>vggish_23</th>\n","      <th>vggish_24</th>\n","      <th>vggish_25</th>\n","      <th>vggish_26</th>\n","      <th>vggish_27</th>\n","      <th>vggish_28</th>\n","      <th>vggish_29</th>\n","      <th>vggish_30</th>\n","      <th>vggish_31</th>\n","      <th>vggish_32</th>\n","      <th>vggish_33</th>\n","      <th>vggish_34</th>\n","      <th>vggish_35</th>\n","      <th>vggish_36</th>\n","      <th>vggish_37</th>\n","      <th>vggish_38</th>\n","      <th>vggish_39</th>\n","      <th>vggish_40</th>\n","      <th>vggish_41</th>\n","      <th>vggish_42</th>\n","      <th>vggish_43</th>\n","      <th>vggish_44</th>\n","      <th>...</th>\n","      <th>vggish_94</th>\n","      <th>vggish_95</th>\n","      <th>vggish_96</th>\n","      <th>vggish_97</th>\n","      <th>vggish_98</th>\n","      <th>vggish_99</th>\n","      <th>vggish_100</th>\n","      <th>vggish_101</th>\n","      <th>vggish_102</th>\n","      <th>vggish_103</th>\n","      <th>vggish_104</th>\n","      <th>vggish_105</th>\n","      <th>vggish_106</th>\n","      <th>vggish_107</th>\n","      <th>vggish_108</th>\n","      <th>vggish_109</th>\n","      <th>vggish_110</th>\n","      <th>vggish_111</th>\n","      <th>vggish_112</th>\n","      <th>vggish_113</th>\n","      <th>vggish_114</th>\n","      <th>vggish_115</th>\n","      <th>vggish_116</th>\n","      <th>vggish_117</th>\n","      <th>vggish_118</th>\n","      <th>vggish_119</th>\n","      <th>vggish_120</th>\n","      <th>vggish_121</th>\n","      <th>vggish_122</th>\n","      <th>vggish_123</th>\n","      <th>vggish_124</th>\n","      <th>vggish_125</th>\n","      <th>vggish_126</th>\n","      <th>vggish_127</th>\n","      <th>age_disc8_0</th>\n","      <th>age_disc8_1</th>\n","      <th>age_disc8_2</th>\n","      <th>age_disc8_3</th>\n","      <th>age_disc8_4</th>\n","      <th>age_disc8_5</th>\n","      <th>age_disc8_6</th>\n","      <th>age_disc8_7</th>\n","      <th>age_disc4_0</th>\n","      <th>age_disc4_1</th>\n","      <th>age_disc4_2</th>\n","      <th>age_disc4_3</th>\n","      <th>symptom_class_0</th>\n","      <th>symptom_class_1</th>\n","      <th>symptom_class_2</th>\n","      <th>vulnerable</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>27.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.237233</td>\n","      <td>-0.073406</td>\n","      <td>0.142055</td>\n","      <td>-0.539462</td>\n","      <td>-0.054550</td>\n","      <td>-0.457476</td>\n","      <td>-0.277106</td>\n","      <td>-0.374962</td>\n","      <td>-0.193999</td>\n","      <td>0.135108</td>\n","      <td>-0.685005</td>\n","      <td>-0.552347</td>\n","      <td>-0.382332</td>\n","      <td>-0.179146</td>\n","      <td>0.085340</td>\n","      <td>-0.287859</td>\n","      <td>-0.125214</td>\n","      <td>0.629780</td>\n","      <td>-0.346372</td>\n","      <td>-0.098754</td>\n","      <td>0.320120</td>\n","      <td>-0.379453</td>\n","      <td>-0.325220</td>\n","      <td>-0.036740</td>\n","      <td>0.455812</td>\n","      <td>0.213303</td>\n","      <td>0.531806</td>\n","      <td>0.213881</td>\n","      <td>-0.179417</td>\n","      <td>-0.270110</td>\n","      <td>-0.239606</td>\n","      <td>-0.155629</td>\n","      <td>-0.018713</td>\n","      <td>0.639012</td>\n","      <td>0.427209</td>\n","      <td>-0.128684</td>\n","      <td>-0.552983</td>\n","      <td>-0.000528</td>\n","      <td>-0.309293</td>\n","      <td>-0.021275</td>\n","      <td>0.377659</td>\n","      <td>-0.134463</td>\n","      <td>0.198480</td>\n","      <td>-0.261788</td>\n","      <td>0.014866</td>\n","      <td>...</td>\n","      <td>0.097233</td>\n","      <td>0.221708</td>\n","      <td>0.016513</td>\n","      <td>-0.646305</td>\n","      <td>-0.445976</td>\n","      <td>0.163898</td>\n","      <td>-0.216428</td>\n","      <td>-0.002031</td>\n","      <td>0.221547</td>\n","      <td>0.330069</td>\n","      <td>-0.362868</td>\n","      <td>-0.170088</td>\n","      <td>0.376592</td>\n","      <td>-0.461542</td>\n","      <td>0.362308</td>\n","      <td>-0.227808</td>\n","      <td>-0.194761</td>\n","      <td>-0.259064</td>\n","      <td>-0.121842</td>\n","      <td>0.059296</td>\n","      <td>-0.381911</td>\n","      <td>-0.278796</td>\n","      <td>-0.497189</td>\n","      <td>0.052893</td>\n","      <td>-0.858647</td>\n","      <td>0.253602</td>\n","      <td>0.176621</td>\n","      <td>-0.052145</td>\n","      <td>-0.088078</td>\n","      <td>0.347476</td>\n","      <td>0.031137</td>\n","      <td>-0.037026</td>\n","      <td>-0.059619</td>\n","      <td>-0.262939</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>26.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.246134</td>\n","      <td>-0.148192</td>\n","      <td>0.244273</td>\n","      <td>-0.538205</td>\n","      <td>-0.337523</td>\n","      <td>-0.593265</td>\n","      <td>-0.311994</td>\n","      <td>-0.429375</td>\n","      <td>-0.532566</td>\n","      <td>0.282096</td>\n","      <td>-0.808344</td>\n","      <td>-0.636284</td>\n","      <td>-0.525758</td>\n","      <td>-0.101947</td>\n","      <td>0.089822</td>\n","      <td>-0.137083</td>\n","      <td>-0.029002</td>\n","      <td>0.338129</td>\n","      <td>-0.202859</td>\n","      <td>-0.015267</td>\n","      <td>0.312917</td>\n","      <td>-0.102637</td>\n","      <td>-0.262040</td>\n","      <td>0.089277</td>\n","      <td>0.297634</td>\n","      <td>0.217412</td>\n","      <td>0.472763</td>\n","      <td>0.407411</td>\n","      <td>-0.235386</td>\n","      <td>-0.319075</td>\n","      <td>0.004311</td>\n","      <td>-0.177443</td>\n","      <td>0.011335</td>\n","      <td>0.692153</td>\n","      <td>0.180619</td>\n","      <td>0.222378</td>\n","      <td>-0.572982</td>\n","      <td>-0.159462</td>\n","      <td>-0.438346</td>\n","      <td>0.069036</td>\n","      <td>0.315911</td>\n","      <td>0.065443</td>\n","      <td>0.149918</td>\n","      <td>-0.115082</td>\n","      <td>0.204928</td>\n","      <td>...</td>\n","      <td>-0.058818</td>\n","      <td>0.199952</td>\n","      <td>-0.109857</td>\n","      <td>-0.661890</td>\n","      <td>-0.535261</td>\n","      <td>-0.008445</td>\n","      <td>-0.197057</td>\n","      <td>0.303818</td>\n","      <td>-0.003170</td>\n","      <td>0.225542</td>\n","      <td>-0.452083</td>\n","      <td>-0.424431</td>\n","      <td>0.354409</td>\n","      <td>-0.191299</td>\n","      <td>0.330623</td>\n","      <td>-0.153216</td>\n","      <td>-0.113831</td>\n","      <td>-0.202335</td>\n","      <td>-0.015565</td>\n","      <td>-0.017402</td>\n","      <td>-0.653620</td>\n","      <td>-0.330262</td>\n","      <td>-0.408605</td>\n","      <td>0.028116</td>\n","      <td>-1.047375</td>\n","      <td>0.246120</td>\n","      <td>0.070484</td>\n","      <td>-0.083004</td>\n","      <td>-0.088040</td>\n","      <td>0.252036</td>\n","      <td>-0.007256</td>\n","      <td>0.090935</td>\n","      <td>-0.067936</td>\n","      <td>-0.358746</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>27.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.061355</td>\n","      <td>-0.104770</td>\n","      <td>0.196464</td>\n","      <td>0.256682</td>\n","      <td>0.008873</td>\n","      <td>-0.406786</td>\n","      <td>-0.341376</td>\n","      <td>-0.238717</td>\n","      <td>-0.307947</td>\n","      <td>0.039246</td>\n","      <td>-0.475410</td>\n","      <td>-0.464350</td>\n","      <td>-0.124527</td>\n","      <td>-0.061451</td>\n","      <td>-0.096154</td>\n","      <td>-0.199233</td>\n","      <td>0.069042</td>\n","      <td>0.077799</td>\n","      <td>0.014512</td>\n","      <td>-0.261682</td>\n","      <td>0.219502</td>\n","      <td>-0.286834</td>\n","      <td>-0.178618</td>\n","      <td>0.106887</td>\n","      <td>0.232501</td>\n","      <td>0.102398</td>\n","      <td>0.084113</td>\n","      <td>0.075658</td>\n","      <td>0.324012</td>\n","      <td>-0.062375</td>\n","      <td>-0.094696</td>\n","      <td>-0.273715</td>\n","      <td>0.143767</td>\n","      <td>0.459655</td>\n","      <td>0.268433</td>\n","      <td>-0.109267</td>\n","      <td>-0.437196</td>\n","      <td>0.252963</td>\n","      <td>-0.247513</td>\n","      <td>-0.107284</td>\n","      <td>0.291054</td>\n","      <td>-0.064484</td>\n","      <td>0.082671</td>\n","      <td>-0.155126</td>\n","      <td>0.336953</td>\n","      <td>...</td>\n","      <td>0.224578</td>\n","      <td>0.291128</td>\n","      <td>0.063938</td>\n","      <td>-0.272569</td>\n","      <td>-0.237854</td>\n","      <td>-0.038205</td>\n","      <td>0.077042</td>\n","      <td>0.109512</td>\n","      <td>0.081803</td>\n","      <td>0.132673</td>\n","      <td>-0.329243</td>\n","      <td>-0.353951</td>\n","      <td>0.336664</td>\n","      <td>-0.255079</td>\n","      <td>0.467882</td>\n","      <td>-0.175494</td>\n","      <td>-0.225324</td>\n","      <td>0.365005</td>\n","      <td>0.024953</td>\n","      <td>0.074411</td>\n","      <td>-0.251430</td>\n","      <td>0.023905</td>\n","      <td>-0.355071</td>\n","      <td>-0.024995</td>\n","      <td>-0.566465</td>\n","      <td>-0.008698</td>\n","      <td>-0.067261</td>\n","      <td>-0.180592</td>\n","      <td>-0.229741</td>\n","      <td>0.200737</td>\n","      <td>0.144086</td>\n","      <td>-0.227922</td>\n","      <td>-0.003502</td>\n","      <td>-0.203552</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.134299</td>\n","      <td>-0.150072</td>\n","      <td>0.405094</td>\n","      <td>-0.510863</td>\n","      <td>-0.086288</td>\n","      <td>-0.418875</td>\n","      <td>0.001268</td>\n","      <td>-0.251929</td>\n","      <td>-0.498573</td>\n","      <td>0.072656</td>\n","      <td>-0.201595</td>\n","      <td>-0.357154</td>\n","      <td>-0.282462</td>\n","      <td>-0.169092</td>\n","      <td>-0.099354</td>\n","      <td>-0.152573</td>\n","      <td>-0.107116</td>\n","      <td>0.530403</td>\n","      <td>-0.247494</td>\n","      <td>-0.064046</td>\n","      <td>0.335060</td>\n","      <td>-0.395883</td>\n","      <td>-0.224059</td>\n","      <td>-0.209563</td>\n","      <td>0.361936</td>\n","      <td>0.483415</td>\n","      <td>0.785008</td>\n","      <td>0.148584</td>\n","      <td>-0.128865</td>\n","      <td>-0.204075</td>\n","      <td>-0.192620</td>\n","      <td>-0.088760</td>\n","      <td>-0.030923</td>\n","      <td>0.896775</td>\n","      <td>0.330546</td>\n","      <td>-0.102979</td>\n","      <td>-0.244311</td>\n","      <td>-0.307154</td>\n","      <td>-0.237432</td>\n","      <td>-0.107218</td>\n","      <td>0.666889</td>\n","      <td>-0.215102</td>\n","      <td>0.207663</td>\n","      <td>-0.194329</td>\n","      <td>0.128014</td>\n","      <td>...</td>\n","      <td>0.059770</td>\n","      <td>0.377729</td>\n","      <td>-0.127123</td>\n","      <td>-0.457038</td>\n","      <td>-0.179448</td>\n","      <td>0.072016</td>\n","      <td>0.060581</td>\n","      <td>0.187417</td>\n","      <td>0.153641</td>\n","      <td>0.205097</td>\n","      <td>-0.260940</td>\n","      <td>-0.244228</td>\n","      <td>0.232359</td>\n","      <td>-0.593012</td>\n","      <td>0.237451</td>\n","      <td>-0.104367</td>\n","      <td>0.126822</td>\n","      <td>-0.298137</td>\n","      <td>-0.204943</td>\n","      <td>-0.079213</td>\n","      <td>-0.305575</td>\n","      <td>-0.057126</td>\n","      <td>-0.387948</td>\n","      <td>-0.043517</td>\n","      <td>-0.397912</td>\n","      <td>0.062316</td>\n","      <td>0.193401</td>\n","      <td>-0.161362</td>\n","      <td>-0.163191</td>\n","      <td>0.223964</td>\n","      <td>-0.052330</td>\n","      <td>-0.260159</td>\n","      <td>0.033872</td>\n","      <td>-0.185345</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>30.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.003999</td>\n","      <td>-0.111768</td>\n","      <td>0.203734</td>\n","      <td>-0.315624</td>\n","      <td>0.017499</td>\n","      <td>-0.426326</td>\n","      <td>-0.214551</td>\n","      <td>-0.391016</td>\n","      <td>-0.603783</td>\n","      <td>0.163266</td>\n","      <td>-0.374928</td>\n","      <td>-0.397450</td>\n","      <td>-0.407722</td>\n","      <td>-0.112975</td>\n","      <td>-0.052706</td>\n","      <td>-0.255393</td>\n","      <td>-0.050976</td>\n","      <td>0.384551</td>\n","      <td>-0.172650</td>\n","      <td>-0.037434</td>\n","      <td>0.243605</td>\n","      <td>-0.092323</td>\n","      <td>-0.337894</td>\n","      <td>0.015879</td>\n","      <td>0.259033</td>\n","      <td>0.281155</td>\n","      <td>0.412646</td>\n","      <td>0.127118</td>\n","      <td>0.044594</td>\n","      <td>-0.146128</td>\n","      <td>-0.161955</td>\n","      <td>-0.105097</td>\n","      <td>0.054556</td>\n","      <td>0.359450</td>\n","      <td>0.364263</td>\n","      <td>-0.158441</td>\n","      <td>-0.350316</td>\n","      <td>-0.143393</td>\n","      <td>-0.397220</td>\n","      <td>0.020893</td>\n","      <td>0.451105</td>\n","      <td>-0.084920</td>\n","      <td>0.002359</td>\n","      <td>-0.116057</td>\n","      <td>0.249334</td>\n","      <td>...</td>\n","      <td>0.063617</td>\n","      <td>0.294652</td>\n","      <td>-0.040682</td>\n","      <td>-0.571301</td>\n","      <td>-0.259738</td>\n","      <td>0.110305</td>\n","      <td>-0.209054</td>\n","      <td>0.250293</td>\n","      <td>0.106468</td>\n","      <td>0.062130</td>\n","      <td>-0.293578</td>\n","      <td>-0.304958</td>\n","      <td>0.187921</td>\n","      <td>-0.595932</td>\n","      <td>0.258083</td>\n","      <td>-0.161055</td>\n","      <td>-0.107944</td>\n","      <td>-0.097554</td>\n","      <td>-0.127796</td>\n","      <td>-0.115905</td>\n","      <td>-0.373250</td>\n","      <td>-0.201690</td>\n","      <td>-0.280232</td>\n","      <td>-0.184360</td>\n","      <td>-0.616527</td>\n","      <td>0.065762</td>\n","      <td>0.004534</td>\n","      <td>-0.077136</td>\n","      <td>-0.274606</td>\n","      <td>0.431737</td>\n","      <td>0.111949</td>\n","      <td>-0.047648</td>\n","      <td>-0.066311</td>\n","      <td>-0.203175</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3769</th>\n","      <td>12.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.246177</td>\n","      <td>-0.034868</td>\n","      <td>-0.129657</td>\n","      <td>0.322590</td>\n","      <td>-0.035816</td>\n","      <td>-0.070364</td>\n","      <td>-0.049894</td>\n","      <td>-0.113831</td>\n","      <td>-0.177191</td>\n","      <td>-0.038279</td>\n","      <td>-0.144693</td>\n","      <td>-0.224197</td>\n","      <td>0.585655</td>\n","      <td>0.157981</td>\n","      <td>-0.093875</td>\n","      <td>-0.038411</td>\n","      <td>0.241772</td>\n","      <td>0.322176</td>\n","      <td>0.244930</td>\n","      <td>-0.338993</td>\n","      <td>-0.056917</td>\n","      <td>-0.147660</td>\n","      <td>-0.060325</td>\n","      <td>-0.120323</td>\n","      <td>0.223656</td>\n","      <td>-0.026892</td>\n","      <td>-0.111760</td>\n","      <td>0.266951</td>\n","      <td>0.456231</td>\n","      <td>-0.000417</td>\n","      <td>-0.045390</td>\n","      <td>-0.142784</td>\n","      <td>0.138059</td>\n","      <td>0.080557</td>\n","      <td>0.251560</td>\n","      <td>-0.261386</td>\n","      <td>-0.196583</td>\n","      <td>-0.055117</td>\n","      <td>-0.084223</td>\n","      <td>-0.119749</td>\n","      <td>0.373634</td>\n","      <td>0.003988</td>\n","      <td>0.048978</td>\n","      <td>-0.088418</td>\n","      <td>0.591958</td>\n","      <td>...</td>\n","      <td>0.249724</td>\n","      <td>0.246064</td>\n","      <td>-0.026176</td>\n","      <td>0.104687</td>\n","      <td>-0.049091</td>\n","      <td>0.103767</td>\n","      <td>0.124341</td>\n","      <td>0.511819</td>\n","      <td>-0.057119</td>\n","      <td>0.265238</td>\n","      <td>-0.373399</td>\n","      <td>-0.099381</td>\n","      <td>0.150342</td>\n","      <td>-0.215783</td>\n","      <td>0.000922</td>\n","      <td>-0.016490</td>\n","      <td>-0.073646</td>\n","      <td>0.701981</td>\n","      <td>0.070872</td>\n","      <td>-0.037070</td>\n","      <td>-0.181049</td>\n","      <td>0.067465</td>\n","      <td>-0.207613</td>\n","      <td>0.023224</td>\n","      <td>-0.092705</td>\n","      <td>-0.034168</td>\n","      <td>-0.087253</td>\n","      <td>-0.025519</td>\n","      <td>-0.038996</td>\n","      <td>-0.108309</td>\n","      <td>0.217582</td>\n","      <td>-0.135895</td>\n","      <td>0.222678</td>\n","      <td>0.166042</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3770</th>\n","      <td>40.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.158730</td>\n","      <td>-0.199019</td>\n","      <td>0.178682</td>\n","      <td>-0.004860</td>\n","      <td>-0.096780</td>\n","      <td>-0.397793</td>\n","      <td>-0.399140</td>\n","      <td>-0.317382</td>\n","      <td>-0.261474</td>\n","      <td>-0.093012</td>\n","      <td>-0.539748</td>\n","      <td>-0.491350</td>\n","      <td>-0.229894</td>\n","      <td>-0.181734</td>\n","      <td>-0.172093</td>\n","      <td>-0.309356</td>\n","      <td>0.008652</td>\n","      <td>0.062677</td>\n","      <td>-0.019963</td>\n","      <td>-0.379514</td>\n","      <td>0.162992</td>\n","      <td>-0.463177</td>\n","      <td>-0.180228</td>\n","      <td>0.160992</td>\n","      <td>0.264425</td>\n","      <td>0.313263</td>\n","      <td>0.199507</td>\n","      <td>-0.000034</td>\n","      <td>0.079976</td>\n","      <td>-0.126106</td>\n","      <td>-0.204884</td>\n","      <td>-0.226403</td>\n","      <td>-0.036004</td>\n","      <td>0.847588</td>\n","      <td>0.416150</td>\n","      <td>-0.128587</td>\n","      <td>-0.468832</td>\n","      <td>0.187249</td>\n","      <td>-0.124072</td>\n","      <td>-0.239662</td>\n","      <td>0.444036</td>\n","      <td>-0.207752</td>\n","      <td>0.118965</td>\n","      <td>-0.136653</td>\n","      <td>0.361643</td>\n","      <td>...</td>\n","      <td>0.237841</td>\n","      <td>0.110573</td>\n","      <td>0.063766</td>\n","      <td>-0.413122</td>\n","      <td>-0.240286</td>\n","      <td>-0.102974</td>\n","      <td>0.177032</td>\n","      <td>-0.009918</td>\n","      <td>0.065011</td>\n","      <td>0.486265</td>\n","      <td>-0.365485</td>\n","      <td>-0.320821</td>\n","      <td>0.291606</td>\n","      <td>-0.350020</td>\n","      <td>0.355880</td>\n","      <td>-0.246199</td>\n","      <td>-0.097304</td>\n","      <td>0.109610</td>\n","      <td>-0.077862</td>\n","      <td>-0.122714</td>\n","      <td>-0.315635</td>\n","      <td>-0.025315</td>\n","      <td>-0.448783</td>\n","      <td>0.000860</td>\n","      <td>-0.650030</td>\n","      <td>0.067289</td>\n","      <td>-0.048413</td>\n","      <td>-0.276386</td>\n","      <td>-0.096411</td>\n","      <td>0.200029</td>\n","      <td>0.124908</td>\n","      <td>-0.157733</td>\n","      <td>-0.173501</td>\n","      <td>-0.230978</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3771</th>\n","      <td>52.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>-0.218338</td>\n","      <td>-0.279103</td>\n","      <td>0.525833</td>\n","      <td>-0.108297</td>\n","      <td>-0.185997</td>\n","      <td>-0.617595</td>\n","      <td>-0.306278</td>\n","      <td>-0.382984</td>\n","      <td>-0.547010</td>\n","      <td>0.008517</td>\n","      <td>-0.510100</td>\n","      <td>-0.514966</td>\n","      <td>-0.360227</td>\n","      <td>0.016236</td>\n","      <td>-0.086525</td>\n","      <td>-0.028767</td>\n","      <td>0.089325</td>\n","      <td>0.223187</td>\n","      <td>-0.172069</td>\n","      <td>-0.165562</td>\n","      <td>0.263279</td>\n","      <td>-0.417298</td>\n","      <td>-0.165531</td>\n","      <td>0.017997</td>\n","      <td>0.256129</td>\n","      <td>0.302277</td>\n","      <td>0.371870</td>\n","      <td>0.194196</td>\n","      <td>0.004732</td>\n","      <td>-0.241855</td>\n","      <td>-0.086430</td>\n","      <td>-0.312172</td>\n","      <td>0.114562</td>\n","      <td>1.078538</td>\n","      <td>0.250224</td>\n","      <td>-0.017264</td>\n","      <td>-0.434224</td>\n","      <td>-0.053257</td>\n","      <td>-0.174554</td>\n","      <td>0.029593</td>\n","      <td>0.315223</td>\n","      <td>-0.163284</td>\n","      <td>0.309744</td>\n","      <td>-0.101035</td>\n","      <td>0.075791</td>\n","      <td>...</td>\n","      <td>0.042481</td>\n","      <td>0.285718</td>\n","      <td>-0.229131</td>\n","      <td>-0.468140</td>\n","      <td>-0.270336</td>\n","      <td>-0.151340</td>\n","      <td>-0.048428</td>\n","      <td>0.090265</td>\n","      <td>0.073199</td>\n","      <td>0.167030</td>\n","      <td>-0.454323</td>\n","      <td>-0.515871</td>\n","      <td>0.255998</td>\n","      <td>-0.334426</td>\n","      <td>0.503163</td>\n","      <td>-0.247414</td>\n","      <td>-0.052052</td>\n","      <td>-0.059043</td>\n","      <td>-0.129739</td>\n","      <td>0.022377</td>\n","      <td>-0.360486</td>\n","      <td>-0.064188</td>\n","      <td>-0.370561</td>\n","      <td>-0.008928</td>\n","      <td>-0.680877</td>\n","      <td>-0.029006</td>\n","      <td>0.163282</td>\n","      <td>-0.156853</td>\n","      <td>-0.127626</td>\n","      <td>0.225402</td>\n","      <td>-0.064305</td>\n","      <td>-0.306678</td>\n","      <td>0.023402</td>\n","      <td>-0.257915</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3772</th>\n","      <td>62.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0.051889</td>\n","      <td>-0.291487</td>\n","      <td>0.405999</td>\n","      <td>-0.360844</td>\n","      <td>-0.227700</td>\n","      <td>-0.609881</td>\n","      <td>-0.569284</td>\n","      <td>-0.463257</td>\n","      <td>-0.558362</td>\n","      <td>0.112397</td>\n","      <td>-0.745912</td>\n","      <td>-0.751715</td>\n","      <td>-0.424379</td>\n","      <td>-0.127704</td>\n","      <td>-0.187913</td>\n","      <td>-0.384001</td>\n","      <td>-0.098747</td>\n","      <td>0.129411</td>\n","      <td>-0.093938</td>\n","      <td>-0.226741</td>\n","      <td>0.305927</td>\n","      <td>-0.403796</td>\n","      <td>-0.308442</td>\n","      <td>0.151738</td>\n","      <td>0.250305</td>\n","      <td>0.173764</td>\n","      <td>0.363731</td>\n","      <td>-0.176612</td>\n","      <td>-0.156437</td>\n","      <td>-0.152733</td>\n","      <td>-0.283506</td>\n","      <td>-0.171910</td>\n","      <td>0.050520</td>\n","      <td>0.606909</td>\n","      <td>0.503767</td>\n","      <td>-0.050151</td>\n","      <td>-0.605851</td>\n","      <td>0.566846</td>\n","      <td>-0.262849</td>\n","      <td>-0.235550</td>\n","      <td>0.624544</td>\n","      <td>-0.238147</td>\n","      <td>0.128646</td>\n","      <td>-0.127235</td>\n","      <td>0.462242</td>\n","      <td>...</td>\n","      <td>0.056546</td>\n","      <td>0.076265</td>\n","      <td>0.064280</td>\n","      <td>-0.595994</td>\n","      <td>-0.325737</td>\n","      <td>-0.139608</td>\n","      <td>0.055215</td>\n","      <td>0.007831</td>\n","      <td>0.270474</td>\n","      <td>0.381247</td>\n","      <td>-0.347967</td>\n","      <td>-0.388788</td>\n","      <td>0.231242</td>\n","      <td>-0.356458</td>\n","      <td>0.468366</td>\n","      <td>-0.242079</td>\n","      <td>-0.061456</td>\n","      <td>0.057655</td>\n","      <td>-0.316158</td>\n","      <td>-0.149110</td>\n","      <td>-0.493682</td>\n","      <td>-0.235335</td>\n","      <td>-0.522741</td>\n","      <td>-0.275454</td>\n","      <td>-0.837066</td>\n","      <td>-0.012493</td>\n","      <td>-0.046785</td>\n","      <td>-0.308019</td>\n","      <td>-0.093133</td>\n","      <td>0.641744</td>\n","      <td>0.198202</td>\n","      <td>-0.256241</td>\n","      <td>-0.471158</td>\n","      <td>-0.419664</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3773</th>\n","      <td>23.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.131263</td>\n","      <td>-0.198462</td>\n","      <td>0.255228</td>\n","      <td>-0.247892</td>\n","      <td>-0.060811</td>\n","      <td>-0.493453</td>\n","      <td>-0.410188</td>\n","      <td>-0.272169</td>\n","      <td>-0.467947</td>\n","      <td>0.083193</td>\n","      <td>-0.603117</td>\n","      <td>-0.531949</td>\n","      <td>-0.437531</td>\n","      <td>-0.097524</td>\n","      <td>-0.097889</td>\n","      <td>-0.240539</td>\n","      <td>-0.162941</td>\n","      <td>0.224409</td>\n","      <td>-0.130183</td>\n","      <td>-0.135267</td>\n","      <td>0.289788</td>\n","      <td>-0.380005</td>\n","      <td>-0.243555</td>\n","      <td>0.149766</td>\n","      <td>0.346917</td>\n","      <td>0.239214</td>\n","      <td>0.323762</td>\n","      <td>0.068050</td>\n","      <td>-0.076920</td>\n","      <td>-0.181245</td>\n","      <td>-0.245248</td>\n","      <td>-0.228070</td>\n","      <td>-0.021809</td>\n","      <td>0.653546</td>\n","      <td>0.313528</td>\n","      <td>-0.077071</td>\n","      <td>-0.490280</td>\n","      <td>0.087046</td>\n","      <td>-0.303495</td>\n","      <td>-0.118340</td>\n","      <td>0.340005</td>\n","      <td>-0.140137</td>\n","      <td>0.056045</td>\n","      <td>-0.139294</td>\n","      <td>0.222145</td>\n","      <td>...</td>\n","      <td>0.193020</td>\n","      <td>0.324166</td>\n","      <td>0.076427</td>\n","      <td>-0.538110</td>\n","      <td>-0.295215</td>\n","      <td>-0.000099</td>\n","      <td>-0.014354</td>\n","      <td>0.026480</td>\n","      <td>0.159162</td>\n","      <td>0.386102</td>\n","      <td>-0.391337</td>\n","      <td>-0.321090</td>\n","      <td>0.372665</td>\n","      <td>-0.277563</td>\n","      <td>0.439164</td>\n","      <td>-0.208235</td>\n","      <td>-0.148346</td>\n","      <td>-0.118792</td>\n","      <td>-0.052307</td>\n","      <td>-0.084601</td>\n","      <td>-0.353431</td>\n","      <td>-0.114149</td>\n","      <td>-0.390444</td>\n","      <td>0.056941</td>\n","      <td>-0.703342</td>\n","      <td>0.190980</td>\n","      <td>-0.029953</td>\n","      <td>-0.215388</td>\n","      <td>-0.237869</td>\n","      <td>0.424635</td>\n","      <td>0.148985</td>\n","      <td>-0.077656</td>\n","      <td>-0.182943</td>\n","      <td>-0.384087</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3774 rows × 149 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef409ea0-c2db-4158-8921-34efe8d7fb15')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ef409ea0-c2db-4158-8921-34efe8d7fb15 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ef409ea0-c2db-4158-8921-34efe8d7fb15');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["# Define utility funcions"],"metadata":{"id":"oo3zFM7xAJrM"},"id":"oo3zFM7xAJrM"},{"cell_type":"code","source":["# optuna function\n","def optuna_objective_function(trial: Trial, fold, train_x, train_y, train_groups, val_x, val_y, val_groups, categoIdx,\n","                              model_name, output_container, ntrees=1000, eta=1e-2):\n","    if model_name == \"LGB_GOSS\":\n","        tuning_params = {\n","            \"num_leaves\": trial.suggest_categorical(\"num_leaves\", [pow(2, i) - 1 for i in [4, 5, 6, 7, 8]]),\n","            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.8, step=0.05),\n","            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.8, step=0.05),\n","            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.001, 10.0, log=True),\n","            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.001, 10.0, log=True),\n","            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 3, 99, log=True),\n","            \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 0.2, 5.0, log=True),\n","        }\n","        cb_list = [\n","            lgb.early_stopping(stopping_rounds=int(ntrees * 0.2), first_metric_only=True, verbose=False, min_delta=0.001),\n","        ]\n","\n","        model = lgb.LGBMClassifier(boosting_type=\"goss\", objective=\"binary\",\n","                                   n_estimators=ntrees, learning_rate=eta,\n","                                   random_state=fold, device_type=\"gpu\",\n","                                   verbose=-1, **tuning_params)\n","        model.fit(train_x, train_y, categorical_feature=categoIdx,\n","                  eval_set=(val_x,val_y), eval_metric=\"auc\", callbacks=cb_list)\n","    elif model_name == \"XGB_GBT\":\n","        tuning_params = {\n","            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 8, step=1),\n","            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.8, step=0.05),\n","            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.8, step=0.05),\n","            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 10.0, step=0.01),\n","            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.01, 10.0, step=0.01),\n","            \"gamma\": trial.suggest_float(\"gamma\", 0.01, 10.0, step=0.01),\n","            \"max_delta_step\":  trial.suggest_float(\"max_delta_step\", 0.01, 10.0, step=0.01)\n","        }\n","        model = xgb.XGBRanker(booster=\"gbtree\", objective=\"rank:ndcg\",\n","                            tree_method=\"gpu_hist\", sampling_method=\"gradient_based\",\n","                            n_estimators=int(ntrees * 0.2),\n","                            learning_rate=eta / 10,\n","                            random_state=fold, verbosity=0,\n","                            **tuning_params)\n","        model.fit(train_x, train_y, group=train_groups, verbose=False)\n","    elif model_name == \"CAT_GBM\":\n","        tuning_params = {\n","            # \"n_estimators\" : trial.suggest_int(\"n_estimators\", 500, 5000, step=500),\n","            # \"learning_rate\" : trial.suggest_categorical(\"learning_rate\", [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]),\n","            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 8, step=1),\n","            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.05, 0.95, step=0.05),\n","            # \"rsm\": trial.suggest_float(\"rsm\", 0.5, 0.9, step=0.05),\n","            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.01, 10.0, step=0.01),\n","            \"random_strength\": trial.suggest_float(\"random_strength\", 0.01, 1.5, step=0.01),\n","            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 3, 99, step=2),  \n","            \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 3.0, step=0.01)\n","        }\n","\n","        model = cat.CatBoostClassifier(boosting_type=\"Plain\", loss_function=\"MultiClass\", task_type=\"GPU\",\n","                                    n_estimators=int(ntrees * 0.2),\n","                                    learning_rate=eta / 10,\n","                                    one_hot_max_size=3, leaf_estimation_method=\"Gradient\",\n","                                    # leaf_estimation_iterations=5,\n","                                    # max_ctr_complexity=2,\n","                                    logging_level=\"Silent\", random_state=fold, thread_count=cpu_count(),\n","                                    **tuning_params)\n","        model.fit(train_x, train_y, cat_features=categoIdx)\n","    elif model_name == \"CAT_ORD\":\n","        tuning_params = {\n","            # \"n_estimators\" : trial.suggest_int(\"n_estimators\", 500, 5000, step=500),\n","            # \"learning_rate\" : trial.suggest_categorical(\"learning_rate\", [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]),\n","            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 8, step=1),\n","            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.05, 0.95, step=0.05),\n","            # \"rsm\": trial.suggest_float(\"rsm\", 0.5, 0.9, step=0.05),\n","            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.01, 10.0, step=0.01),\n","            \"random_strength\": trial.suggest_float(\"random_strength\", 0.01, 1.5, step=0.01),\n","            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 3, 99, step=2),\n","            \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 3.0, step=0.01)\n","        }\n","        \n","        model = cat.CatBoostClassifier(boosting_type=\"Ordered\", loss_function=\"MultiClass\", task_type=\"GPU\",\n","                                    n_estimators=int(ntrees * 0.2),\n","                                    learning_rate=eta / 10,\n","                                    one_hot_max_size=3, leaf_estimation_method=\"Gradient\",\n","                                    # leaf_estimation_iterations=5,\n","                                    # max_ctr_complexity=2,\n","                                    logging_level=\"Silent\", random_state=fold, thread_count=cpu_count(),\n","                                    **tuning_params)\n","        model.fit(train_x, train_y, cat_features=categoIdx)\n","    else:\n","        print(\"unknown\")\n","        return -1\n","    \n","    pred = model.predict_proba(val_x)\n","    # f1 score\n","    optuna_score = metrics.f1_score(val_y.tolist(), [1.0 if i > threshold else 0.0 for i in pred[:,1]])\n","\n","    if optuna_score > output_container[\"score\"]:\n","        output_container[\"model\"] = model\n","        output_container[\"pred\"] = pred\n","        output_container[\"score\"] = optuna_score\n","\n","    return optuna_score"],"metadata":{"id":"lhY0rmtQ_Fyi"},"id":"lhY0rmtQ_Fyi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"22qoM6-hANaQ"},"id":"22qoM6-hANaQ"},{"cell_type":"code","source":["architecture_name = \"vggish_lgb_goss_thresholdOri_tuningPosWeight_try1\"\n","createFolder(folder_path + \"architecture/\" + architecture_name)"],"metadata":{"id":"sA2es-s0huVj"},"id":"sA2es-s0huVj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def do_fold_training(fold, train_idx, val_idx, sample_weight=None, finetuning=False):\n","    tmp_time = time()\n","    print(\"\\n===== Fold\", fold, \"=====\\n\")\n","    global val_pred, test_pred\n","\n","    wandb.init(\n","        project=\"dacon_covid19_classification\",\n","        group=architecture_name,\n","        name=\"fold_\" + str(fold)\n","    ); wandb.config.step = 0\n","\n","    train_x = df_full.iloc[train_idx][num_vars + bin_vars + audio_vars].copy()\n","    train_y = df_full.iloc[train_idx][target_var].copy()\n","    \n","    val_x = df_full.iloc[val_idx][num_vars + bin_vars + audio_vars].copy()\n","    val_y = df_full.iloc[val_idx][target_var].copy()\n","    \n","    output_container = {\"model\": None, \"pred\": None, \"score\": -np.inf}\n","    optuna_timout = int(6 * 3600 / kfolds_spliter.get_n_splits())\n","    optuna_study = create_study(direction='maximize', sampler=TPESampler())\n","    optuna_study.optimize(\n","        lambda trial: optuna_objective_function(\n","            trial, fold, train_x, train_y, None, val_x, val_y, None, categoIdx=None, model_name=\"LGB_GOSS\", output_container=output_container, ntrees=ntrees, eta=eta\n","        ),\n","        n_jobs=1, n_trials=300, timeout=optuna_timout\n","    )\n","    \n","    model_list.append(output_container[\"model\"])\n","    params_list.append(optuna_study.best_params)\n","    print(\"fold\", fold, \"best params :\", params_list[-1])\n","    val_pred[val_idx] = output_container[\"pred\"]\n","    fold_metric.append(output_container[\"score\"])\n","    print(\"fold\", fold, \"score :\", fold_metric[-1])\n","\n","    test_x = df_test[num_vars + bin_vars + audio_vars].copy()\n","    test_pred += model_list[-1].predict_proba(test_x) / n_folds\n","\n","    wandb.log({\"fold\": fold,\n","               \"logloss\": metrics.log_loss(val_y.tolist(), val_pred[val_idx, 1]),\n","               \"auc\": metrics.roc_auc_score(val_y.tolist(), val_pred[val_idx, 1]),\n","               \"f1_score\": fold_metric[-1]})\n","    wandb.finish()\n"," \n","    print(\"fold\", fold, \"time to training :\", round(time() - tmp_time, 3))"],"metadata":{"id":"qxbjShlK_GOC"},"id":"qxbjShlK_GOC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# learning parameter setting\n","# The range of converged trees is about 2000 ~ 4000\n","ntrees = 5000\n","eta = 5e-3\n","\n","# ntrees = 1000\n","# eta = 1e-2\n","\n","threshold = round(df_full[target_var].mean(), 5)\n","# threshold = 0.5"],"metadata":{"id":"pTlOiZye_GLs"},"id":"pTlOiZye_GLs","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d646f9d2-a911-4ad7-83f3-1a5b79cb5a2a","metadata":{"id":"d646f9d2-a911-4ad7-83f3-1a5b79cb5a2a","colab":{"base_uri":"https://localhost:8080/","height":662},"outputId":"476351da-0dcb-460d-b4ae-6013b1f21bbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["[debug] memory usage:    746.559 MB\n","\n","===== Fold 0 =====\n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrony\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.18"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/LightGBM/python-package/wandb/run-20220616_042132-1rqs3n2v</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/frony/dacon_covid19_classification/runs/1rqs3n2v\" target=\"_blank\">fold_0</a></strong> to <a href=\"https://wandb.ai/frony/dacon_covid19_classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-16 04:21:35,078]\u001b[0m A new study created in memory with name: no-name-2c51b461-fff4-4f95-aa28-9ec6ab3251ad\u001b[0m\n","\u001b[32m[I 2022-06-16 04:21:50,690]\u001b[0m Trial 0 finished with value: 0.1794871794871795 and parameters: {'num_leaves': 15, 'subsample': 0.8, 'colsample_bytree': 0.65, 'reg_lambda': 0.02589094297065214, 'min_child_weight': 0.6880660203135626, 'min_child_samples': 17, 'scale_pos_weight': 1.107691836763628}. Best is trial 0 with value: 0.1794871794871795.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:22:11,505]\u001b[0m Trial 1 finished with value: 0.1809045226130653 and parameters: {'num_leaves': 127, 'subsample': 0.75, 'colsample_bytree': 0.5, 'reg_lambda': 0.02386718429551674, 'min_child_weight': 1.1668092599252171, 'min_child_samples': 18, 'scale_pos_weight': 0.5432022857929192}. Best is trial 1 with value: 0.1809045226130653.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:22:25,046]\u001b[0m Trial 2 finished with value: 0.1884498480243161 and parameters: {'num_leaves': 255, 'subsample': 0.75, 'colsample_bytree': 0.7, 'reg_lambda': 0.10540546892906015, 'min_child_weight': 0.17254376196171892, 'min_child_samples': 22, 'scale_pos_weight': 2.1624084845400446}. Best is trial 2 with value: 0.1884498480243161.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:22:50,255]\u001b[0m Trial 3 finished with value: 0.20512820512820515 and parameters: {'num_leaves': 255, 'subsample': 0.6, 'colsample_bytree': 0.7, 'reg_lambda': 0.15210422539632865, 'min_child_weight': 0.010238573651960542, 'min_child_samples': 6, 'scale_pos_weight': 0.7110743839933777}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:23:42,535]\u001b[0m Trial 4 finished with value: 0.12068965517241378 and parameters: {'num_leaves': 255, 'subsample': 0.65, 'colsample_bytree': 0.5, 'reg_lambda': 0.007823273077695423, 'min_child_weight': 0.003519678574383172, 'min_child_samples': 3, 'scale_pos_weight': 1.1318803570935716}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:23:50,248]\u001b[0m Trial 5 finished with value: 0.17582417582417584 and parameters: {'num_leaves': 63, 'subsample': 0.55, 'colsample_bytree': 0.5, 'reg_lambda': 4.805184905460639, 'min_child_weight': 0.0496433074035737, 'min_child_samples': 59, 'scale_pos_weight': 0.2215634765296849}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:24:03,990]\u001b[0m Trial 6 finished with value: 0.18025751072961374 and parameters: {'num_leaves': 15, 'subsample': 0.55, 'colsample_bytree': 0.5, 'reg_lambda': 5.437434563991228, 'min_child_weight': 2.3186663116551833, 'min_child_samples': 9, 'scale_pos_weight': 0.8510000278690987}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:24:19,445]\u001b[0m Trial 7 finished with value: 0.18243243243243243 and parameters: {'num_leaves': 127, 'subsample': 0.7, 'colsample_bytree': 0.6, 'reg_lambda': 0.329903667878416, 'min_child_weight': 2.1350248229369524, 'min_child_samples': 92, 'scale_pos_weight': 1.1306524975750447}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:24:40,088]\u001b[0m Trial 8 finished with value: 0.16000000000000003 and parameters: {'num_leaves': 255, 'subsample': 0.7, 'colsample_bytree': 0.55, 'reg_lambda': 5.222078535462175, 'min_child_weight': 0.016776636820748713, 'min_child_samples': 82, 'scale_pos_weight': 0.6395380359410106}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:24:48,769]\u001b[0m Trial 9 finished with value: 0.1498708010335917 and parameters: {'num_leaves': 15, 'subsample': 0.8, 'colsample_bytree': 0.55, 'reg_lambda': 0.04465522943054736, 'min_child_weight': 0.06729357683187608, 'min_child_samples': 26, 'scale_pos_weight': 2.793945194798537}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:25:05,398]\u001b[0m Trial 10 finished with value: 0.1568627450980392 and parameters: {'num_leaves': 31, 'subsample': 0.5, 'colsample_bytree': 0.8, 'reg_lambda': 0.5999411234488659, 'min_child_weight': 0.0012868496833858389, 'min_child_samples': 5, 'scale_pos_weight': 0.27948932800213755}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:25:28,089]\u001b[0m Trial 11 finished with value: 0.1794871794871795 and parameters: {'num_leaves': 255, 'subsample': 0.6, 'colsample_bytree': 0.75, 'reg_lambda': 0.2146338247930371, 'min_child_weight': 0.2890893637906485, 'min_child_samples': 8, 'scale_pos_weight': 3.1868426835139565}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:25:39,093]\u001b[0m Trial 12 finished with value: 0.1774580335731415 and parameters: {'num_leaves': 255, 'subsample': 0.65, 'colsample_bytree': 0.7, 'reg_lambda': 0.0026456935431999626, 'min_child_weight': 0.012622076016602918, 'min_child_samples': 36, 'scale_pos_weight': 2.00818535280237}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:26:01,426]\u001b[0m Trial 13 finished with value: 0.16981132075471697 and parameters: {'num_leaves': 255, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_lambda': 0.9058243019421504, 'min_child_weight': 0.21124464154429295, 'min_child_samples': 11, 'scale_pos_weight': 4.436672677322273}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:26:16,552]\u001b[0m Trial 14 finished with value: 0.175 and parameters: {'num_leaves': 63, 'subsample': 0.6, 'colsample_bytree': 0.7, 'reg_lambda': 0.10625122830951238, 'min_child_weight': 8.31269052113397, 'min_child_samples': 5, 'scale_pos_weight': 1.6849582253381525}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:26:29,106]\u001b[0m Trial 15 finished with value: 0.20134228187919465 and parameters: {'num_leaves': 31, 'subsample': 0.75, 'colsample_bytree': 0.8, 'reg_lambda': 1.5693307776741472, 'min_child_weight': 0.011835188602150453, 'min_child_samples': 33, 'scale_pos_weight': 0.42176609951832605}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:26:40,888]\u001b[0m Trial 16 finished with value: 0.16296296296296298 and parameters: {'num_leaves': 31, 'subsample': 0.6, 'colsample_bytree': 0.8, 'reg_lambda': 1.6109727397517206, 'min_child_weight': 0.00791355908587905, 'min_child_samples': 39, 'scale_pos_weight': 0.3732193384787976}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:26:58,418]\u001b[0m Trial 17 finished with value: 0.13043478260869568 and parameters: {'num_leaves': 31, 'subsample': 0.75, 'colsample_bytree': 0.75, 'reg_lambda': 1.4951503202657521, 'min_child_weight': 0.001133660108473408, 'min_child_samples': 3, 'scale_pos_weight': 0.44852498420694703}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:27:13,734]\u001b[0m Trial 18 finished with value: 0.15517241379310345 and parameters: {'num_leaves': 31, 'subsample': 0.5, 'colsample_bytree': 0.75, 'reg_lambda': 0.28233307776143, 'min_child_weight': 0.004260036493515185, 'min_child_samples': 5, 'scale_pos_weight': 0.3611269129374958}. Best is trial 3 with value: 0.20512820512820515.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:27:25,380]\u001b[0m Trial 19 finished with value: 0.21965317919075142 and parameters: {'num_leaves': 31, 'subsample': 0.55, 'colsample_bytree': 0.65, 'reg_lambda': 2.2964069570360106, 'min_child_weight': 0.026980414093576235, 'min_child_samples': 12, 'scale_pos_weight': 0.7020714048693707}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:27:43,386]\u001b[0m Trial 20 finished with value: 0.1646090534979424 and parameters: {'num_leaves': 63, 'subsample': 0.55, 'colsample_bytree': 0.65, 'reg_lambda': 0.007999379384733345, 'min_child_weight': 0.033569523067244665, 'min_child_samples': 11, 'scale_pos_weight': 0.7409751342676693}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:27:53,634]\u001b[0m Trial 21 finished with value: 0.20238095238095238 and parameters: {'num_leaves': 31, 'subsample': 0.65, 'colsample_bytree': 0.6, 'reg_lambda': 2.289360575072851, 'min_child_weight': 0.020427903641858553, 'min_child_samples': 33, 'scale_pos_weight': 0.5357069946529869}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:28:50,283]\u001b[0m Trial 22 finished with value: 0.12612612612612614 and parameters: {'num_leaves': 31, 'subsample': 0.65, 'colsample_bytree': 0.6, 'reg_lambda': 8.419082100313219, 'min_child_weight': 0.019992770577528646, 'min_child_samples': 7, 'scale_pos_weight': 0.5936520635021616}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:29:04,446]\u001b[0m Trial 23 finished with value: 0.16267942583732056 and parameters: {'num_leaves': 31, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_lambda': 2.776193182816957, 'min_child_weight': 0.004079959555468798, 'min_child_samples': 12, 'scale_pos_weight': 0.8909257653349633}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:29:16,286]\u001b[0m Trial 24 finished with value: 0.21164021164021166 and parameters: {'num_leaves': 31, 'subsample': 0.55, 'colsample_bytree': 0.65, 'reg_lambda': 0.6666436260978911, 'min_child_weight': 0.035133016146882634, 'min_child_samples': 15, 'scale_pos_weight': 1.4643796980749395}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:29:35,689]\u001b[0m Trial 25 finished with value: 0.16170212765957448 and parameters: {'num_leaves': 127, 'subsample': 0.55, 'colsample_bytree': 0.65, 'reg_lambda': 0.643043269198229, 'min_child_weight': 0.07166247079019282, 'min_child_samples': 14, 'scale_pos_weight': 1.4598839104128025}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n","\u001b[32m[I 2022-06-16 04:29:49,328]\u001b[0m Trial 26 finished with value: 0.16129032258064516 and parameters: {'num_leaves': 31, 'subsample': 0.5, 'colsample_bytree': 0.65, 'reg_lambda': 0.1455795031911768, 'min_child_weight': 0.12315775198194703, 'min_child_samples': 6, 'scale_pos_weight': 1.2984080002526437}. Best is trial 19 with value: 0.21965317919075142.\u001b[0m\n"]}],"source":["model_list = []\n","params_list = []\n","fold_metric = []\n","\n","val_pred = np.zeros(shape=(df_full.shape[0], 2))\n","test_pred = np.zeros(shape=(df_test.shape[0], 2))\n","seed_everything()\n","\n","n_folds = 5\n","kfolds_spliter = StratifiedKFold(n_folds, shuffle=True, random_state=42)\n","\n","start_time_training = time()\n","# fold training\n","for fold, (train_idx, val_idx) in enumerate(kfolds_spliter.split(df_full, y=df_full[target_var])):\n","    start_mem = memory_usage()   \n","    do_fold_training(fold, train_idx, val_idx, None, finetuning=False)\n","    gc.collect()\n","    end_mem = memory_usage()\n","    print(\"@Memory leaked :\", end_mem - start_mem, \"\\n\")\n","end_time_training = time()"]},{"cell_type":"code","source":["avg_score = 0\n","for idx, value in enumerate(fold_metric):\n","    print(\"fold\", idx, \"score :\", value)\n","    avg_score += value / n_folds\n","print(\"average score :\", avg_score)"],"metadata":{"id":"Xhyg99zDMwd7"},"id":"Xhyg99zDMwd7","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"a8585466-800b-4125-8289-5dce5070990c","metadata":{"id":"a8585466-800b-4125-8289-5dce5070990c"},"source":["# Submission"]},{"cell_type":"code","source":["dataframe(val_pred).to_csv(folder_path + \"architecture/\" + architecture_name + \"/valPred_\" + architecture_name + \".csv\", index=False)\n","dataframe(test_pred).to_csv(folder_path + \"architecture/\" + architecture_name + \"/testPred_\" + architecture_name + \".csv\", index=False)"],"metadata":{"id":"AmbRCTkpYWzI"},"id":"AmbRCTkpYWzI","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"91e91fa6-ef2c-418f-90f2-e5662abe51b9","metadata":{"id":"91e91fa6-ef2c-418f-90f2-e5662abe51b9"},"outputs":[],"source":["submission = pd.read_csv(folder_path + 'open.zip (Unzipped Files)/sample_submission.csv')\n","submission['covid19'] = [1 if i > threshold else 0 for i in test_pred[:,1]]\n","submission.to_csv(folder_path + \"architecture/\" + architecture_name + \"/submission_\" + architecture_name + \".csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"main_datasave_.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":["AHNQiz11XWr6","qfIQDoMKKgfj","oo3zFM7xAJrM","a8585466-800b-4125-8289-5dce5070990c"]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}