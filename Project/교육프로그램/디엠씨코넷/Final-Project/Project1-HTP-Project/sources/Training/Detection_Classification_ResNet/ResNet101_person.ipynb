{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\bax\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gc  # 메모리를 초기화 하는 모듈\n",
    "from PIL import Image  #이미지 처리를 위한 모듈\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version:  1.13.1  Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU (cuda) 설정\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version: ', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTP 데이터프레임 로드\n",
    "# House, Tree, Person 데이터를 각각 따로 불러와서 모델 훈련\n",
    "with open(r\"C:\\BAX_HTP_project\\deeplearning\\df_metainfo.pkl\",\"rb\") as fr:\n",
    "    person_data = pickle.load(fr)[\"person\"]\n",
    "person_full_folder = r'./full_images/' + person_data[\"id\"].astype(\"str\") + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 작업 및 이미지 텐서화 후, np.stack 함수를 이용하여 1차원으로 만드는 과정\n",
    "# 이미지 전처리 후, numpy 안의 stack 함수를 이용해 img 피처를 만든다\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "img_features = np.stack([preprocess(Image.open(img_path)) for img_path in person_full_folder]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "\n",
    "# pre-trained 된 ResNet101 1k 버전을 torch.hub를 통해 load\n",
    "class HTP_Classifier(nn.Module):\n",
    "    def __init__(self, n_output):\n",
    "        super(HTP_Classifier, self).__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, n_output) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# 데이터셋 레이블 배치에 맞게 합치기\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, img_feature, label):\n",
    "        self.data = img_feature \n",
    "        self.labels = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 데이터셋의 크기를 반환합니다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def training(model_htp, train_dataloader, test_dataloader):\n",
    "    # model_htp = HTP_Classifier(n_output=class_counts)\n",
    "    # model_htp.to(DEVICE)\n",
    "\n",
    "    # 다중 분류\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_htp.parameters(), lr=1e-5)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        \n",
    "        # train\n",
    "        running_loss = 0.0\n",
    "        for data in train_dataloader:\n",
    "            model_htp.train()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model_htp(inputs)\n",
    "            loss = criterion(outputs, labels.to(torch.int64))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Train Loss ({label}): {running_loss / len(train_dataloader):.3f}')\n",
    "        \n",
    "        # test\n",
    "        val_loss = 0.0\n",
    "        for data in test_dataloader:\n",
    "            model_htp.eval()\n",
    "            with torch.no_grad():\n",
    "                images, labels = data\n",
    "                images = images.to(DEVICE)   \n",
    "                labels = labels.to(DEVICE)\n",
    "                outputs = model_htp(images)\n",
    "                loss = criterion(outputs, labels.to(torch.int64))\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Val Loss ({label}): {val_loss / len(test_dataloader):.3f}')\n",
    "        \n",
    "        early_stopping(val_loss, model_htp)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early Stopping ({label})')\n",
    "            torch.save(model_htp.state_dict(), PATH)\n",
    "            break\n",
    "    \n",
    "    print('Finished Training')\n",
    "    model_htp.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss (eye_yn): 0.314\n",
      "Val Loss (eye_yn): 0.156\n",
      "Validation loss decreased (inf --> 7.787522).  Saving model ...\n",
      "Train Loss (eye_yn): 0.126\n",
      "Val Loss (eye_yn): 0.141\n",
      "Validation loss decreased (7.787522 --> 7.047907).  Saving model ...\n",
      "Train Loss (eye_yn): 0.069\n",
      "Val Loss (eye_yn): 0.143\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Train Loss (eye_yn): 0.027\n",
      "Val Loss (eye_yn): 0.162\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Train Loss (eye_yn): 0.020\n",
      "Val Loss (eye_yn): 0.161\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Train Loss (eye_yn): 0.013\n",
      "Val Loss (eye_yn): 0.175\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Train Loss (eye_yn): 0.007\n",
      "Val Loss (eye_yn): 0.180\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early Stopping (eye_yn)\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss (leg_yn): 0.254\n",
      "Val Loss (leg_yn): 0.197\n",
      "Validation loss decreased (inf --> 9.870692).  Saving model ...\n",
      "Train Loss (leg_yn): 0.142\n",
      "Val Loss (leg_yn): 0.179\n",
      "Validation loss decreased (9.870692 --> 8.946040).  Saving model ...\n",
      "Train Loss (leg_yn): 0.072\n",
      "Val Loss (leg_yn): 0.168\n",
      "Validation loss decreased (8.946040 --> 8.376342).  Saving model ...\n",
      "Train Loss (leg_yn): 0.038\n",
      "Val Loss (leg_yn): 0.198\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Train Loss (leg_yn): 0.024\n",
      "Val Loss (leg_yn): 0.199\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Train Loss (leg_yn): 0.013\n",
      "Val Loss (leg_yn): 0.220\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Train Loss (leg_yn): 0.009\n",
      "Val Loss (leg_yn): 0.184\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Train Loss (leg_yn): 0.006\n",
      "Val Loss (leg_yn): 0.190\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early Stopping (leg_yn)\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss (mouth_yn): 0.358\n",
      "Val Loss (mouth_yn): 0.279\n",
      "Validation loss decreased (inf --> 13.936497).  Saving model ...\n",
      "Train Loss (mouth_yn): 0.228\n",
      "Val Loss (mouth_yn): 0.238\n",
      "Validation loss decreased (13.936497 --> 11.908119).  Saving model ...\n",
      "Train Loss (mouth_yn): 0.134\n",
      "Val Loss (mouth_yn): 0.291\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Train Loss (mouth_yn): 0.073\n",
      "Val Loss (mouth_yn): 0.317\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Train Loss (mouth_yn): 0.042\n",
      "Val Loss (mouth_yn): 0.310\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Train Loss (mouth_yn): 0.027\n",
      "Val Loss (mouth_yn): 0.331\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Train Loss (mouth_yn): 0.015\n",
      "Val Loss (mouth_yn): 0.339\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early Stopping (mouth_yn)\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss (arm_yn): 0.309\n",
      "Val Loss (arm_yn): 0.224\n",
      "Validation loss decreased (inf --> 11.222726).  Saving model ...\n",
      "Train Loss (arm_yn): 0.175\n",
      "Val Loss (arm_yn): 0.225\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Train Loss (arm_yn): 0.112\n",
      "Val Loss (arm_yn): 0.212\n",
      "Validation loss decreased (11.222726 --> 10.576431).  Saving model ...\n",
      "Train Loss (arm_yn): 0.063\n",
      "Val Loss (arm_yn): 0.257\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Train Loss (arm_yn): 0.029\n",
      "Val Loss (arm_yn): 0.232\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Train Loss (arm_yn): 0.019\n",
      "Val Loss (arm_yn): 0.245\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Train Loss (arm_yn): 0.012\n",
      "Val Loss (arm_yn): 0.250\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Train Loss (arm_yn): 0.008\n",
      "Val Loss (arm_yn): 0.287\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early Stopping (arm_yn)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "labels = ['eye_yn', 'leg_yn', 'mouth_yn', 'arm_yn']\n",
    "\n",
    "for label in labels:\n",
    "    PATH = f'./save_moel/ResNet101_{label}.pt'\n",
    "    train_img_features = img_features[person_data[f'fold_{label}'] != 3]\n",
    "    train_labels = person_data[person_data[f'fold_{label}'] != 3][label].reset_index(drop=True).values\n",
    "    test_img_features = img_features[person_data[f'fold_{label}'] == 3]\n",
    "    test_labels = person_data[person_data[f'fold_{label}'] == 3][label].reset_index(drop=True).values\n",
    "    \n",
    "    train_dataloader = DataLoader(MyDataset(train_img_features, train_labels), batch_size=4, shuffle=True)\n",
    "    test_dataloader = DataLoader(MyDataset(test_img_features, test_labels), batch_size=4, shuffle=False)\n",
    "    \n",
    "    model_htp = HTP_Classifier(n_output=person_data[label].nunique())\n",
    "    model_htp.to(DEVICE)\n",
    "\n",
    "    training(model_htp, train_dataloader, test_dataloader)\n",
    "    \n",
    "    del model_htp, train_dataloader, test_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
