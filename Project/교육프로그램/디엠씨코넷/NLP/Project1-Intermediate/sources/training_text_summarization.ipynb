{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "GLOBAL_SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "from numpy import random as np_rnd\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random as rnd\n",
    "from nltk.corpus import stopwords\n",
    "import time \n",
    "import sentencepiece as spm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # python random\n",
    "    rnd.seed(seed)\n",
    "    # numpy random\n",
    "    np_rnd.seed(seed)\n",
    "    # RAPIDS random\n",
    "    try:\n",
    "        cupy.random.seed(seed)\n",
    "    except:\n",
    "        pass\n",
    "    # tf random\n",
    "    try:\n",
    "        tf_rnd.set_seed(seed)\n",
    "    except:\n",
    "        pass\n",
    "    # pytorch random\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def pickleIO(obj, src, op=\"r\"):\n",
    "    if op==\"w\":\n",
    "        with open(src, op + \"b\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif op==\"r\":\n",
    "        with open(src, op + \"b\") as f:\n",
    "            tmp = pickle.load(f)\n",
    "        return tmp\n",
    "    else:\n",
    "        print(\"unknown operation\")\n",
    "        return obj\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print('Error: Creating directory. ' + directory)\n",
    "\n",
    "def findIdx(data_x, col_names):\n",
    "    return [int(i) for i, j in enumerate(data_x) if j in col_names]\n",
    "\n",
    "def diff(first, second):\n",
    "    second = set(second)\n",
    "    return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    epochs = 2 if debug else 30\n",
    "    early_stopping_rounds = 10\n",
    "    batch_size = 64\n",
    "    eta = 5e-4\n",
    "    weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = []\n",
    "for catPath in glob.glob(r\".\\022.요약문 및 레포트 생성 데이터\\01.데이터\\1.Training\\라벨링데이터\\TL1\\*\"):\n",
    "    df = []\n",
    "    for fpath in glob.glob(catPath + \"./2~3sent/*\"):\n",
    "        with open(fpath, encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            df.append({\n",
    "                \"doc\": data['Meta(Refine)'][\"passage\"],\n",
    "                \"label\": data['Annotation'][\"summary2\"] if data['Annotation'][\"summary2\"] is not None else data['Annotation'][\"summary1\"],\n",
    "                \"cat\": catPath.split(\".\")[-1],\n",
    "            })\n",
    "    df_merge.append(pd.DataFrame(df))\n",
    "df_train = pd.concat(df_merge).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = []\n",
    "for catPath in glob.glob(r\".\\022.요약문 및 레포트 생성 데이터\\01.데이터\\2.Validation\\라벨링데이터\\VL1\\*\"):\n",
    "    df = []\n",
    "    for fpath in glob.glob(catPath + \"./2~3sent/*\"):\n",
    "        with open(fpath, encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            df.append({\n",
    "                \"doc\": data['Meta(Refine)'][\"passage\"],\n",
    "                \"label\": data['Annotation'][\"summary2\"] if data['Annotation'][\"summary2\"] is not None else data['Annotation'][\"summary1\"],\n",
    "                \"cat\": catPath.split(\".\")[-1],\n",
    "            })\n",
    "    df_merge.append(pd.DataFrame(df))\n",
    "df_valid = pd.concat(df_merge).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_merge\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"[^가-힣 ]\", \"\", text)\n",
    "    text = \" \".join([i for i in text.split() if len(i) > 1])\n",
    "    return text\n",
    "\n",
    "sample = \"안녕하세요.\\n 오랜만입니다.\"\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_lbe = pd.Series(range(df_train[\"cat\"].nunique()), index=df_train[\"cat\"].unique())\n",
    "\n",
    "df_train[\"doc\"] = df_train[\"doc\"].apply(preprocess)\n",
    "df_train[\"label\"] = df_train[\"label\"].apply(preprocess)\n",
    "df_train[\"cat\"] = df_train[\"cat\"].map(cat_lbe)\n",
    "\n",
    "df_valid[\"doc\"] = df_valid[\"doc\"].apply(preprocess)\n",
    "df_valid[\"label\"] = df_valid[\"label\"].apply(preprocess)\n",
    "df_valid[\"cat\"] = df_valid[\"cat\"].map(cat_lbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_lbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickleIO(cat_lbe, \"./cat_lbe.pkl\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training sentence piece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/spt_train_data.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.writelines((df_train[\"doc\"].astype(\"str\") + \"\\n\").to_list() + (df_train[\"label\"].astype(\"str\") + \"\\n\").to_list() + (df_valid[\"doc\"].astype(\"str\") + \"\\n\").to_list() + (df_valid[\"label\"].astype(\"str\") + \"\\n\").to_list())\n",
    "\n",
    "vocab_size = 16384 + 4\n",
    "spm.SentencePieceTrainer.train(f'--input=./dataset/spt_train_data.txt --model_prefix=tokenizer --model_type=bpe --max_sentence_length=8192 --vocab_size={vocab_size} --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tokenizer.model')\n",
    "vocab = {sp.id_to_piece(i): i for i in range(sp.get_piece_size())}\n",
    "print(\"number of vocab\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(np.percentile([len(sp.EncodeAsIds(i)) for i in df_train[\"doc\"]], 0.95))\n",
    "display(np.percentile([len(sp.EncodeAsIds(i)) for i in df_train[\"label\"]], 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_max_len = 122\n",
    "label_max_len = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1, random_state=GLOBAL_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_container = {}\n",
    "valid_container = {}\n",
    "\n",
    "def tokenizing(x, max_len):\n",
    "    token = torch.tensor([vocab[\"<s>\"]] + sp.EncodeAsIds(x)[:(max_len-2)] + [vocab[\"</s>\"]], dtype=torch.int64)\n",
    "    if len(token) < max_len:\n",
    "        token = torch.cat([token, torch.zeros(max_len - len(token), dtype=torch.int64)])\n",
    "    assert len(token) == max_len\n",
    "    return token\n",
    "\n",
    "train_container[\"doc\"] = torch.stack([tokenizing(i, feature_max_len) for i in df_train[\"doc\"]])\n",
    "train_container[\"label\"] = torch.stack([tokenizing(i, label_max_len) for i in df_train[\"label\"]])\n",
    "train_container[\"cat\"] = torch.tensor(df_train[\"cat\"], dtype=torch.int64)\n",
    "\n",
    "valid_container[\"doc\"] = torch.stack([tokenizing(i, feature_max_len) for i in df_valid[\"doc\"]])\n",
    "valid_container[\"label\"] = torch.stack([tokenizing(i, label_max_len) for i in df_valid[\"label\"]])\n",
    "valid_container[\"cat\"] = torch.tensor(df_valid[\"cat\"], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, eta, weight_decay):\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        # apply weight decay\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': eta, 'weight_decay': weight_decay},\n",
    "        # don't apply weight decay for LayerNormalization layer\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': eta, 'weight_decay': 0.0},\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
    "    scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps, power=0.5, lr_end=1e-7\n",
    "    )\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, dl, criterion, optimizer, scheduler, grad_scaler):\n",
    "    model.train()\n",
    "    metrics = {\n",
    "        \"loss\": AverageMeter(\"loss\", fmt=\":.5f\"),\n",
    "        \"accuracy\": AverageMeter(\"accuracy\", fmt=\":.5f\"),\n",
    "    }\n",
    "    \n",
    "    for idx, batch in enumerate(train_dl):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            batch[0] = batch[0].to(device)\n",
    "            batch[1] = batch[1].to(device)\n",
    "            batch[2] = batch[2].to(device)\n",
    "\n",
    "            encoder_pos = torch.tile(torch.arange(batch[0].shape[1], dtype=torch.int64).view(1, -1), (batch[0].shape[0], 1)).to(device)\n",
    "            last_hc = torch.zeros(generating_params[\"num_layers\"], len(batch[0]), generating_params[\"embed_dim\"], device=device) + 1e-7, \\\n",
    "                    torch.zeros(generating_params[\"num_layers\"], len(batch[0]), generating_params[\"embed_dim\"], device=device) + 1e-7\n",
    "            encoder_output, last_hc = model.get_encoder_output(\n",
    "                input=batch[0], input_pos=encoder_pos, input_cat=torch.tile(batch[2].view(-1, 1), (1, batch[0].shape[-1])), last_hc=last_hc,\n",
    "            )\n",
    "            decoder_output = torch.zeros(len(batch[0]), 1, generating_params[\"embed_dim\"], dtype=torch.int64, device=device) + 1e-7\n",
    "            decoder_input = batch[1][:, [0]]\n",
    "\n",
    "            loss = 0\n",
    "            acc = 0\n",
    "            for i in range(label_max_len-1):\n",
    "                decoder_pos = (torch.zeros(len(batch[0]), dtype=torch.int64, device=device).view(-1, 1) + i)\n",
    "                decoder_output, decoder_output_prob, last_hc = model.get_decoder_output(\n",
    "                    input=decoder_input, input_pos=decoder_pos, input_cat=batch[2].view(-1, 1),\n",
    "                    encoder_output=encoder_output, last_hc=last_hc, last_hidden_state=decoder_output\n",
    "                )\n",
    "                decoder_output_prob = decoder_output_prob.squeeze(dim=1)\n",
    "                decoder_output_cls = F.softmax(decoder_output_prob, dim=-1).argmax(dim=-1)\n",
    "                loss += criterion(decoder_output_prob, batch[1][:, (i+1)]) / (label_max_len-1)\n",
    "                acc += (((decoder_output_cls == batch[1][:, (i+1)]).float())[batch[1][:, (i+1)] != 0]).mean() / (label_max_len-1)\n",
    "                if rnd.random() < generating_params[\"teacher_forcing_ratio\"]:\n",
    "                    decoder_input = batch[1][:, [(i+1)]]\n",
    "                else:\n",
    "                    decoder_input = decoder_output_cls.view(-1, 1)\n",
    "\n",
    "        # initialization gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # get scaled gradients by float16 (default)\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        # apply original gradients (unscaling) to parameters\n",
    "        # if these gradients do not contain infs or NaNs, optimizer.step() is then called.\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "        # calcuate metrics\n",
    "        metrics[\"loss\"].update(loss.item())\n",
    "        metrics[\"accuracy\"].update(acc.item())\n",
    "    \n",
    "    # update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def valid_fn(model, dl, criterion):\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        \"loss\": AverageMeter(\"loss\", fmt=\":.5f\"),\n",
    "        \"accuracy\": AverageMeter(\"accuracy\", fmt=\":.5f\"),\n",
    "    }\n",
    "    \n",
    "    for idx, batch in enumerate(dl):\n",
    "        with torch.no_grad():\n",
    "            batch[0] = batch[0].to(device)\n",
    "            batch[1] = batch[1].to(device)\n",
    "            batch[2] = batch[2].to(device)\n",
    "\n",
    "            encoder_pos = torch.tile(torch.arange(batch[0].shape[1], dtype=torch.int64).view(1, -1), (batch[0].shape[0], 1)).to(device)\n",
    "            last_hc = torch.zeros(generating_params[\"num_layers\"], len(batch[0]), generating_params[\"embed_dim\"], device=device) + 1e-7, \\\n",
    "                    torch.zeros(generating_params[\"num_layers\"], len(batch[0]), generating_params[\"embed_dim\"], device=device) + 1e-7            \n",
    "            encoder_output, last_hc = model.get_encoder_output(\n",
    "                input=batch[0], input_pos=encoder_pos, input_cat=torch.tile(batch[2].view(-1, 1), (1, batch[0].shape[-1])), last_hc=last_hc,\n",
    "            )\n",
    "            decoder_output = torch.zeros(len(batch[0]), 1, generating_params[\"embed_dim\"], dtype=torch.int64, device=device) + 1e-7\n",
    "            decoder_input = batch[1][:, [0]]\n",
    "\n",
    "            loss = 0\n",
    "            acc = 0\n",
    "            for i in range(label_max_len-1):\n",
    "                decoder_pos = (torch.zeros(len(batch[0]), dtype=torch.int64, device=device).view(-1, 1) + i)\n",
    "                decoder_output, decoder_output_prob, last_hc = model.get_decoder_output(\n",
    "                    input=decoder_input, input_pos=decoder_pos, input_cat=batch[2].view(-1, 1),\n",
    "                    encoder_output=encoder_output, last_hc=last_hc, last_hidden_state=decoder_output\n",
    "                )\n",
    "                decoder_output_prob = decoder_output_prob.squeeze(dim=1)\n",
    "                decoder_output_cls = F.softmax(decoder_output_prob, dim=-1).argmax(dim=-1)\n",
    "                loss += criterion(decoder_output_prob, batch[1][:, (i+1)]) / (label_max_len-1)\n",
    "                acc += (((decoder_output_cls == batch[1][:, (i+1)]).float())[batch[1][:, (i+1)] != 0]).mean() / (label_max_len-1)\n",
    "                if rnd.random() < generating_params[\"teacher_forcing_ratio\"]:\n",
    "                    decoder_input = batch[1][:, [(i+1)]]\n",
    "                else:\n",
    "                    decoder_input = decoder_output_cls.view(-1, 1)\n",
    "\n",
    "        # calcuate metrics\n",
    "        metrics[\"loss\"].update(loss.item())\n",
    "        metrics[\"accuracy\"].update(acc.item())\n",
    "                \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(fold, model):\n",
    "    # set loss & optimizer\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model,\n",
    "        eta=CFG.eta,\n",
    "        weight_decay=CFG.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.eta, weight_decay=CFG.weight_decay)\n",
    "    scheduler = get_scheduler(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=CFG.epochs\n",
    "    )\n",
    "    grad_scaler = torch.cuda.amp.GradScaler()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    best_score = np.inf\n",
    "    early_stopping_cnt = 0\n",
    "    for epoch in range(CFG.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # training\n",
    "        train_metrics = train_fn(model, train_dl, criterion, optimizer, scheduler, grad_scaler)\n",
    "        # evaluation\n",
    "        valid_metrics = valid_fn(model, valid_dl, criterion)\n",
    "        score = valid_metrics[\"loss\"].avg\n",
    "\n",
    "        print(\"Epoch[{0}/{1}]\\n train loss : {2}\\n train accuracy : {3}\\n valid loss : {4}\\n valid accuracy : {5}\\n eta : {6}\\n Elapsed : {7}\\n\"\n",
    "              .format(\n",
    "                  epoch+1, CFG.epochs,\n",
    "                  round(train_metrics[\"loss\"].avg, 5), round(train_metrics[\"accuracy\"].avg, 5),\n",
    "                  round(valid_metrics[\"loss\"].avg, 5), round(valid_metrics[\"accuracy\"].avg, 5),\n",
    "                  round(optimizer.param_groups[-1]['lr'], 5), round(time.time() - epoch_start_time, 3)\n",
    "              )\n",
    "        )\n",
    "    \n",
    "        torch.save(\n",
    "            {'model': model.state_dict()},\n",
    "            f\"./model_fold{fold}_best.pth\",\n",
    "        )\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            return_score_dic = {\n",
    "                \"fold\": fold,\n",
    "                \"train_loss\": train_metrics[\"loss\"].avg,\n",
    "                \"valid_loss\": valid_metrics[\"loss\"].avg,\n",
    "                \"train_accuracy\": train_metrics[\"accuracy\"].avg,\n",
    "                \"valid_accuracy\": valid_metrics[\"accuracy\"].avg,                \n",
    "            }\n",
    "            print(\"INFO: Found best weight\\n\\n\")\n",
    "            early_stopping_cnt = 0\n",
    "        else:\n",
    "            early_stopping_cnt += 1\n",
    "        \n",
    "        if early_stopping_cnt == CFG.early_stopping_rounds:\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"./model_fold{return_score_dic['fold']}_best.pth\")[\"model\"])\n",
    "    \n",
    "    return return_score_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Block\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_dim, max_len, num_layers=2, dropout_p=0.5):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.lin_embed = nn.Linear(embed_dim + (embed_dim // 4) + (embed_dim // 4 // 4), embed_dim)\n",
    "        # learing layer for latent vector\n",
    "        self.gru = nn.LSTM(self.embed_dim, self.embed_dim, num_layers=self.num_layers, dropout=self.dropout_p/4, bidirectional=False, batch_first=True)\n",
    "        # learning block\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.Linear(self.embed_dim, self.embed_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.Linear(self.embed_dim * 2, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input, input_pos, input_cat, last_hc, global_embedding_layer, pos_embedding, cat_embedding):\n",
    "        # input (B, SEQ)\n",
    "        # embedding (B, SEQ, EMBED)\n",
    "        embed = torch.cat([\n",
    "            global_embedding_layer(input),\n",
    "            pos_embedding(input_pos),\n",
    "            cat_embedding(input_cat),\n",
    "        ], dim=-1)\n",
    "        embed = self.lin_embed(embed)\n",
    "        # GRU (B, SEQ, hidden_layer_size)\n",
    "        output, (hn, cn) = self.gru(embed, last_hc)\n",
    "        # linear transformation on output\n",
    "        output = self.lin(output) + output\n",
    "        return output, (hn, cn)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, embed_dim, max_len, num_layers=2, dropout_p=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.lin_embed = nn.Linear(embed_dim + (embed_dim // 4) + (embed_dim // 4 // 4), embed_dim)\n",
    "        # learing layer for new latent vector with recent hidden cell state\n",
    "        self.gru = nn.LSTM(self.embed_dim, self.embed_dim, num_layers=self.num_layers, dropout=self.dropout_p/4, bidirectional=False, batch_first=True)\n",
    "        # learning block\n",
    "        self.lin1 = nn.Sequential(\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(self.embed_dim, self.embed_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(self.embed_dim * 2, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # learning block\n",
    "        self.lin2 = nn.Sequential(\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(self.embed_dim * 3, self.embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(self.embed_dim * 4, self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.embed_dim, self.output_size)\n",
    "\n",
    "    def forward(self, input, input_pos, input_cat, encoder_output, last_hc, last_hidden_state, global_embedding_layer, pos_embedding, cat_embedding):\n",
    "        # input (B, 1) - only one token\n",
    "        embed = torch.cat([\n",
    "            global_embedding_layer(input),\n",
    "            pos_embedding(input_pos),\n",
    "            cat_embedding(input_cat),\n",
    "        ], dim=-1)\n",
    "        embed = self.lin_embed(embed)\n",
    "        # GRU (B, SEQ, hidden_layer_size)\n",
    "        output, (hn, cn) = self.gru(embed, last_hc)\n",
    "        # linear transformation on output\n",
    "        output = self.lin1(output) + output\n",
    "        # (B, 1, EMBED) * (B, EMBED, SEQ) -> (B, 1, SEQ)\n",
    "        attn_weights = F.softmax(torch.bmm(output, encoder_output.transpose(1, 2)), dim=-1)\n",
    "        # (B, 1, SEQ) * (B, EMBED, SEQ) -> (B, 1, EMBED)\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_output)\n",
    "        # linear transformation (attention ouptut + output + last output)\n",
    "        output = self.lin2(torch.cat([attn_applied, output, last_hidden_state], dim=-1))\n",
    "        # get probablity\n",
    "        output_prob = self.classifier(output)\n",
    "\n",
    "        return output, output_prob, (hn, cn)\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder_block, decoder_block, input_size, embed_dim, feature_max_len,  label_max_len, cat_size):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(input_size, embed_dim)\n",
    "        self.encoder_pos_embedding = nn.Embedding(feature_max_len, embed_dim // 4)\n",
    "        self.decoder_pos_embedding = nn.Embedding(label_max_len, embed_dim // 4)\n",
    "        self.cat_embedding = nn.Embedding(cat_size, embed_dim // 4 // 4)\n",
    "        self.encoder_block = encoder_block\n",
    "        self.decoder_block = decoder_block\n",
    "    def get_encoder_output(self, input, input_pos, input_cat, last_hc):\n",
    "        return self.encoder_block(input, input_pos, input_cat, last_hc, self.token_embedding, self.encoder_pos_embedding, self.cat_embedding)\n",
    "    def get_decoder_output(self, input, input_pos, input_cat, encoder_output, last_hc, last_hidden_state):\n",
    "        return self.decoder_block(input, input_pos, input_cat, encoder_output, last_hc, last_hidden_state, self.token_embedding, self.decoder_pos_embedding, self.cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed_everything(GLOBAL_SEED)\n",
    "\n",
    "generating_params = {\n",
    "    \"embed_dim\": 512,\n",
    "    \"num_layers\": 2,\n",
    "    \"teacher_forcing_ratio\": 0.5,\n",
    "}\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    input_size=sp.get_piece_size(), embed_dim=generating_params[\"embed_dim\"], max_len=feature_max_len, num_layers=generating_params[\"num_layers\"],\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    input_size=sp.get_piece_size(), output_size=sp.get_piece_size(), embed_dim=generating_params[\"embed_dim\"], max_len=label_max_len, num_layers=generating_params[\"num_layers\"],\n",
    ")\n",
    "model = Seq2SeqModel(encoder, decoder, input_size=sp.get_piece_size(), embed_dim=generating_params[\"embed_dim\"], feature_max_len=feature_max_len, label_max_len=label_max_len, cat_size=len(cat_lbe))\n",
    "model.to(device)\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(train_container[\"doc\"], train_container[\"label\"], train_container[\"cat\"]), batch_size=CFG.batch_size, shuffle=True, drop_last=True)\n",
    "print(\"number of iteration :\", len(train_dl))\n",
    "valid_dl = DataLoader(TensorDataset(valid_container[\"doc\"], valid_container[\"label\"], valid_container[\"cat\"]), batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "# training \n",
    "best_score = do_training(0, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamGPT_TextSummarizer():\n",
    "    def __init__(self, model, tokenizer, generating_params, max_len, token_length_limit=128):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generating_params = generating_params\n",
    "        self.max_len = max_len\n",
    "        self.token_length_limit =token_length_limit\n",
    "        self.model.eval()\n",
    "    def preprocessing(self, text):\n",
    "        text = re.sub(r\"[^가-힣 ]\", \"\", text)\n",
    "        text = \" \".join([i for i in text.split() if len(i) > 1])\n",
    "        return text\n",
    "    def tokenizing(self, text, max_len):\n",
    "        token = torch.tensor([vocab[\"<s>\"]] + self.tokenizer.EncodeAsIds(text)[:(max_len-2)] + [vocab[\"</s>\"]], dtype=torch.int64)\n",
    "        if len(token) < max_len:\n",
    "            token = torch.cat([token, torch.zeros(max_len - len(token), dtype=torch.int64)])\n",
    "        assert len(token) == max_len\n",
    "        return token\n",
    "    def summarize(self, text, cat, device):\n",
    "        text = self.preprocessing(text)\n",
    "        text = self.tokenizing(text, self.max_len)\n",
    "        text = text.view(1, -1).to(device)\n",
    "        cat = torch.tensor([cat], dtype=torch.int64, device=device)\n",
    "        self.model.to(device)\n",
    "        output_cls = []\n",
    "        with torch.no_grad():\n",
    "            encoder_pos = torch.tile(torch.arange(text.shape[1], dtype=torch.int64).view(1, -1), (text.shape[0], 1)).to(device)\n",
    "            last_hc = torch.zeros(generating_params[\"num_layers\"], len(text), generating_params[\"embed_dim\"], device=device) + 1e-7, \\\n",
    "                    torch.zeros(generating_params[\"num_layers\"], len(text), generating_params[\"embed_dim\"], device=device) + 1e-7            \n",
    "            encoder_output, last_hc = model.get_encoder_output(\n",
    "                input=text, input_pos=encoder_pos, input_cat=torch.tile(cat.view(-1, 1), (1, text.shape[-1])), last_hc=last_hc,\n",
    "            )\n",
    "            decoder_output = torch.zeros(len(text), 1, generating_params[\"embed_dim\"], dtype=torch.int64, device=device) + 1e-7\n",
    "            decoder_input = torch.zeros(len(text), 1, dtype=torch.int64, device=device) + vocab[\"</s>\"]\n",
    "            for i in range(self.token_length_limit):\n",
    "                decoder_pos = (torch.zeros(len(text), dtype=torch.int64, device=device).view(-1, 1) + i)\n",
    "                decoder_output, decoder_output_prob, last_hc = self.model.get_decoder_output(\n",
    "                    input=decoder_input, input_pos=decoder_pos, input_cat=cat.view(-1, 1),\n",
    "                    encoder_output=encoder_output, last_hc=last_hc, last_hidden_state=decoder_output\n",
    "                )\n",
    "                decoder_output_prob = decoder_output_prob.squeeze(dim=1)\n",
    "                decoder_output_cls = F.softmax(decoder_output_prob, dim=-1).argmax(dim=-1)\n",
    "                pred_token = decoder_output_cls.squeeze().item()\n",
    "                if pred_token == vocab[\"</s>\"]:\n",
    "                    break\n",
    "                else:\n",
    "                    decoder_input = decoder_output_cls.view(-1, 1)\n",
    "                    output_cls.append(pred_token)\n",
    "        return sp.DecodeIds(output_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = []\n",
    "for catPath in glob.glob(r\".\\022.요약문 및 레포트 생성 데이터\\01.데이터\\1.Training\\라벨링데이터\\TL1\\*\"):\n",
    "    df = []\n",
    "    for fpath in glob.glob(catPath + \"./2~3sent/*\"):\n",
    "        with open(fpath, encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            df.append({\n",
    "                \"doc\": data['Meta(Refine)'][\"passage\"],\n",
    "                \"label\": data['Annotation'][\"summary2\"] if data['Annotation'][\"summary2\"] is not None else data['Annotation'][\"summary1\"],\n",
    "                \"cat\": catPath.split(\".\")[-1],\n",
    "            })\n",
    "    df_merge.append(pd.DataFrame(df))\n",
    "df_train = pd.concat(df_merge).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TeamGPT_TextSummarizer(model, sp, generating_params=generating_params, max_len=feature_max_len, token_length_limit=128)\n",
    "sample_features = df_train.sample(10, random_state=GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, value in sample_features.iterrows():\n",
    "    print(\"original ->\", value[\"label\"])\n",
    "    print(\"summarized ->\", summarizer.summarize(value[\"doc\"], 0, device=torch.device(\"cpu\")))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickleIO(generating_params, \"./model_params.pkl\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
